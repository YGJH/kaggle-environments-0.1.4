#!/usr/bin/env python3
"""
æ¸¬è©¦æ–°çš„è©•ä¼°ç³»çµ±
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from train_connectx_rl_robust import ConnectXTrainer
import yaml

def test_evaluation_modes():
    """æ¸¬è©¦ä¸åŒçš„è©•ä¼°æ¨¡å¼"""
    
    print("ğŸ§ª æ¸¬è©¦ConnectXè©•ä¼°ç³»çµ±")
    print("=" * 50)
    
    # è¼‰å…¥é è¨“ç·´æ¨¡å‹
    if not os.path.exists("config.yaml"):
        print("âŒ è«‹å…ˆç¢ºä¿config.yamlå­˜åœ¨")
        return
    
    if not os.path.exists("checkpoints"):
        print("âŒ æ²’æœ‰æ‰¾åˆ°é è¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆè¨“ç·´æ¨¡å‹")
        return
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = ConnectXTrainer("config.yaml")
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    checkpoints = [f for f in os.listdir("checkpoints") if f.startswith("best_model")]
    if checkpoints:
        import torch
        best_checkpoint = f"checkpoints/{sorted(checkpoints)[-1]}"
        print(f"ğŸ“¥ è¼‰å…¥æ¨¡å‹: {best_checkpoint}")
        
        checkpoint = torch.load(best_checkpoint, map_location=trainer.agent.device)
        trainer.agent.policy_net.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    else:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–çš„æ¨¡å‹")
    
    # æ¸¬è©¦ä¸åŒè©•ä¼°æ¨¡å¼
    test_games = 20  # æ¸¬è©¦ç”¨è¼ƒå°‘éŠæˆ²æ•¸
    
    print("\nğŸ¯ è©•ä¼°æ¨¡å¼ 1: å°éš¨æ©Ÿå°æ‰‹")
    random_win_rate = trainer.evaluate_against_random(test_games)
    print(f"å‹ç‡: {random_win_rate:.3f}")
    
    print("\nğŸ¤– è©•ä¼°æ¨¡å¼ 2: å°Minimaxå°æ‰‹")
    try:
        minimax_win_rate = trainer.evaluate_against_minimax(test_games // 2)
        print(f"å‹ç‡: {minimax_win_rate:.3f}")
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
    
    print("\nğŸ”„ è©•ä¼°æ¨¡å¼ 3: è‡ªå°å¼ˆ")
    try:
        self_play_score = trainer.evaluate_self_play(test_games // 2)
        print(f"å¹³è¡¡åˆ†æ•¸: {self_play_score:.3f}")
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
    
    print("\nğŸ“Š è©•ä¼°æ¨¡å¼ 4: è©³ç´°æŒ‡æ¨™")
    try:
        metrics = trainer.evaluate_with_metrics(test_games)
        print(f"å‹ç‡: {metrics['win_rate']:.3f}")
        print(f"å¹³å‡æ­¥æ•¸: {metrics['avg_game_length']:.1f}")
        print(f"å¿«é€Ÿç²å‹: {metrics['quick_wins']}")
        print(f"é•·éŠæˆ²ç²å‹: {metrics['comeback_wins']}")
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
    
    print("\nğŸ† è©•ä¼°æ¨¡å¼ 5: ç¶œåˆè©•ä¼°")
    try:
        comprehensive = trainer.evaluate_comprehensive(test_games)
        print(f"ç¶œåˆåˆ†æ•¸: {comprehensive['comprehensive_score']:.3f}")
        print(f"  vs éš¨æ©Ÿ: {comprehensive['vs_random']:.3f}")
        print(f"  vs Minimax: {comprehensive['vs_minimax']:.3f}")
        print(f"  è‡ªå°å¼ˆ: {comprehensive['self_play']:.3f}")
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
    
    print("\nâœ… è©•ä¼°æ¸¬è©¦å®Œæˆï¼")

def show_evaluation_options():
    """é¡¯ç¤ºè©•ä¼°é¸é …èªªæ˜"""
    print("\nğŸ“– è©•ä¼°æ¨¡å¼èªªæ˜")
    print("=" * 50)
    
    print("ğŸ² random: å°éš¨æ©Ÿå°æ‰‹è©•ä¼°")
    print("   - æœ€åŸºç¤çš„è©•ä¼°æ–¹å¼")
    print("   - é©åˆåˆæœŸè¨“ç·´ç›£æ§")
    
    print("\nğŸ¤– minimax: å°Minimaxç®—æ³•è©•ä¼°")
    print("   - æ¸¬è©¦å°ç­–ç•¥æ€§å°æ‰‹çš„è¡¨ç¾")
    print("   - æ›´æœ‰æŒ‘æˆ°æ€§çš„è©•ä¼°")
    
    print("\nğŸ“Š detailed: è©³ç´°æŒ‡æ¨™è©•ä¼°")
    print("   - æä¾›è±å¯Œçš„çµ±è¨ˆä¿¡æ¯")
    print("   - åŒ…æ‹¬éŠæˆ²é•·åº¦ã€å¿«é€Ÿç²å‹ç­‰")
    
    print("\nğŸ† comprehensive: ç¶œåˆè©•ä¼°")
    print("   - çµåˆå¤šç¨®å°æ‰‹çš„è©•ä¼°")
    print("   - æœ€å…¨é¢çš„æ€§èƒ½è©•ä¼°")
    
    print("\nâš™ï¸ é…ç½®æ–¹å¼:")
    print("åœ¨config.yamlä¸­è¨­ç½®:")
    print("evaluation:")
    print("  mode: comprehensive  # é¸æ“‡è©•ä¼°æ¨¡å¼")
    print("  weights:")
    print("    vs_random: 0.4")
    print("    vs_minimax: 0.4") 
    print("    self_play: 0.2")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¸¬è©¦è©•ä¼°ç³»çµ±")
    parser.add_argument("--test", action="store_true", help="é‹è¡Œè©•ä¼°æ¸¬è©¦")
    parser.add_argument("--help-eval", action="store_true", help="é¡¯ç¤ºè©•ä¼°é¸é …èªªæ˜")
    
    args = parser.parse_args()
    
    if args.help_eval:
        show_evaluation_options()
    elif args.test:
        test_evaluation_modes()
    else:
        print("ä½¿ç”¨ --test é‹è¡Œè©•ä¼°æ¸¬è©¦")
        print("ä½¿ç”¨ --help-eval æŸ¥çœ‹è©•ä¼°é¸é …èªªæ˜")
