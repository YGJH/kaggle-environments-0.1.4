#!/usr/bin/env python3
"""
Connect4 æ¨¡ä»¿å­¸ç¿’é è¨“ç·´
ä½¿ç”¨C4Solveré€²è¡Œç›£ç£å­¸ç¿’é è¨“ç·´ï¼Œç„¶å¾Œå¯æ¥å…¥RLè¨“ç·´
"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import yaml
import logging
from collections import deque
from datetime import datetime
from typing import List, Dict, Tuple
import time

# å°å…¥å¿…è¦çš„çµ„ä»¶
from train_connectx_rl_robust import ConnectXNet, PPOAgent
from c4solver_wrapper import get_c4solver, C4SolverWrapper
from kaggle_environments import make

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('imitation_learning.log')
    ]
)
logger = logging.getLogger(__name__)

class ImitationDataset:
    """æ¨¡ä»¿å­¸ç¿’æ•¸æ“šé›†"""
    
    def __init__(self, solver: C4SolverWrapper, max_moves: int = 20):
        """
        Args:
            solver: C4SolveråŒ…è£å™¨
            max_moves: æ¯å±€æœ€å¤§ç§»å‹•æ•¸(é¿å…éé•·çš„éŠæˆ²)
        """
        self.solver = solver
        self.max_moves = max_moves
        self.data = []
        
    def generate_position(self) -> Tuple[List[int], int]:
        """
        ç”Ÿæˆä¸€å€‹éš¨æ©Ÿå±€é¢
        
        Returns:
            (board, current_player): æ£‹ç›¤ç‹€æ…‹å’Œç•¶å‰ç©å®¶
        """
        env = make('connectx', debug=False)
        env.reset()
        
        board = [0] * 42
        current_player = 1
        move_count = 0
        
        # éš¨æ©ŸéŠæˆ²åˆ°æŒ‡å®šæ­¥æ•¸
        target_moves = random.randint(0, min(self.max_moves, 30))
        
        while move_count < target_moves and not env.done:
            # ç²å–æœ‰æ•ˆå‹•ä½œ
            valid_actions = [c for c in range(7) if board[c] == 0]
            if not valid_actions:
                break
                
            # éš¨æ©Ÿé¸æ“‡å‹•ä½œ
            action = random.choice(valid_actions)
            
            # åŸ·è¡Œå‹•ä½œ
            env.step([action if current_player == 1 else None, 
                     action if current_player == 2 else None])
            
            # æ›´æ–°æ£‹ç›¤
            board = env.state[0]['observation']['board']
            current_player = 3 - current_player
            move_count += 1
            
            # æª¢æŸ¥éŠæˆ²æ˜¯å¦çµæŸ
            if env.done:
                break
        
        return board, current_player
    
    def get_expert_action_distribution(self, board: List[int], current_player: int) -> np.ndarray:
        """
        ä½¿ç”¨C4Solverç²å–å°ˆå®¶å‹•ä½œåˆ†ä½ˆ
        
        Args:
            board: æ£‹ç›¤ç‹€æ…‹
            current_player: ç•¶å‰ç©å®¶(1æˆ–2)
            
        Returns:
            7ç¶­çš„å‹•ä½œæ¦‚ç‡åˆ†ä½ˆ
        """
        try:
            # ç²å–æœ‰æ•ˆå‹•ä½œ
            valid_actions = [c for c in range(7) if board[c] == 0]
            if not valid_actions:
                return np.ones(7) / 7  # å‡å‹»åˆ†ä½ˆä½œç‚ºfallback
            
            # ç²å–C4Solveråˆ†æçµæœ
            result = self.solver.evaluate_board(board, analyze=True)
            
            if not result['valid'] or len(result['scores']) != 7:
                # Fallback: å‡å‹»åˆ†ä½ˆæ–¼æœ‰æ•ˆå‹•ä½œ
                dist = np.zeros(7)
                for action in valid_actions:
                    dist[action] = 1.0 / len(valid_actions)
                return dist
            
            scores = np.array(result['scores'])
            
            # åªè€ƒæ…®æœ‰æ•ˆå‹•ä½œçš„åˆ†æ•¸
            valid_scores = np.full(7, -1000)  # ç„¡æ•ˆå‹•ä½œçµ¦æ¥µä½åˆ†
            for action in valid_actions:
                valid_scores[action] = scores[action]
            
            # è½‰æ›ç‚ºæ¦‚ç‡åˆ†ä½ˆ (softmax with temperature)
            temperature = 1.0
            exp_scores = np.exp(valid_scores / temperature)
            
            # ç¢ºä¿ç„¡æ•ˆå‹•ä½œæ¦‚ç‡ç‚º0
            for i in range(7):
                if i not in valid_actions:
                    exp_scores[i] = 0
            
            # æ­¸ä¸€åŒ–
            prob_dist = exp_scores / (np.sum(exp_scores) + 1e-8)
            
            return prob_dist
            
        except Exception as e:
            logger.warning(f"Expert action failed: {e}")
            # Fallback: ä¸­å¤®åå¥½åˆ†ä½ˆ
            dist = np.zeros(7)
            center_weights = [0.05, 0.1, 0.15, 0.4, 0.15, 0.1, 0.05]
            for i, action in enumerate([0, 1, 2, 3, 4, 5, 6]):
                if action in valid_actions:
                    dist[action] = center_weights[i]
            
            if np.sum(dist) > 0:
                dist = dist / np.sum(dist)
            else:
                dist = np.ones(7) / 7
                
            return dist
    
    def generate_training_samples(self, num_samples: int) -> List[Dict]:
        """
        ç”Ÿæˆè¨“ç·´æ¨£æœ¬
        
        Args:
            num_samples: æ¨£æœ¬æ•¸é‡
            
        Returns:
            è¨“ç·´æ¨£æœ¬åˆ—è¡¨
        """
        samples = []
        
        logger.info(f"ç”Ÿæˆ {num_samples} å€‹è¨“ç·´æ¨£æœ¬...")
        
        for i in range(num_samples):
            if i % 1000 == 0:
                logger.info(f"é€²åº¦: {i}/{num_samples}")
            
            try:
                # ç”Ÿæˆéš¨æ©Ÿå±€é¢
                board, current_player = self.generate_position()
                
                # æª¢æŸ¥éŠæˆ²æ˜¯å¦å·²çµæŸ
                env = make('connectx', debug=False)
                env.state = [{'observation': {'board': board, 'mark': current_player}}, 
                           {'observation': {'board': board, 'mark': 3-current_player}}]
                
                if env.done:
                    continue
                
                # ç·¨ç¢¼ç‹€æ…‹ (ä½¿ç”¨PPOAgentçš„ç·¨ç¢¼æ–¹å¼)
                encoded_state = self.encode_state_for_model(board, current_player)
                
                # ç²å–å°ˆå®¶å‹•ä½œåˆ†ä½ˆ
                expert_dist = self.get_expert_action_distribution(board, current_player)
                
                samples.append({
                    'state': encoded_state,
                    'action_dist': expert_dist,
                    'board': board.copy(),
                    'player': current_player
                })
                
            except Exception as e:
                logger.warning(f"Sample {i} generation failed: {e}")
                continue
        
        logger.info(f"æˆåŠŸç”Ÿæˆ {len(samples)} å€‹è¨“ç·´æ¨£æœ¬")
        return samples
    
    def encode_state_for_model(self, board: List[int], mark: int) -> np.ndarray:
        """
        ç·¨ç¢¼ç‹€æ…‹ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼ (126ç¶­)
        
        Args:
            board: æ£‹ç›¤ç‹€æ…‹
            mark: ç•¶å‰ç©å®¶æ¨™è¨˜
            
        Returns:
            ç·¨ç¢¼å¾Œçš„ç‹€æ…‹
        """
        try:
            # è½‰æ›ç‚º6x7æ ¼å¼
            grid = np.array(board).reshape(6, 7)
            
            # å‰µå»º3é€šé“ç·¨ç¢¼
            # é€šé“0: ç•¶å‰ç©å®¶çš„æ£‹å­
            # é€šé“1: å°æ‰‹çš„æ£‹å­  
            # é€šé“2: ç©ºä½
            encoded = np.zeros((3, 6, 7), dtype=np.float32)
            
            for r in range(6):
                for c in range(7):
                    if grid[r, c] == mark:
                        encoded[0, r, c] = 1.0
                    elif grid[r, c] == (3 - mark):
                        encoded[1, r, c] = 1.0
                    else:
                        encoded[2, r, c] = 1.0
            
            # å±•å¹³ç‚º126ç¶­
            return encoded.flatten()
            
        except Exception as e:
            logger.error(f"State encoding failed: {e}")
            return np.zeros(126, dtype=np.float32)


class ImitationLearner:
    """æ¨¡ä»¿å­¸ç¿’è¨“ç·´å™¨"""
    
    def __init__(self, config: Dict):
        """
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–C4Solver
        self.solver = get_c4solver()
        if self.solver is None:
            raise RuntimeError("ç„¡æ³•åˆå§‹åŒ–C4Solver")
        
        # åˆå§‹åŒ–æ•¸æ“šé›†
        self.dataset = ImitationDataset(self.solver)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = ConnectXNet(
            input_size=config['model']['input_size'],
            hidden_size=config['model']['hidden_size'], 
            num_layers=config['model']['num_layers']
        ).to(self.device)
        
        # å„ªåŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay']
        )
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.8,
            patience=5,
            verbose=True
        )
        
        # è¨“ç·´çµ±è¨ˆ
        self.training_stats = {
            'losses': [],
            'accuracies': [],
            'learning_rates': []
        }
        
    def load_model(self, model_path: str):
        """è¼‰å…¥é è¨“ç·´æ¨¡å‹"""
        if os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                if 'model_state_dict' in checkpoint:
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                    logger.info(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹: {model_path}")
                else:
                    self.model.load_state_dict(checkpoint)
                    logger.info(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹ç‹€æ…‹: {model_path}")
            except Exception as e:
                logger.warning(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
                logger.info("å°‡ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–çš„æ¨¡å‹")
        else:
            logger.info(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}ï¼Œä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–")
    
    def save_model(self, save_path: str, epoch: int = 0, loss: float = 0.0):
        """ä¿å­˜æ¨¡å‹"""
        try:
            checkpoint = {
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'epoch': epoch,
                'loss': loss,
                'config': self.config,
                'training_stats': self.training_stats
            }
            torch.save(checkpoint, save_path)
            logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜: {save_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±æ•—: {e}")
    
    def train_epoch(self, samples: List[Dict], batch_size: int) -> Tuple[float, float]:
        """
        è¨“ç·´ä¸€å€‹epoch
        
        Args:
            samples: è¨“ç·´æ¨£æœ¬
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            (average_loss, accuracy)
        """
        self.model.train()
        
        # æ‰“äº‚æ¨£æœ¬
        random.shuffle(samples)
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        for i in range(0, len(samples), batch_size):
            batch_samples = samples[i:i + batch_size]
            
            # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
            states = []
            target_dists = []
            
            for sample in batch_samples:
                states.append(sample['state'])
                target_dists.append(sample['action_dist'])
            
            # è½‰æ›ç‚ºå¼µé‡
            states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
            target_dists_tensor = torch.FloatTensor(np.array(target_dists)).to(self.device)
            
            # å‰å‘å‚³æ’­
            policy_probs, _ = self.model(states_tensor)
            
            # è¨ˆç®—æå¤± (äº¤å‰ç†µæå¤±)
            loss = nn.KLDivLoss(reduction='batchmean')(
                torch.log(policy_probs + 1e-8), 
                target_dists_tensor
            )
            
            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            # è¨ˆç®—æº–ç¢ºç‡ (é æ¸¬å‹•ä½œæ˜¯å¦èˆ‡æœ€ä½³å‹•ä½œä¸€è‡´)
            predicted_actions = torch.argmax(policy_probs, dim=1)
            target_actions = torch.argmax(target_dists_tensor, dim=1)
            accuracy = (predicted_actions == target_actions).float().mean().item()
            
            total_loss += loss.item()
            total_accuracy += accuracy
            num_batches += 1
        
        avg_loss = total_loss / max(1, num_batches)
        avg_accuracy = total_accuracy / max(1, num_batches)
        
        return avg_loss, avg_accuracy
    
    def evaluate(self, samples: List[Dict], batch_size: int) -> Tuple[float, float]:
        """
        è©•ä¼°æ¨¡å‹
        
        Args:
            samples: è©•ä¼°æ¨£æœ¬
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            (average_loss, accuracy)
        """
        self.model.eval()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for i in range(0, len(samples), batch_size):
                batch_samples = samples[i:i + batch_size]
                
                # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
                states = []
                target_dists = []
                
                for sample in batch_samples:
                    states.append(sample['state'])
                    target_dists.append(sample['action_dist'])
                
                # è½‰æ›ç‚ºå¼µé‡
                states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
                target_dists_tensor = torch.FloatTensor(np.array(target_dists)).to(self.device)
                
                # å‰å‘å‚³æ’­
                policy_probs, _ = self.model(states_tensor)
                
                # è¨ˆç®—æå¤±
                loss = nn.KLDivLoss(reduction='batchmean')(
                    torch.log(policy_probs + 1e-8), 
                    target_dists_tensor
                )
                
                # è¨ˆç®—æº–ç¢ºç‡
                predicted_actions = torch.argmax(policy_probs, dim=1)
                target_actions = torch.argmax(target_dists_tensor, dim=1)
                accuracy = (predicted_actions == target_actions).float().mean().item()
                
                total_loss += loss.item()
                total_accuracy += accuracy
                num_batches += 1
        
        avg_loss = total_loss / max(1, num_batches)
        avg_accuracy = total_accuracy / max(1, num_batches)
        
        return avg_loss, avg_accuracy
    
    def train(self, num_samples: int = 10000, num_epochs: int = 50, batch_size: int = 64, 
              val_split: float = 0.1, save_path: str = "imitation_model.pt"):
        """
        ä¸»è¨“ç·´å¾ªç’°
        
        Args:
            num_samples: è¨“ç·´æ¨£æœ¬æ•¸é‡
            num_epochs: è¨“ç·´è¼ªæ•¸
            batch_size: æ‰¹æ¬¡å¤§å°
            val_split: é©—è­‰é›†æ¯”ä¾‹
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾‘
        """
        logger.info("é–‹å§‹æ¨¡ä»¿å­¸ç¿’è¨“ç·´...")
        
        # ç”Ÿæˆè¨“ç·´æ•¸æ“š
        logger.info("ç”Ÿæˆè¨“ç·´æ•¸æ“š...")
        all_samples = self.dataset.generate_training_samples(num_samples)
        
        if len(all_samples) == 0:
            logger.error("ç„¡æ³•ç”Ÿæˆè¨“ç·´æ¨£æœ¬ï¼Œè¨“ç·´ä¸­æ­¢")
            return
        
        # åˆ†å‰²è¨“ç·´å’Œé©—è­‰é›†
        val_size = int(len(all_samples) * val_split)
        train_samples = all_samples[:-val_size] if val_size > 0 else all_samples
        val_samples = all_samples[-val_size:] if val_size > 0 else []
        
        logger.info(f"è¨“ç·´æ¨£æœ¬: {len(train_samples)}, é©—è­‰æ¨£æœ¬: {len(val_samples)}")
        
        best_val_loss = float('inf')
        patience_counter = 0
        max_patience = 10
        
        for epoch in range(num_epochs):
            start_time = time.time()
            
            # è¨“ç·´
            train_loss, train_acc = self.train_epoch(train_samples, batch_size)
            
            # é©—è­‰
            val_loss, val_acc = 0.0, 0.0
            if val_samples:
                val_loss, val_acc = self.evaluate(val_samples, batch_size)
            
            # æ›´æ–°å­¸ç¿’ç‡
            self.scheduler.step(val_loss if val_samples else train_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è¨˜éŒ„çµ±è¨ˆ
            self.training_stats['losses'].append(train_loss)
            self.training_stats['accuracies'].append(train_acc)
            self.training_stats['learning_rates'].append(current_lr)
            
            epoch_time = time.time() - start_time
            
            logger.info(
                f"Epoch {epoch+1}/{num_epochs}: "
                f"train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, "
                f"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}, "
                f"lr={current_lr:.2e}, time={epoch_time:.1f}s"
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_samples and val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_save_path = save_path.replace('.pt', '_best.pt')
                self.save_model(best_save_path, epoch, val_loss)
                logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹! é©—è­‰æå¤±: {val_loss:.4f}")
            else:
                patience_counter += 1
            
            # å®šæœŸä¿å­˜æª¢æŸ¥é»
            if (epoch + 1) % 10 == 0:
                checkpoint_path = save_path.replace('.pt', f'_epoch_{epoch+1}.pt')
                self.save_model(checkpoint_path, epoch, train_loss)
            
            # æ—©åœ
            if patience_counter >= max_patience:
                logger.info(f"Early stopping after {epoch+1} epochs (patience={max_patience})")
                break
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        self.save_model(save_path, num_epochs, train_loss)
        logger.info("âœ… æ¨¡ä»¿å­¸ç¿’è¨“ç·´å®Œæˆ!")


def load_config(config_path: str = "imitation_config.yaml") -> Dict:
    """è¼‰å…¥é…ç½®æ–‡ä»¶"""
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                logger.info(f"âœ… è¼‰å…¥é…ç½®æ–‡ä»¶: {config_path}")
                return config
        except Exception as e:
            logger.warning(f"è¼‰å…¥é…ç½®æ–‡ä»¶å¤±æ•—: {e}")
    
    # é»˜èªé…ç½®
    default_config = {
        'model': {
            'input_size': 126,
            'hidden_size': 192,
            'num_layers': 256
        },
        'training': {
            'learning_rate': 0.001,
            'weight_decay': 0.0001,
            'num_samples': 20000,
            'num_epochs': 100,
            'batch_size': 128,
            'val_split': 0.1
        },
        'paths': {
            'input_model': None,  # å¯é¸ï¼šè¼‰å…¥ç¾æœ‰æ¨¡å‹
            'output_model': 'imitation_pretrained_model.pt'
        },
        'c4solver': {
            'path': './c4solver',
            'timeout': 5.0,
            'max_depth': 20
        }
    }
    
    logger.info("ä½¿ç”¨é»˜èªé…ç½®")
    return default_config


def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸ¯ Connect4 æ¨¡ä»¿å­¸ç¿’é è¨“ç·´")
    logger.info("=" * 50)
    
    try:
        # è¼‰å…¥é…ç½®
        config = load_config()
        
        # å‰µå»ºæ¨¡ä»¿å­¸ç¿’å™¨
        learner = ImitationLearner(config)
        
        # è¼‰å…¥ç¾æœ‰æ¨¡å‹ (å¦‚æœæŒ‡å®š)
        input_model_path = config.get('paths', {}).get('input_model')
        if input_model_path:
            learner.load_model(input_model_path)
        
        # é–‹å§‹è¨“ç·´
        training_config = config['training']
        output_path = config.get('paths', {}).get('output_model', 'imitation_pretrained_model.pt')
        
        learner.train(
            num_samples=training_config.get('num_samples', 20000),
            num_epochs=training_config.get('num_epochs', 100),
            batch_size=training_config.get('batch_size', 128),
            val_split=training_config.get('val_split', 0.1),
            save_path=output_path
        )
        
        logger.info("ğŸ‰ æ¨¡ä»¿å­¸ç¿’é è¨“ç·´å®Œæˆ!")
        logger.info(f"æ¨¡å‹å·²ä¿å­˜: {output_path}")
        
    except Exception as e:
        logger.error(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
