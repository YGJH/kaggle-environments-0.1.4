{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from kaggle_environments import make, evaluate\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# 设置matplotlib中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS'] \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"导入完成！\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# 设置随机种子\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectXNet(nn.Module):\n",
    "    \"\"\"ConnectX神经网络模型\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=42, hidden_size=512, output_size=7):\n",
    "        super(ConnectXNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 特征提取层\n",
    "        self.feature_layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # 策略头 (选择动作的概率分布)\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, output_size)\n",
    "        )\n",
    "        \n",
    "        # 价值头 (状态价值评估)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_layers(x)\n",
    "        policy = self.policy_head(features)\n",
    "        value = self.value_head(features)\n",
    "        return policy, value\n",
    "\n",
    "# 测试模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConnectXNet().to(device)\n",
    "print(f\"模型已创建，使用设备: {device}\")\n",
    "print(f\"模型参数数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 测试前向传播\n",
    "test_input = torch.randn(1, 42).to(device)\n",
    "with torch.no_grad():\n",
    "    policy, value = model(test_input)\n",
    "    print(f\"策略输出形状: {policy.shape}, 价值输出形状: {value.shape}\")\n",
    "    print(f\"策略输出: {policy}\")\n",
    "    print(f\"价值输出: {value}\")\n",
    "\n",
    "del test_input, policy, value  # 清理内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BattleVisualizer:\n",
    "    \"\"\"对战可视化器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.battle_history = []\n",
    "        self.performance_data = {\n",
    "            'episode': [],\n",
    "            'agent1_wins': [],\n",
    "            'agent2_wins': [],\n",
    "            'draws': [],\n",
    "            'avg_game_length': []\n",
    "        }\n",
    "        \n",
    "    def render_board(self, board, title=\"ConnectX Board\"):\n",
    "        \"\"\"渲染游戏板\"\"\"\n",
    "        board_2d = np.array(board).reshape(6, 7)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # 创建颜色映射\n",
    "        colors = ['white', 'red', 'yellow']\n",
    "        cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "        \n",
    "        plt.imshow(board_2d, cmap=cmap, vmin=0, vmax=2)\n",
    "        \n",
    "        # 添加网格线\n",
    "        for i in range(7):\n",
    "            plt.axvline(x=i-0.5, color='black', linewidth=2)\n",
    "        for i in range(6):\n",
    "            plt.axhline(y=i-0.5, color='black', linewidth=2)\n",
    "            \n",
    "        # 在格子中显示棋子\n",
    "        for i in range(6):\n",
    "            for j in range(7):\n",
    "                if board_2d[i, j] == 1:\n",
    "                    plt.text(j, i, '●', fontsize=20, ha='center', va='center', color='darkred')\n",
    "                elif board_2d[i, j] == 2:\n",
    "                    plt.text(j, i, '●', fontsize=20, ha='center', va='center', color='orange')\n",
    "        \n",
    "        plt.title(title, fontsize=16, fontweight='bold')\n",
    "        plt.xticks(range(7), [f'Col {i}' for i in range(7)])\n",
    "        plt.yticks(range(6), [f'Row {i}' for i in range(6)])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def show_game_replay(self, env_steps, agent1_name=\"Agent1\", agent2_name=\"Agent2\"):\n",
    "        \"\"\"显示游戏回放\"\"\"\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"🎮 对战回放: {agent1_name} vs {agent2_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for step_idx, step in enumerate(env_steps):\n",
    "            if step_idx == 0:\n",
    "                print(f\"\\\\n初始状态:\")\n",
    "                self.render_board(step[0]['board'])\n",
    "            else:\n",
    "                player = step[0]['mark']\n",
    "                action = step[1]\n",
    "                board = step[0]['board']\n",
    "                \n",
    "                player_name = agent1_name if player == 1 else agent2_name\n",
    "                color = \"🔴\" if player == 1 else \"🟡\"\n",
    "                \n",
    "                print(f\"\\\\n回合 {step_idx}: {color} {player_name} 在第 {action} 列落子\")\n",
    "                self.render_board(board, f\"回合 {step_idx} - {player_name} 的行动\")\n",
    "                \n",
    "                # 检查游戏是否结束\n",
    "                if len(env_steps) == step_idx + 1:\n",
    "                    # 判断游戏结果\n",
    "                    if self.check_winner(board) == 1:\n",
    "                        print(f\"\\\\n🏆 {agent1_name} 获胜！\")\n",
    "                    elif self.check_winner(board) == 2:\n",
    "                        print(f\"\\\\n🏆 {agent2_name} 获胜！\")\n",
    "                    else:\n",
    "                        print(f\"\\\\n🤝 平局！\")\n",
    "    \n",
    "    def check_winner(self, board):\n",
    "        \"\"\"检查获胜者\"\"\"\n",
    "        board_2d = np.array(board).reshape(6, 7)\n",
    "        \n",
    "        # 检查水平、垂直和对角线\n",
    "        for player in [1, 2]:\n",
    "            # 水平检查\n",
    "            for row in range(6):\n",
    "                for col in range(4):\n",
    "                    if all(board_2d[row, col+i] == player for i in range(4)):\n",
    "                        return player\n",
    "            \n",
    "            # 垂直检查\n",
    "            for row in range(3):\n",
    "                for col in range(7):\n",
    "                    if all(board_2d[row+i, col] == player for i in range(4)):\n",
    "                        return player\n",
    "            \n",
    "            # 对角线检查（左上到右下）\n",
    "            for row in range(3):\n",
    "                for col in range(4):\n",
    "                    if all(board_2d[row+i, col+i] == player for i in range(4)):\n",
    "                        return player\n",
    "            \n",
    "            # 对角线检查（右上到左下）\n",
    "            for row in range(3):\n",
    "                for col in range(3, 7):\n",
    "                    if all(board_2d[row+i, col-i] == player for i in range(4)):\n",
    "                        return player\n",
    "        \n",
    "        return 0  # 无获胜者\n",
    "    \n",
    "    def plot_performance_stats(self):\n",
    "        \"\"\"绘制性能统计图\"\"\"\n",
    "        if not self.performance_data['episode']:\n",
    "            print(\"暂无性能数据\")\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        episodes = self.performance_data['episode']\n",
    "        \n",
    "        # 胜率趋势\n",
    "        axes[0, 0].plot(episodes, self.performance_data['agent1_wins'], 'r-o', label='Agent1 胜率', alpha=0.7)\n",
    "        axes[0, 0].plot(episodes, self.performance_data['agent2_wins'], 'b-o', label='Agent2 胜率', alpha=0.7)\n",
    "        axes[0, 0].plot(episodes, self.performance_data['draws'], 'g-o', label='平局率', alpha=0.7)\n",
    "        axes[0, 0].set_title('胜率趋势')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('胜率')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 平均游戏长度\n",
    "        axes[0, 1].plot(episodes, self.performance_data['avg_game_length'], 'purple', marker='s')\n",
    "        axes[0, 1].set_title('平均游戏长度')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('平均回合数')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 累计胜负统计\n",
    "        cumulative_wins1 = np.cumsum(self.performance_data['agent1_wins'])\n",
    "        cumulative_wins2 = np.cumsum(self.performance_data['agent2_wins'])\n",
    "        cumulative_draws = np.cumsum(self.performance_data['draws'])\n",
    "        \n",
    "        axes[1, 0].plot(episodes, cumulative_wins1, 'r-', label='Agent1 累计胜场')\n",
    "        axes[1, 0].plot(episodes, cumulative_wins2, 'b-', label='Agent2 累计胜场')\n",
    "        axes[1, 0].plot(episodes, cumulative_draws, 'g-', label='累计平局')\n",
    "        axes[1, 0].set_title('累计胜负统计')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('累计场次')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 最近的胜率分布\n",
    "        if len(episodes) > 0:\n",
    "            recent_data = [\n",
    "                self.performance_data['agent1_wins'][-1] if self.performance_data['agent1_wins'] else 0,\n",
    "                self.performance_data['agent2_wins'][-1] if self.performance_data['agent2_wins'] else 0,\n",
    "                self.performance_data['draws'][-1] if self.performance_data['draws'] else 0\n",
    "            ]\n",
    "            labels = ['Agent1', 'Agent2', '平局']\n",
    "            colors = ['red', 'blue', 'green']\n",
    "            axes[1, 1].pie(recent_data, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "            axes[1, 1].set_title('最新胜率分布')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 创建可视化器实例\n",
    "visualizer = BattleVisualizer()\n",
    "print(\"对战可视化器已创建！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectXAgent:\n",
    "    \"\"\"ConnectX智能体基类\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Agent\"):\n",
    "        self.name = name\n",
    "        self.wins = 0\n",
    "        self.losses = 0\n",
    "        self.draws = 0\n",
    "    \n",
    "    def get_action(self, observation, configuration):\n",
    "        \"\"\"获取下一步行动\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"重置统计数据\"\"\"\n",
    "        self.wins = self.losses = self.draws = 0\n",
    "\n",
    "class NeuralNetworkAgent(ConnectXAgent):\n",
    "    \"\"\"基于神经网络的智能体\"\"\"\n",
    "    \n",
    "    def __init__(self, model, name=\"NeuralAgent\", use_exploration=True, epsilon=0.1):\n",
    "        super().__init__(name)\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.use_exploration = use_exploration\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def board_to_input(self, board, mark):\n",
    "        \"\"\"将游戏板转换为神经网络输入\"\"\"\n",
    "        # 将对手标记为-1，自己标记为1，空位为0\n",
    "        processed_board = []\n",
    "        for cell in board:\n",
    "            if cell == 0:\n",
    "                processed_board.append(0.0)\n",
    "            elif cell == mark:\n",
    "                processed_board.append(1.0)\n",
    "            else:\n",
    "                processed_board.append(-1.0)\n",
    "        \n",
    "        return torch.FloatTensor(processed_board).unsqueeze(0).to(self.device)\n",
    "    \n",
    "    def get_valid_actions(self, board):\n",
    "        \"\"\"获取有效动作列表\"\"\"\n",
    "        return [col for col in range(7) if board[col] == 0]\n",
    "    \n",
    "    def get_action(self, observation, configuration):\n",
    "        \"\"\"获取下一步行动\"\"\"\n",
    "        board = observation.board\n",
    "        mark = observation.mark\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(board)\n",
    "        if not valid_actions:\n",
    "            return random.choice(range(7))  # 应该不会发生\n",
    "        \n",
    "        # 使用神经网络预测\n",
    "        with torch.no_grad():\n",
    "            board_input = self.board_to_input(board, mark)\n",
    "            policy, value = self.model(board_input)\n",
    "            \n",
    "            # 应用softmax获得动作概率\n",
    "            action_probs = F.softmax(policy, dim=-1).cpu().numpy()[0]\n",
    "            \n",
    "            # 只考虑有效动作\n",
    "            valid_probs = [(action, action_probs[action]) for action in valid_actions]\n",
    "            valid_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # 探索 vs 利用\n",
    "            if self.use_exploration and random.random() < self.epsilon:\n",
    "                # 探索：随机选择有效动作\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                # 利用：选择概率最高的动作\n",
    "                action = valid_probs[0][0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "class DataBasedAgent(ConnectXAgent):\n",
    "    \"\"\"基于数据集的智能体\"\"\"\n",
    "    \n",
    "    def __init__(self, data_file=\"connectx-state-action-value.txt\", name=\"DataAgent\"):\n",
    "        super().__init__(name)\n",
    "        self.state_values = {}\n",
    "        self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        \"\"\"加载状态-动作-价值数据\"\"\"\n",
    "        try:\n",
    "            print(f\"正在加载数据文件: {data_file}\")\n",
    "            with open(data_file, 'r') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    if line_num > 100000:  # 限制数据量\n",
    "                        break\n",
    "                        \n",
    "                    parts = line.strip().split(',')\n",
    "                    if len(parts) == 8:  # 42字符状态 + 7个动作价值\n",
    "                        state = parts[0]\n",
    "                        values = [float(v) for v in parts[1:]]\n",
    "                        self.state_values[state] = values\n",
    "                        \n",
    "                        if line_num % 10000 == 0:\n",
    "                            print(f\"已加载 {line_num} 行数据\")\n",
    "            \n",
    "            print(f\"数据加载完成！总共加载了 {len(self.state_values)} 个状态\")\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"警告：数据文件 {data_file} 未找到，将使用随机策略\")\n",
    "            self.state_values = {}\n",
    "        except Exception as e:\n",
    "            print(f\"加载数据时出错: {e}\")\n",
    "            self.state_values = {}\n",
    "    \n",
    "    def board_to_state_string(self, board, mark):\n",
    "        \"\"\"将游戏板转换为状态字符串\"\"\"\n",
    "        # 转换为数据集格式：自己是1，对手是2，空位是0\n",
    "        state_chars = []\n",
    "        for cell in board:\n",
    "            if cell == 0:\n",
    "                state_chars.append('0')\n",
    "            elif cell == mark:\n",
    "                state_chars.append('1')\n",
    "            else:\n",
    "                state_chars.append('2')\n",
    "        return ''.join(state_chars)\n",
    "    \n",
    "    def get_valid_actions(self, board):\n",
    "        \"\"\"获取有效动作列表\"\"\"\n",
    "        return [col for col in range(7) if board[col] == 0]\n",
    "    \n",
    "    def get_action(self, observation, configuration):\n",
    "        \"\"\"根据数据集获取最佳动作\"\"\"\n",
    "        board = observation.board\n",
    "        mark = observation.mark\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(board)\n",
    "        if not valid_actions:\n",
    "            return random.choice(range(7))\n",
    "        \n",
    "        # 生成状态字符串\n",
    "        state_str = self.board_to_state_string(board, mark)\n",
    "        \n",
    "        if state_str in self.state_values:\n",
    "            # 使用数据集中的价值\n",
    "            action_values = self.state_values[state_str]\n",
    "            # 找到价值最高的有效动作\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            \n",
    "            for action in valid_actions:\n",
    "                if action_values[action] > best_value:\n",
    "                    best_value = action_values[action]\n",
    "                    best_action = action\n",
    "            \n",
    "            return best_action if best_action is not None else random.choice(valid_actions)\n",
    "        else:\n",
    "            # 状态未在数据集中，使用启发式规则\n",
    "            return self.heuristic_action(board, mark, valid_actions)\n",
    "    \n",
    "    def heuristic_action(self, board, mark, valid_actions):\n",
    "        \"\"\"启发式动作选择\"\"\"\n",
    "        # 简单的启发式：优先选择中间列\n",
    "        center_preference = [3, 2, 4, 1, 5, 0, 6]\n",
    "        for col in center_preference:\n",
    "            if col in valid_actions:\n",
    "                return col\n",
    "        return random.choice(valid_actions)\n",
    "\n",
    "# 创建不同类型的智能体用于测试\n",
    "print(\"智能体类定义完成！\")\n",
    "\n",
    "# 创建基于数据集的智能体\n",
    "data_agent = DataBasedAgent(name=\"DataAgent\")\n",
    "print(f\"数据代理创建完成，加载了 {len(data_agent.state_values)} 个状态\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc799483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_battle_with_visualization(agent1, agent2, visualizer, show_replay=True):\n",
    "    \"\"\"运行单场对战并可视化\"\"\"\n",
    "    \n",
    "    # 创建环境\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    \n",
    "    # 设置智能体函数\n",
    "    def agent1_func(observation, configuration):\n",
    "        return agent1.get_action(observation, configuration)\n",
    "    \n",
    "    def agent2_func(observation, configuration):\n",
    "        return agent2.get_action(observation, configuration)\n",
    "    \n",
    "    # 运行游戏\n",
    "    env_steps = env.run([agent1_func, agent2_func])\n",
    "    \n",
    "    if show_replay:\n",
    "        visualizer.show_game_replay(env_steps, agent1.name, agent2.name)\n",
    "    \n",
    "    # 分析结果\n",
    "    if len(env_steps) > 0:\n",
    "        final_step = env_steps[-1]\n",
    "        final_board = final_step[0]['board']\n",
    "        winner = visualizer.check_winner(final_board)\n",
    "        game_length = len(env_steps) - 1  # 减去初始状态\n",
    "        \n",
    "        return winner, game_length\n",
    "    \n",
    "    return 0, 0\n",
    "\n",
    "def run_training_with_battles(agent1, agent2, visualizer, total_episodes=500, \n",
    "                             battle_interval=50, games_per_battle=10):\n",
    "    \"\"\"运行训练并每隔指定episode显示对战\"\"\"\n",
    "    \n",
    "    print(f\"\\\\n{'='*80}\")\n",
    "    print(f\"🚀 开始训练！总共 {total_episodes} episodes\")\n",
    "    print(f\"📊 每 {battle_interval} episodes 进行一次对战展示 (每次 {games_per_battle} 场游戏)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    stats = {\n",
    "        'agent1_total_wins': 0,\n",
    "        'agent2_total_wins': 0,\n",
    "        'total_draws': 0,\n",
    "        'total_games': 0\n",
    "    }\n",
    "    \n",
    "    for episode in range(0, total_episodes, battle_interval):\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"📍 Episode {episode}-{min(episode + battle_interval - 1, total_episodes - 1)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # 运行多场游戏进行统计\n",
    "        episode_stats = {\n",
    "            'agent1_wins': 0,\n",
    "            'agent2_wins': 0,\n",
    "            'draws': 0,\n",
    "            'game_lengths': []\n",
    "        }\n",
    "        \n",
    "        # 快速批量测试（不显示详细过程）\n",
    "        print(f\"\\\\n🎯 正在进行 {games_per_battle} 场快速对战...\")\n",
    "        for game in range(games_per_battle):\n",
    "            winner, game_length = run_battle_with_visualization(\n",
    "                agent1, agent2, visualizer, show_replay=False\n",
    "            )\n",
    "            \n",
    "            episode_stats['game_lengths'].append(game_length)\n",
    "            \n",
    "            if winner == 1:\n",
    "                episode_stats['agent1_wins'] += 1\n",
    "                agent1.wins += 1\n",
    "                agent2.losses += 1\n",
    "            elif winner == 2:\n",
    "                episode_stats['agent2_wins'] += 1\n",
    "                agent2.wins += 1\n",
    "                agent1.losses += 1\n",
    "            else:\n",
    "                episode_stats['draws'] += 1\n",
    "                agent1.draws += 1\n",
    "                agent2.draws += 1\n",
    "        \n",
    "        # 计算统计数据\n",
    "        agent1_winrate = episode_stats['agent1_wins'] / games_per_battle * 100\n",
    "        agent2_winrate = episode_stats['agent2_wins'] / games_per_battle * 100\n",
    "        draw_rate = episode_stats['draws'] / games_per_battle * 100\n",
    "        avg_game_length = np.mean(episode_stats['game_lengths'])\n",
    "        \n",
    "        # 更新总统计\n",
    "        stats['agent1_total_wins'] += episode_stats['agent1_wins']\n",
    "        stats['agent2_total_wins'] += episode_stats['agent2_wins']\n",
    "        stats['total_draws'] += episode_stats['draws']\n",
    "        stats['total_games'] += games_per_battle\n",
    "        \n",
    "        # 显示统计结果\n",
    "        print(f\"\\\\n📈 本轮统计结果:\")\n",
    "        print(f\"  🔴 {agent1.name}: {episode_stats['agent1_wins']}/{games_per_battle} 胜 ({agent1_winrate:.1f}%)\")\n",
    "        print(f\"  🔵 {agent2.name}: {episode_stats['agent2_wins']}/{games_per_battle} 胜 ({agent2_winrate:.1f}%)\")\n",
    "        print(f\"  🤝 平局: {episode_stats['draws']}/{games_per_battle} 场 ({draw_rate:.1f}%)\")\n",
    "        print(f\"  ⏱️  平均游戏长度: {avg_game_length:.1f} 回合\")\n",
    "        \n",
    "        # 保存性能数据用于绘图\n",
    "        visualizer.performance_data['episode'].append(episode + battle_interval)\n",
    "        visualizer.performance_data['agent1_wins'].append(agent1_winrate / 100)\n",
    "        visualizer.performance_data['agent2_wins'].append(agent2_winrate / 100) \n",
    "        visualizer.performance_data['draws'].append(draw_rate / 100)\n",
    "        visualizer.performance_data['avg_game_length'].append(avg_game_length)\n",
    "        \n",
    "        # 显示一场详细的对战过程\n",
    "        print(f\"\\\\n🎮 展示一场详细对战过程:\")\n",
    "        run_battle_with_visualization(agent1, agent2, visualizer, show_replay=True)\n",
    "        \n",
    "        # 显示累计统计\n",
    "        total_winrate1 = stats['agent1_total_wins'] / stats['total_games'] * 100\\n        total_winrate2 = stats['agent2_total_wins'] / stats['total_games'] * 100\\n        total_drawrate = stats['total_draws'] / stats['total_games'] * 100\\n        \\n        print(f\\\"\\\\n📊 累计统计 (总共 {stats['total_games']} 场游戏):\\\")\\n        print(f\\\"  🔴 {agent1.name}: {stats['agent1_total_wins']} 胜 ({total_winrate1:.1f}%)\\\")\\n        print(f\\\"  🔵 {agent2.name}: {stats['agent2_total_wins']} 胜 ({total_winrate2:.1f}%)\\\")\\n        print(f\\\"  🤝 平局: {stats['total_draws']} 场 ({total_drawrate:.1f}%)\\\")\\n        \\n        # 绘制性能图表\\n        print(f\\\"\\\\n📈 绘制性能趋势图...\\\")\\n        visualizer.plot_performance_stats()\\n        \\n        # 等待用户确认继续\\n        if episode + battle_interval < total_episodes:\\n            input(f\\\"\\\\n⏸️  按 Enter 键继续下一轮训练...\\\")\\n            clear_output(wait=True)  # 清理输出，保持界面整洁\\n    \\n    print(f\\\"\\\\n{'='*80}\\\")\\n    print(f\\\"🏁 训练完成！\\\")\\n    print(f\\\"📊 最终统计 (总共 {stats['total_games']} 场游戏):\\\")\\n    print(f\\\"  🔴 {agent1.name}: {stats['agent1_total_wins']} 胜 ({stats['agent1_total_wins']/stats['total_games']*100:.1f}%)\\\")\\n    print(f\\\"  🔵 {agent2.name}: {stats['agent2_total_wins']} 胜 ({stats['agent2_total_wins']/stats['total_games']*100:.1f}%)\\\")\\n    print(f\\\"  🤝 平局: {stats['total_draws']} 场 ({stats['total_draws']/stats['total_games']*100:.1f}%)\\\")\\n    print(f\\\"{'='*80}\\\")\\n    \\n    return stats\\n\\nprint(\\\"训练和对战功能已准备完成！\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎮 运行训练和对战展示！\\n# 这个cell会每50个episode展示一次对战过程\\n\\n# 创建神经网络智能体\\nneural_agent = NeuralNetworkAgent(model, name=\\\"NeuralAgent\\\", epsilon=0.2)\\n\\n# 选择对手 (可以是数据代理、随机代理等)\\nopponent_options = {\\n    \\\"data\\\": data_agent,\\n    \\\"random\\\": ConnectXAgent(\\\"RandomAgent\\\")  # 这里可以实现随机代理\\n}\\n\\nprint(\\\"可选择的对手:\\\")\\nfor key, agent in opponent_options.items():\\n    print(f\\\"  {key}: {agent.name}\\\")\\n\\n# 设置对手\\nopponent = data_agent  # 默认使用数据代理作为对手\\n\\nprint(f\\\"\\\\n🤖 选择的智能体组合:\\\")\\nprint(f\\\"  玩家1: {neural_agent.name} (神经网络)\\\")\\nprint(f\\\"  玩家2: {opponent.name} (基于数据集)\\\")\\n\\n# 开始训练！\\nprint(f\\\"\\\\n准备开始训练... 你可以修改以下参数:\\\")\\nprint(f\\\"  - total_episodes: 总的训练轮数\\\")\\nprint(f\\\"  - battle_interval: 每隔多少episode展示一次对战\\\")\\nprint(f\\\"  - games_per_battle: 每次展示时运行多少场游戏进行统计\\\")\\nprint(f\\\"\\\\n运行下面的代码开始训练:\\\")\\nprint(f\\\"final_stats = run_training_with_battles(\\\")\\nprint(f\\\"    agent1=neural_agent,\\\")\\nprint(f\\\"    agent2=opponent,\\\")\\nprint(f\\\"    visualizer=visualizer,\\\")\\nprint(f\\\"    total_episodes=200,    # 总共200轮\\\")\\nprint(f\\\"    battle_interval=50,    # 每50轮展示一次\\\")\\nprint(f\\\"    games_per_battle=10    # 每次展示10场游戏\\\")\\nprint(f\\\")\\\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 执行训练！\\n# 运行这个cell开始实际的训练过程\\n\\ntry:\\n    final_stats = run_training_with_battles(\\n        agent1=neural_agent,\\n        agent2=opponent, \\n        visualizer=visualizer,\\n        total_episodes=200,     # 总共200个episode\\n        battle_interval=50,     # 每50个episode展示一次对战\\n        games_per_battle=10     # 每次展示运行10场游戏统计\\n    )\\n    \\n    print(\\\"\\\\n🎉 训练完成！\\\")\\n    print(f\\\"最终统计结果: {final_stats}\\\")\\n    \\nexcept KeyboardInterrupt:\\n    print(\\\"\\\\n⏹️ 训练被用户中断\\\")\\nexcept Exception as e:\\n    print(f\\\"\\\\n❌ 训练过程中出现错误: {e}\\\")\\n    import traceback\\n    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fe37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 额外测试和分析\\n\\n# 1. 单独测试一场游戏\\nprint(\\\"=\\\" * 50)\\nprint(\\\"🎯 单场对战测试\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# 运行单场游戏并显示详细过程\\nwinner, game_length = run_battle_with_visualization(neural_agent, data_agent, visualizer)\\n\\nif winner == 1:\\n    print(f\\\"\\\\n🏆 {neural_agent.name} 获胜！游戏长度: {game_length} 回合\\\")\\nelif winner == 2:\\n    print(f\\\"\\\\n🏆 {data_agent.name} 获胜！游戏长度: {game_length} 回合\\\")\\nelse:\\n    print(f\\\"\\\\n🤝 平局！游戏长度: {game_length} 回合\\\")\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f291629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 性能分析和最终总结\\n\\n# 显示最终的性能图表\\nif visualizer.performance_data['episode']:\\n    print(\\\"📈 显示完整的性能趋势图...\\\")\\n    visualizer.plot_performance_stats()\\nelse:\\n    print(\\\"⚠️ 还没有性能数据，请先运行训练\\\")\\n\\n# 智能体统计信息\\nprint(\\\"\\\\n🤖 智能体统计信息:\\\")\\nprint(f\\\"  {neural_agent.name}:\\\")\\nprint(f\\\"    胜: {neural_agent.wins}, 负: {neural_agent.losses}, 平: {neural_agent.draws}\\\")\\nprint(f\\\"  {data_agent.name}:\\\")\\nprint(f\\\"    胜: {data_agent.wins}, 负: {data_agent.losses}, 平: {data_agent.draws}\\\")\\n\\n# 使用说明\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*80)\\nprint(\\\"📖 使用说明\\\")\\nprint(\\\"=\\\"*80)\\nprint(\\\"\\\"\\\"\\n🎯 这个notebook的主要功能:\\n1. 创建基于神经网络的ConnectX智能体\\n2. 创建基于数据集的ConnectX智能体  \\n3. 每50个episode展示一次详细的对战过程\\n4. 实时显示性能统计图表\\n5. 可视化游戏板和对战过程\\n\\n🔧 可以调整的参数:\\n- total_episodes: 总的训练轮数\\n- battle_interval: 每隔多少episode展示对战\\n- games_per_battle: 每次统计的游戏场数\\n- epsilon: 神经网络智能体的探索率\\n\\n🎮 支持的智能体类型:\\n- NeuralNetworkAgent: 基于PyTorch神经网络\\n- DataBasedAgent: 基于connectx-state-action-value.txt数据集\\n- 可以轻松扩展其他类型的智能体\\n\\n📊 可视化功能:\\n- 实时显示游戏板状态\\n- 对战过程回放\\n- 胜率趋势图\\n- 游戏长度统计\\n- 累计性能分析\\n\\n💡 提示:\\n- 每次对战展示后会暂停，按Enter继续\\n- 可以随时中断训练（Ctrl+C）\\n- 所有统计数据都会保存在visualizer中\\n\\\"\\\"\\\")\\nprint(\\\"=\\\"*80)\\n\\nprint(\\\"\\\\n🎉 ConnectX训练和对战可视化系统已准备就绪！\\\")\\nprint(\\\"现在可以运行训练cell来开始训练，或者单独运行测试cell来观看对战。\\\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
