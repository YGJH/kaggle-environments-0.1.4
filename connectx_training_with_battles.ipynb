{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from kaggle_environments import make, evaluate\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS'] \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"å¯¼å…¥å®Œæˆï¼\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectXNet(nn.Module):\n",
    "    \"\"\"ConnectXç¥ç»ç½‘ç»œæ¨¡å‹\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=42, hidden_size=512, output_size=7):\n",
    "        super(ConnectXNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # ç‰¹å¾æå–å±‚\n",
    "        self.feature_layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # ç­–ç•¥å¤´ (é€‰æ‹©åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ)\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, output_size)\n",
    "        )\n",
    "        \n",
    "        # ä»·å€¼å¤´ (çŠ¶æ€ä»·å€¼è¯„ä¼°)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # åˆå§‹åŒ–æƒé‡\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_layers(x)\n",
    "        policy = self.policy_head(features)\n",
    "        value = self.value_head(features)\n",
    "        return policy, value\n",
    "\n",
    "# æµ‹è¯•æ¨¡å‹\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConnectXNet().to(device)\n",
    "print(f\"æ¨¡å‹å·²åˆ›å»ºï¼Œä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "print(f\"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# æµ‹è¯•å‰å‘ä¼ æ’­\n",
    "test_input = torch.randn(1, 42).to(device)\n",
    "with torch.no_grad():\n",
    "    policy, value = model(test_input)\n",
    "    print(f\"ç­–ç•¥è¾“å‡ºå½¢çŠ¶: {policy.shape}, ä»·å€¼è¾“å‡ºå½¢çŠ¶: {value.shape}\")\n",
    "    print(f\"ç­–ç•¥è¾“å‡º: {policy}\")\n",
    "    print(f\"ä»·å€¼è¾“å‡º: {value}\")\n",
    "\n",
    "del test_input, policy, value  # æ¸…ç†å†…å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BattleVisualizer:\n",
    "    \"\"\"å¯¹æˆ˜å¯è§†åŒ–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.battle_history = []\n",
    "        self.performance_data = {\n",
    "            'episode': [],\n",
    "            'agent1_wins': [],\n",
    "            'agent2_wins': [],\n",
    "            'draws': [],\n",
    "            'avg_game_length': []\n",
    "        }\n",
    "        \n",
    "    def render_board(self, board, title=\"ConnectX Board\"):\n",
    "        \"\"\"æ¸²æŸ“æ¸¸æˆæ¿\"\"\"\n",
    "        board_2d = np.array(board).reshape(6, 7)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # åˆ›å»ºé¢œè‰²æ˜ å°„\n",
    "        colors = ['white', 'red', 'yellow']\n",
    "        cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "        \n",
    "        plt.imshow(board_2d, cmap=cmap, vmin=0, vmax=2)\n",
    "        \n",
    "        # æ·»åŠ ç½‘æ ¼çº¿\n",
    "        for i in range(7):\n",
    "            plt.axvline(x=i-0.5, color='black', linewidth=2)\n",
    "        for i in range(6):\n",
    "            plt.axhline(y=i-0.5, color='black', linewidth=2)\n",
    "            \n",
    "        # åœ¨æ ¼å­ä¸­æ˜¾ç¤ºæ£‹å­\n",
    "        for i in range(6):\n",
    "            for j in range(7):\n",
    "                if board_2d[i, j] == 1:\n",
    "                    plt.text(j, i, 'â—', fontsize=20, ha='center', va='center', color='darkred')\n",
    "                elif board_2d[i, j] == 2:\n",
    "                    plt.text(j, i, 'â—', fontsize=20, ha='center', va='center', color='orange')\n",
    "        \n",
    "        plt.title(title, fontsize=16, fontweight='bold')\n",
    "        plt.xticks(range(7), [f'Col {i}' for i in range(7)])\n",
    "        plt.yticks(range(6), [f'Row {i}' for i in range(6)])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def show_game_replay(self, env_steps, agent1_name=\"Agent1\", agent2_name=\"Agent2\"):\n",
    "        \"\"\"æ˜¾ç¤ºæ¸¸æˆå›æ”¾\"\"\"\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"ğŸ® å¯¹æˆ˜å›æ”¾: {agent1_name} vs {agent2_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for step_idx, step in enumerate(env_steps):\n",
    "            if step_idx == 0:\n",
    "                print(f\"\\\\nåˆå§‹çŠ¶æ€:\")\n",
    "                self.render_board(step[0]['board'])\n",
    "            else:\n",
    "                player = step[0]['mark']\n",
    "                action = step[1]\n",
    "                board = step[0]['board']\n",
    "                \n",
    "                player_name = agent1_name if player == 1 else agent2_name\n",
    "                color = \"ğŸ”´\" if player == 1 else \"ğŸŸ¡\"\n",
    "                \n",
    "                print(f\"\\\\nå›åˆ {step_idx}: {color} {player_name} åœ¨ç¬¬ {action} åˆ—è½å­\")\n",
    "                self.render_board(board, f\"å›åˆ {step_idx} - {player_name} çš„è¡ŒåŠ¨\")\n",
    "                \n",
    "                # æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ\n",
    "                if len(env_steps) == step_idx + 1:\n",
    "                    # åˆ¤æ–­æ¸¸æˆç»“æœ\n",
    "                    if self.check_winner(board) == 1:\n",
    "                        print(f\"\\\\nğŸ† {agent1_name} è·èƒœï¼\")\n",
    "                    elif self.check_winner(board) == 2:\n",
    "                        print(f\"\\\\nğŸ† {agent2_name} è·èƒœï¼\")\n",
    "                    else:\n",
    "                        print(f\"\\\\nğŸ¤ å¹³å±€ï¼\")\n",
    "    \n",
    "    def check_winner(self, board):\n",
    "        \"\"\"æ£€æŸ¥è·èƒœè€…\"\"\"\n",
    "        board_2d = np.array(board).reshape(6, 7)\n",
    "        \n",
    "        # æ£€æŸ¥æ°´å¹³ã€å‚ç›´å’Œå¯¹è§’çº¿\n",
    "        for player in [1, 2]:\n",
    "            # æ°´å¹³æ£€æŸ¥\n",
    "            for row in range(6):\n",
    "                for col in range(4):\n",
    "                    if all(board_2d[row, col+i] == player for i in range(4)):\n",
    "                        return player\n",
    "            \n",
    "            # å‚ç›´æ£€æŸ¥\n",
    "            for row in range(3):\n",
    "                for col in range(7):\n",
    "                    if all(board_2d[row+i, col] == player for i in range(4)):\n",
    "                        return player\n",
    "            \n",
    "            # å¯¹è§’çº¿æ£€æŸ¥ï¼ˆå·¦ä¸Šåˆ°å³ä¸‹ï¼‰\n",
    "            for row in range(3):\n",
    "                for col in range(4):\n",
    "                    if all(board_2d[row+i, col+i] == player for i in range(4)):\n",
    "                        return player\n",
    "            \n",
    "            # å¯¹è§’çº¿æ£€æŸ¥ï¼ˆå³ä¸Šåˆ°å·¦ä¸‹ï¼‰\n",
    "            for row in range(3):\n",
    "                for col in range(3, 7):\n",
    "                    if all(board_2d[row+i, col-i] == player for i in range(4)):\n",
    "                        return player\n",
    "        \n",
    "        return 0  # æ— è·èƒœè€…\n",
    "    \n",
    "    def plot_performance_stats(self):\n",
    "        \"\"\"ç»˜åˆ¶æ€§èƒ½ç»Ÿè®¡å›¾\"\"\"\n",
    "        if not self.performance_data['episode']:\n",
    "            print(\"æš‚æ— æ€§èƒ½æ•°æ®\")\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        episodes = self.performance_data['episode']\n",
    "        \n",
    "        # èƒœç‡è¶‹åŠ¿\n",
    "        axes[0, 0].plot(episodes, self.performance_data['agent1_wins'], 'r-o', label='Agent1 èƒœç‡', alpha=0.7)\n",
    "        axes[0, 0].plot(episodes, self.performance_data['agent2_wins'], 'b-o', label='Agent2 èƒœç‡', alpha=0.7)\n",
    "        axes[0, 0].plot(episodes, self.performance_data['draws'], 'g-o', label='å¹³å±€ç‡', alpha=0.7)\n",
    "        axes[0, 0].set_title('èƒœç‡è¶‹åŠ¿')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('èƒœç‡')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # å¹³å‡æ¸¸æˆé•¿åº¦\n",
    "        axes[0, 1].plot(episodes, self.performance_data['avg_game_length'], 'purple', marker='s')\n",
    "        axes[0, 1].set_title('å¹³å‡æ¸¸æˆé•¿åº¦')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('å¹³å‡å›åˆæ•°')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ç´¯è®¡èƒœè´Ÿç»Ÿè®¡\n",
    "        cumulative_wins1 = np.cumsum(self.performance_data['agent1_wins'])\n",
    "        cumulative_wins2 = np.cumsum(self.performance_data['agent2_wins'])\n",
    "        cumulative_draws = np.cumsum(self.performance_data['draws'])\n",
    "        \n",
    "        axes[1, 0].plot(episodes, cumulative_wins1, 'r-', label='Agent1 ç´¯è®¡èƒœåœº')\n",
    "        axes[1, 0].plot(episodes, cumulative_wins2, 'b-', label='Agent2 ç´¯è®¡èƒœåœº')\n",
    "        axes[1, 0].plot(episodes, cumulative_draws, 'g-', label='ç´¯è®¡å¹³å±€')\n",
    "        axes[1, 0].set_title('ç´¯è®¡èƒœè´Ÿç»Ÿè®¡')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('ç´¯è®¡åœºæ¬¡')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # æœ€è¿‘çš„èƒœç‡åˆ†å¸ƒ\n",
    "        if len(episodes) > 0:\n",
    "            recent_data = [\n",
    "                self.performance_data['agent1_wins'][-1] if self.performance_data['agent1_wins'] else 0,\n",
    "                self.performance_data['agent2_wins'][-1] if self.performance_data['agent2_wins'] else 0,\n",
    "                self.performance_data['draws'][-1] if self.performance_data['draws'] else 0\n",
    "            ]\n",
    "            labels = ['Agent1', 'Agent2', 'å¹³å±€']\n",
    "            colors = ['red', 'blue', 'green']\n",
    "            axes[1, 1].pie(recent_data, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "            axes[1, 1].set_title('æœ€æ–°èƒœç‡åˆ†å¸ƒ')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# åˆ›å»ºå¯è§†åŒ–å™¨å®ä¾‹\n",
    "visualizer = BattleVisualizer()\n",
    "print(\"å¯¹æˆ˜å¯è§†åŒ–å™¨å·²åˆ›å»ºï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectXAgent:\n",
    "    \"\"\"ConnectXæ™ºèƒ½ä½“åŸºç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Agent\"):\n",
    "        self.name = name\n",
    "        self.wins = 0\n",
    "        self.losses = 0\n",
    "        self.draws = 0\n",
    "    \n",
    "    def get_action(self, observation, configuration):\n",
    "        \"\"\"è·å–ä¸‹ä¸€æ­¥è¡ŒåŠ¨\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"é‡ç½®ç»Ÿè®¡æ•°æ®\"\"\"\n",
    "        self.wins = self.losses = self.draws = 0\n",
    "\n",
    "class NeuralNetworkAgent(ConnectXAgent):\n",
    "    \"\"\"åŸºäºç¥ç»ç½‘ç»œçš„æ™ºèƒ½ä½“\"\"\"\n",
    "    \n",
    "    def __init__(self, model, name=\"NeuralAgent\", use_exploration=True, epsilon=0.1):\n",
    "        super().__init__(name)\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.use_exploration = use_exploration\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def board_to_input(self, board, mark):\n",
    "        \"\"\"å°†æ¸¸æˆæ¿è½¬æ¢ä¸ºç¥ç»ç½‘ç»œè¾“å…¥\"\"\"\n",
    "        # å°†å¯¹æ‰‹æ ‡è®°ä¸º-1ï¼Œè‡ªå·±æ ‡è®°ä¸º1ï¼Œç©ºä½ä¸º0\n",
    "        processed_board = []\n",
    "        for cell in board:\n",
    "            if cell == 0:\n",
    "                processed_board.append(0.0)\n",
    "            elif cell == mark:\n",
    "                processed_board.append(1.0)\n",
    "            else:\n",
    "                processed_board.append(-1.0)\n",
    "        \n",
    "        return torch.FloatTensor(processed_board).unsqueeze(0).to(self.device)\n",
    "    \n",
    "    def get_valid_actions(self, board):\n",
    "        \"\"\"è·å–æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨\"\"\"\n",
    "        return [col for col in range(7) if board[col] == 0]\n",
    "    \n",
    "    def get_action(self, observation, configuration):\n",
    "        \"\"\"è·å–ä¸‹ä¸€æ­¥è¡ŒåŠ¨\"\"\"\n",
    "        board = observation.board\n",
    "        mark = observation.mark\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(board)\n",
    "        if not valid_actions:\n",
    "            return random.choice(range(7))  # åº”è¯¥ä¸ä¼šå‘ç”Ÿ\n",
    "        \n",
    "        # ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹\n",
    "        with torch.no_grad():\n",
    "            board_input = self.board_to_input(board, mark)\n",
    "            policy, value = self.model(board_input)\n",
    "            \n",
    "            # åº”ç”¨softmaxè·å¾—åŠ¨ä½œæ¦‚ç‡\n",
    "            action_probs = F.softmax(policy, dim=-1).cpu().numpy()[0]\n",
    "            \n",
    "            # åªè€ƒè™‘æœ‰æ•ˆåŠ¨ä½œ\n",
    "            valid_probs = [(action, action_probs[action]) for action in valid_actions]\n",
    "            valid_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # æ¢ç´¢ vs åˆ©ç”¨\n",
    "            if self.use_exploration and random.random() < self.epsilon:\n",
    "                # æ¢ç´¢ï¼šéšæœºé€‰æ‹©æœ‰æ•ˆåŠ¨ä½œ\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                # åˆ©ç”¨ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ\n",
    "                action = valid_probs[0][0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "class DataBasedAgent(ConnectXAgent):\n",
    "    \"\"\"åŸºäºæ•°æ®é›†çš„æ™ºèƒ½ä½“\"\"\"\n",
    "    \n",
    "    def __init__(self, data_file=\"connectx-state-action-value.txt\", name=\"DataAgent\"):\n",
    "        super().__init__(name)\n",
    "        self.state_values = {}\n",
    "        self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        \"\"\"åŠ è½½çŠ¶æ€-åŠ¨ä½œ-ä»·å€¼æ•°æ®\"\"\"\n",
    "        try:\n",
    "            print(f\"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}\")\n",
    "            with open(data_file, 'r') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    if line_num > 100000:  # é™åˆ¶æ•°æ®é‡\n",
    "                        break\n",
    "                        \n",
    "                    parts = line.strip().split(',')\n",
    "                    if len(parts) == 8:  # 42å­—ç¬¦çŠ¶æ€ + 7ä¸ªåŠ¨ä½œä»·å€¼\n",
    "                        state = parts[0]\n",
    "                        values = [float(v) for v in parts[1:]]\n",
    "                        self.state_values[state] = values\n",
    "                        \n",
    "                        if line_num % 10000 == 0:\n",
    "                            print(f\"å·²åŠ è½½ {line_num} è¡Œæ•°æ®\")\n",
    "            \n",
    "            print(f\"æ•°æ®åŠ è½½å®Œæˆï¼æ€»å…±åŠ è½½äº† {len(self.state_values)} ä¸ªçŠ¶æ€\")\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"è­¦å‘Šï¼šæ•°æ®æ–‡ä»¶ {data_file} æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨éšæœºç­–ç•¥\")\n",
    "            self.state_values = {}\n",
    "        except Exception as e:\n",
    "            print(f\"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}\")\n",
    "            self.state_values = {}\n",
    "    \n",
    "    def board_to_state_string(self, board, mark):\n",
    "        \"\"\"å°†æ¸¸æˆæ¿è½¬æ¢ä¸ºçŠ¶æ€å­—ç¬¦ä¸²\"\"\"\n",
    "        # è½¬æ¢ä¸ºæ•°æ®é›†æ ¼å¼ï¼šè‡ªå·±æ˜¯1ï¼Œå¯¹æ‰‹æ˜¯2ï¼Œç©ºä½æ˜¯0\n",
    "        state_chars = []\n",
    "        for cell in board:\n",
    "            if cell == 0:\n",
    "                state_chars.append('0')\n",
    "            elif cell == mark:\n",
    "                state_chars.append('1')\n",
    "            else:\n",
    "                state_chars.append('2')\n",
    "        return ''.join(state_chars)\n",
    "    \n",
    "    def get_valid_actions(self, board):\n",
    "        \"\"\"è·å–æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨\"\"\"\n",
    "        return [col for col in range(7) if board[col] == 0]\n",
    "    \n",
    "    def get_action(self, observation, configuration):\n",
    "        \"\"\"æ ¹æ®æ•°æ®é›†è·å–æœ€ä½³åŠ¨ä½œ\"\"\"\n",
    "        board = observation.board\n",
    "        mark = observation.mark\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(board)\n",
    "        if not valid_actions:\n",
    "            return random.choice(range(7))\n",
    "        \n",
    "        # ç”ŸæˆçŠ¶æ€å­—ç¬¦ä¸²\n",
    "        state_str = self.board_to_state_string(board, mark)\n",
    "        \n",
    "        if state_str in self.state_values:\n",
    "            # ä½¿ç”¨æ•°æ®é›†ä¸­çš„ä»·å€¼\n",
    "            action_values = self.state_values[state_str]\n",
    "            # æ‰¾åˆ°ä»·å€¼æœ€é«˜çš„æœ‰æ•ˆåŠ¨ä½œ\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            \n",
    "            for action in valid_actions:\n",
    "                if action_values[action] > best_value:\n",
    "                    best_value = action_values[action]\n",
    "                    best_action = action\n",
    "            \n",
    "            return best_action if best_action is not None else random.choice(valid_actions)\n",
    "        else:\n",
    "            # çŠ¶æ€æœªåœ¨æ•°æ®é›†ä¸­ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™\n",
    "            return self.heuristic_action(board, mark, valid_actions)\n",
    "    \n",
    "    def heuristic_action(self, board, mark, valid_actions):\n",
    "        \"\"\"å¯å‘å¼åŠ¨ä½œé€‰æ‹©\"\"\"\n",
    "        # ç®€å•çš„å¯å‘å¼ï¼šä¼˜å…ˆé€‰æ‹©ä¸­é—´åˆ—\n",
    "        center_preference = [3, 2, 4, 1, 5, 0, 6]\n",
    "        for col in center_preference:\n",
    "            if col in valid_actions:\n",
    "                return col\n",
    "        return random.choice(valid_actions)\n",
    "\n",
    "# åˆ›å»ºä¸åŒç±»å‹çš„æ™ºèƒ½ä½“ç”¨äºæµ‹è¯•\n",
    "print(\"æ™ºèƒ½ä½“ç±»å®šä¹‰å®Œæˆï¼\")\n",
    "\n",
    "# åˆ›å»ºåŸºäºæ•°æ®é›†çš„æ™ºèƒ½ä½“\n",
    "data_agent = DataBasedAgent(name=\"DataAgent\")\n",
    "print(f\"æ•°æ®ä»£ç†åˆ›å»ºå®Œæˆï¼ŒåŠ è½½äº† {len(data_agent.state_values)} ä¸ªçŠ¶æ€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc799483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_battle_with_visualization(agent1, agent2, visualizer, show_replay=True):\n",
    "    \"\"\"è¿è¡Œå•åœºå¯¹æˆ˜å¹¶å¯è§†åŒ–\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºç¯å¢ƒ\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    \n",
    "    # è®¾ç½®æ™ºèƒ½ä½“å‡½æ•°\n",
    "    def agent1_func(observation, configuration):\n",
    "        return agent1.get_action(observation, configuration)\n",
    "    \n",
    "    def agent2_func(observation, configuration):\n",
    "        return agent2.get_action(observation, configuration)\n",
    "    \n",
    "    # è¿è¡Œæ¸¸æˆ\n",
    "    env_steps = env.run([agent1_func, agent2_func])\n",
    "    \n",
    "    if show_replay:\n",
    "        visualizer.show_game_replay(env_steps, agent1.name, agent2.name)\n",
    "    \n",
    "    # åˆ†æç»“æœ\n",
    "    if len(env_steps) > 0:\n",
    "        final_step = env_steps[-1]\n",
    "        final_board = final_step[0]['board']\n",
    "        winner = visualizer.check_winner(final_board)\n",
    "        game_length = len(env_steps) - 1  # å‡å»åˆå§‹çŠ¶æ€\n",
    "        \n",
    "        return winner, game_length\n",
    "    \n",
    "    return 0, 0\n",
    "\n",
    "def run_training_with_battles(agent1, agent2, visualizer, total_episodes=500, \n",
    "                             battle_interval=50, games_per_battle=10):\n",
    "    \"\"\"è¿è¡Œè®­ç»ƒå¹¶æ¯éš”æŒ‡å®šepisodeæ˜¾ç¤ºå¯¹æˆ˜\"\"\"\n",
    "    \n",
    "    print(f\"\\\\n{'='*80}\")\n",
    "    print(f\"ğŸš€ å¼€å§‹è®­ç»ƒï¼æ€»å…± {total_episodes} episodes\")\n",
    "    print(f\"ğŸ“Š æ¯ {battle_interval} episodes è¿›è¡Œä¸€æ¬¡å¯¹æˆ˜å±•ç¤º (æ¯æ¬¡ {games_per_battle} åœºæ¸¸æˆ)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    stats = {\n",
    "        'agent1_total_wins': 0,\n",
    "        'agent2_total_wins': 0,\n",
    "        'total_draws': 0,\n",
    "        'total_games': 0\n",
    "    }\n",
    "    \n",
    "    for episode in range(0, total_episodes, battle_interval):\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"ğŸ“ Episode {episode}-{min(episode + battle_interval - 1, total_episodes - 1)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # è¿è¡Œå¤šåœºæ¸¸æˆè¿›è¡Œç»Ÿè®¡\n",
    "        episode_stats = {\n",
    "            'agent1_wins': 0,\n",
    "            'agent2_wins': 0,\n",
    "            'draws': 0,\n",
    "            'game_lengths': []\n",
    "        }\n",
    "        \n",
    "        # å¿«é€Ÿæ‰¹é‡æµ‹è¯•ï¼ˆä¸æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹ï¼‰\n",
    "        print(f\"\\\\nğŸ¯ æ­£åœ¨è¿›è¡Œ {games_per_battle} åœºå¿«é€Ÿå¯¹æˆ˜...\")\n",
    "        for game in range(games_per_battle):\n",
    "            winner, game_length = run_battle_with_visualization(\n",
    "                agent1, agent2, visualizer, show_replay=False\n",
    "            )\n",
    "            \n",
    "            episode_stats['game_lengths'].append(game_length)\n",
    "            \n",
    "            if winner == 1:\n",
    "                episode_stats['agent1_wins'] += 1\n",
    "                agent1.wins += 1\n",
    "                agent2.losses += 1\n",
    "            elif winner == 2:\n",
    "                episode_stats['agent2_wins'] += 1\n",
    "                agent2.wins += 1\n",
    "                agent1.losses += 1\n",
    "            else:\n",
    "                episode_stats['draws'] += 1\n",
    "                agent1.draws += 1\n",
    "                agent2.draws += 1\n",
    "        \n",
    "        # è®¡ç®—ç»Ÿè®¡æ•°æ®\n",
    "        agent1_winrate = episode_stats['agent1_wins'] / games_per_battle * 100\n",
    "        agent2_winrate = episode_stats['agent2_wins'] / games_per_battle * 100\n",
    "        draw_rate = episode_stats['draws'] / games_per_battle * 100\n",
    "        avg_game_length = np.mean(episode_stats['game_lengths'])\n",
    "        \n",
    "        # æ›´æ–°æ€»ç»Ÿè®¡\n",
    "        stats['agent1_total_wins'] += episode_stats['agent1_wins']\n",
    "        stats['agent2_total_wins'] += episode_stats['agent2_wins']\n",
    "        stats['total_draws'] += episode_stats['draws']\n",
    "        stats['total_games'] += games_per_battle\n",
    "        \n",
    "        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ\n",
    "        print(f\"\\\\nğŸ“ˆ æœ¬è½®ç»Ÿè®¡ç»“æœ:\")\n",
    "        print(f\"  ğŸ”´ {agent1.name}: {episode_stats['agent1_wins']}/{games_per_battle} èƒœ ({agent1_winrate:.1f}%)\")\n",
    "        print(f\"  ğŸ”µ {agent2.name}: {episode_stats['agent2_wins']}/{games_per_battle} èƒœ ({agent2_winrate:.1f}%)\")\n",
    "        print(f\"  ğŸ¤ å¹³å±€: {episode_stats['draws']}/{games_per_battle} åœº ({draw_rate:.1f}%)\")\n",
    "        print(f\"  â±ï¸  å¹³å‡æ¸¸æˆé•¿åº¦: {avg_game_length:.1f} å›åˆ\")\n",
    "        \n",
    "        # ä¿å­˜æ€§èƒ½æ•°æ®ç”¨äºç»˜å›¾\n",
    "        visualizer.performance_data['episode'].append(episode + battle_interval)\n",
    "        visualizer.performance_data['agent1_wins'].append(agent1_winrate / 100)\n",
    "        visualizer.performance_data['agent2_wins'].append(agent2_winrate / 100) \n",
    "        visualizer.performance_data['draws'].append(draw_rate / 100)\n",
    "        visualizer.performance_data['avg_game_length'].append(avg_game_length)\n",
    "        \n",
    "        # æ˜¾ç¤ºä¸€åœºè¯¦ç»†çš„å¯¹æˆ˜è¿‡ç¨‹\n",
    "        print(f\"\\\\nğŸ® å±•ç¤ºä¸€åœºè¯¦ç»†å¯¹æˆ˜è¿‡ç¨‹:\")\n",
    "        run_battle_with_visualization(agent1, agent2, visualizer, show_replay=True)\n",
    "        \n",
    "        # æ˜¾ç¤ºç´¯è®¡ç»Ÿè®¡\n",
    "        total_winrate1 = stats['agent1_total_wins'] / stats['total_games'] * 100\\n        total_winrate2 = stats['agent2_total_wins'] / stats['total_games'] * 100\\n        total_drawrate = stats['total_draws'] / stats['total_games'] * 100\\n        \\n        print(f\\\"\\\\nğŸ“Š ç´¯è®¡ç»Ÿè®¡ (æ€»å…± {stats['total_games']} åœºæ¸¸æˆ):\\\")\\n        print(f\\\"  ğŸ”´ {agent1.name}: {stats['agent1_total_wins']} èƒœ ({total_winrate1:.1f}%)\\\")\\n        print(f\\\"  ğŸ”µ {agent2.name}: {stats['agent2_total_wins']} èƒœ ({total_winrate2:.1f}%)\\\")\\n        print(f\\\"  ğŸ¤ å¹³å±€: {stats['total_draws']} åœº ({total_drawrate:.1f}%)\\\")\\n        \\n        # ç»˜åˆ¶æ€§èƒ½å›¾è¡¨\\n        print(f\\\"\\\\nğŸ“ˆ ç»˜åˆ¶æ€§èƒ½è¶‹åŠ¿å›¾...\\\")\\n        visualizer.plot_performance_stats()\\n        \\n        # ç­‰å¾…ç”¨æˆ·ç¡®è®¤ç»§ç»­\\n        if episode + battle_interval < total_episodes:\\n            input(f\\\"\\\\nâ¸ï¸  æŒ‰ Enter é”®ç»§ç»­ä¸‹ä¸€è½®è®­ç»ƒ...\\\")\\n            clear_output(wait=True)  # æ¸…ç†è¾“å‡ºï¼Œä¿æŒç•Œé¢æ•´æ´\\n    \\n    print(f\\\"\\\\n{'='*80}\\\")\\n    print(f\\\"ğŸ è®­ç»ƒå®Œæˆï¼\\\")\\n    print(f\\\"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ (æ€»å…± {stats['total_games']} åœºæ¸¸æˆ):\\\")\\n    print(f\\\"  ğŸ”´ {agent1.name}: {stats['agent1_total_wins']} èƒœ ({stats['agent1_total_wins']/stats['total_games']*100:.1f}%)\\\")\\n    print(f\\\"  ğŸ”µ {agent2.name}: {stats['agent2_total_wins']} èƒœ ({stats['agent2_total_wins']/stats['total_games']*100:.1f}%)\\\")\\n    print(f\\\"  ğŸ¤ å¹³å±€: {stats['total_draws']} åœº ({stats['total_draws']/stats['total_games']*100:.1f}%)\\\")\\n    print(f\\\"{'='*80}\\\")\\n    \\n    return stats\\n\\nprint(\\\"è®­ç»ƒå’Œå¯¹æˆ˜åŠŸèƒ½å·²å‡†å¤‡å®Œæˆï¼\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® è¿è¡Œè®­ç»ƒå’Œå¯¹æˆ˜å±•ç¤ºï¼\\n# è¿™ä¸ªcellä¼šæ¯50ä¸ªepisodeå±•ç¤ºä¸€æ¬¡å¯¹æˆ˜è¿‡ç¨‹\\n\\n# åˆ›å»ºç¥ç»ç½‘ç»œæ™ºèƒ½ä½“\\nneural_agent = NeuralNetworkAgent(model, name=\\\"NeuralAgent\\\", epsilon=0.2)\\n\\n# é€‰æ‹©å¯¹æ‰‹ (å¯ä»¥æ˜¯æ•°æ®ä»£ç†ã€éšæœºä»£ç†ç­‰)\\nopponent_options = {\\n    \\\"data\\\": data_agent,\\n    \\\"random\\\": ConnectXAgent(\\\"RandomAgent\\\")  # è¿™é‡Œå¯ä»¥å®ç°éšæœºä»£ç†\\n}\\n\\nprint(\\\"å¯é€‰æ‹©çš„å¯¹æ‰‹:\\\")\\nfor key, agent in opponent_options.items():\\n    print(f\\\"  {key}: {agent.name}\\\")\\n\\n# è®¾ç½®å¯¹æ‰‹\\nopponent = data_agent  # é»˜è®¤ä½¿ç”¨æ•°æ®ä»£ç†ä½œä¸ºå¯¹æ‰‹\\n\\nprint(f\\\"\\\\nğŸ¤– é€‰æ‹©çš„æ™ºèƒ½ä½“ç»„åˆ:\\\")\\nprint(f\\\"  ç©å®¶1: {neural_agent.name} (ç¥ç»ç½‘ç»œ)\\\")\\nprint(f\\\"  ç©å®¶2: {opponent.name} (åŸºäºæ•°æ®é›†)\\\")\\n\\n# å¼€å§‹è®­ç»ƒï¼\\nprint(f\\\"\\\\nå‡†å¤‡å¼€å§‹è®­ç»ƒ... ä½ å¯ä»¥ä¿®æ”¹ä»¥ä¸‹å‚æ•°:\\\")\\nprint(f\\\"  - total_episodes: æ€»çš„è®­ç»ƒè½®æ•°\\\")\\nprint(f\\\"  - battle_interval: æ¯éš”å¤šå°‘episodeå±•ç¤ºä¸€æ¬¡å¯¹æˆ˜\\\")\\nprint(f\\\"  - games_per_battle: æ¯æ¬¡å±•ç¤ºæ—¶è¿è¡Œå¤šå°‘åœºæ¸¸æˆè¿›è¡Œç»Ÿè®¡\\\")\\nprint(f\\\"\\\\nè¿è¡Œä¸‹é¢çš„ä»£ç å¼€å§‹è®­ç»ƒ:\\\")\\nprint(f\\\"final_stats = run_training_with_battles(\\\")\\nprint(f\\\"    agent1=neural_agent,\\\")\\nprint(f\\\"    agent2=opponent,\\\")\\nprint(f\\\"    visualizer=visualizer,\\\")\\nprint(f\\\"    total_episodes=200,    # æ€»å…±200è½®\\\")\\nprint(f\\\"    battle_interval=50,    # æ¯50è½®å±•ç¤ºä¸€æ¬¡\\\")\\nprint(f\\\"    games_per_battle=10    # æ¯æ¬¡å±•ç¤º10åœºæ¸¸æˆ\\\")\\nprint(f\\\")\\\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ æ‰§è¡Œè®­ç»ƒï¼\\n# è¿è¡Œè¿™ä¸ªcellå¼€å§‹å®é™…çš„è®­ç»ƒè¿‡ç¨‹\\n\\ntry:\\n    final_stats = run_training_with_battles(\\n        agent1=neural_agent,\\n        agent2=opponent, \\n        visualizer=visualizer,\\n        total_episodes=200,     # æ€»å…±200ä¸ªepisode\\n        battle_interval=50,     # æ¯50ä¸ªepisodeå±•ç¤ºä¸€æ¬¡å¯¹æˆ˜\\n        games_per_battle=10     # æ¯æ¬¡å±•ç¤ºè¿è¡Œ10åœºæ¸¸æˆç»Ÿè®¡\\n    )\\n    \\n    print(\\\"\\\\nğŸ‰ è®­ç»ƒå®Œæˆï¼\\\")\\n    print(f\\\"æœ€ç»ˆç»Ÿè®¡ç»“æœ: {final_stats}\\\")\\n    \\nexcept KeyboardInterrupt:\\n    print(\\\"\\\\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­\\\")\\nexcept Exception as e:\\n    print(f\\\"\\\\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\\\")\\n    import traceback\\n    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fe37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” é¢å¤–æµ‹è¯•å’Œåˆ†æ\\n\\n# 1. å•ç‹¬æµ‹è¯•ä¸€åœºæ¸¸æˆ\\nprint(\\\"=\\\" * 50)\\nprint(\\\"ğŸ¯ å•åœºå¯¹æˆ˜æµ‹è¯•\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# è¿è¡Œå•åœºæ¸¸æˆå¹¶æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹\\nwinner, game_length = run_battle_with_visualization(neural_agent, data_agent, visualizer)\\n\\nif winner == 1:\\n    print(f\\\"\\\\nğŸ† {neural_agent.name} è·èƒœï¼æ¸¸æˆé•¿åº¦: {game_length} å›åˆ\\\")\\nelif winner == 2:\\n    print(f\\\"\\\\nğŸ† {data_agent.name} è·èƒœï¼æ¸¸æˆé•¿åº¦: {game_length} å›åˆ\\\")\\nelse:\\n    print(f\\\"\\\\nğŸ¤ å¹³å±€ï¼æ¸¸æˆé•¿åº¦: {game_length} å›åˆ\\\")\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f291629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š æ€§èƒ½åˆ†æå’Œæœ€ç»ˆæ€»ç»“\\n\\n# æ˜¾ç¤ºæœ€ç»ˆçš„æ€§èƒ½å›¾è¡¨\\nif visualizer.performance_data['episode']:\\n    print(\\\"ğŸ“ˆ æ˜¾ç¤ºå®Œæ•´çš„æ€§èƒ½è¶‹åŠ¿å›¾...\\\")\\n    visualizer.plot_performance_stats()\\nelse:\\n    print(\\\"âš ï¸ è¿˜æ²¡æœ‰æ€§èƒ½æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ\\\")\\n\\n# æ™ºèƒ½ä½“ç»Ÿè®¡ä¿¡æ¯\\nprint(\\\"\\\\nğŸ¤– æ™ºèƒ½ä½“ç»Ÿè®¡ä¿¡æ¯:\\\")\\nprint(f\\\"  {neural_agent.name}:\\\")\\nprint(f\\\"    èƒœ: {neural_agent.wins}, è´Ÿ: {neural_agent.losses}, å¹³: {neural_agent.draws}\\\")\\nprint(f\\\"  {data_agent.name}:\\\")\\nprint(f\\\"    èƒœ: {data_agent.wins}, è´Ÿ: {data_agent.losses}, å¹³: {data_agent.draws}\\\")\\n\\n# ä½¿ç”¨è¯´æ˜\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*80)\\nprint(\\\"ğŸ“– ä½¿ç”¨è¯´æ˜\\\")\\nprint(\\\"=\\\"*80)\\nprint(\\\"\\\"\\\"\\nğŸ¯ è¿™ä¸ªnotebookçš„ä¸»è¦åŠŸèƒ½:\\n1. åˆ›å»ºåŸºäºç¥ç»ç½‘ç»œçš„ConnectXæ™ºèƒ½ä½“\\n2. åˆ›å»ºåŸºäºæ•°æ®é›†çš„ConnectXæ™ºèƒ½ä½“  \\n3. æ¯50ä¸ªepisodeå±•ç¤ºä¸€æ¬¡è¯¦ç»†çš„å¯¹æˆ˜è¿‡ç¨‹\\n4. å®æ—¶æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡å›¾è¡¨\\n5. å¯è§†åŒ–æ¸¸æˆæ¿å’Œå¯¹æˆ˜è¿‡ç¨‹\\n\\nğŸ”§ å¯ä»¥è°ƒæ•´çš„å‚æ•°:\\n- total_episodes: æ€»çš„è®­ç»ƒè½®æ•°\\n- battle_interval: æ¯éš”å¤šå°‘episodeå±•ç¤ºå¯¹æˆ˜\\n- games_per_battle: æ¯æ¬¡ç»Ÿè®¡çš„æ¸¸æˆåœºæ•°\\n- epsilon: ç¥ç»ç½‘ç»œæ™ºèƒ½ä½“çš„æ¢ç´¢ç‡\\n\\nğŸ® æ”¯æŒçš„æ™ºèƒ½ä½“ç±»å‹:\\n- NeuralNetworkAgent: åŸºäºPyTorchç¥ç»ç½‘ç»œ\\n- DataBasedAgent: åŸºäºconnectx-state-action-value.txtæ•°æ®é›†\\n- å¯ä»¥è½»æ¾æ‰©å±•å…¶ä»–ç±»å‹çš„æ™ºèƒ½ä½“\\n\\nğŸ“Š å¯è§†åŒ–åŠŸèƒ½:\\n- å®æ—¶æ˜¾ç¤ºæ¸¸æˆæ¿çŠ¶æ€\\n- å¯¹æˆ˜è¿‡ç¨‹å›æ”¾\\n- èƒœç‡è¶‹åŠ¿å›¾\\n- æ¸¸æˆé•¿åº¦ç»Ÿè®¡\\n- ç´¯è®¡æ€§èƒ½åˆ†æ\\n\\nğŸ’¡ æç¤º:\\n- æ¯æ¬¡å¯¹æˆ˜å±•ç¤ºåä¼šæš‚åœï¼ŒæŒ‰Enterç»§ç»­\\n- å¯ä»¥éšæ—¶ä¸­æ–­è®­ç»ƒï¼ˆCtrl+Cï¼‰\\n- æ‰€æœ‰ç»Ÿè®¡æ•°æ®éƒ½ä¼šä¿å­˜åœ¨visualizerä¸­\\n\\\"\\\"\\\")\\nprint(\\\"=\\\"*80)\\n\\nprint(\\\"\\\\nğŸ‰ ConnectXè®­ç»ƒå’Œå¯¹æˆ˜å¯è§†åŒ–ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼\\\")\\nprint(\\\"ç°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒcellæ¥å¼€å§‹è®­ç»ƒï¼Œæˆ–è€…å•ç‹¬è¿è¡Œæµ‹è¯•cellæ¥è§‚çœ‹å¯¹æˆ˜ã€‚\\\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
