# ConnectXæ¨¡å‹Attentionå„ªåŒ–å ±å‘Š

## å•é¡Œæè¿°
åŸå§‹éŒ¯èª¤ï¼š`shape '[1, 3, 24, 5, 42]' is invalid for input of size 16128`

**æ ¹æœ¬åŸå› **: Attention headsæ•¸é‡(24)ç„¡æ³•è¢«channelsæ•¸é‡(128)æ•´é™¤ï¼Œå°è‡´ç¶­åº¦è¨ˆç®—éŒ¯èª¤
- 128 Ã· 24 = 5.33... (ä¸æ˜¯æ•´æ•¸)
- é€™æœƒåœ¨ `qkv.reshape(B, 3, self.heads, C // self.heads, H * W)` æ™‚å‡ºéŒ¯

## è§£æ±ºæ–¹æ¡ˆ

### 1. èª¿æ•´Channelæ•¸é‡
```python
# ä¹‹å‰: self.channels = 128
# ç¾åœ¨: self.channels = 144  # 144 = 24 Ã— 6ï¼Œå®Œç¾æ•´é™¤
```

**144çš„å„ªå‹¢**:
- 144 Ã· 24 = 6 (head_dim = 6ï¼Œå®Œç¾æ•´æ•¸)
- 144æ˜¯24çš„å€æ•¸ï¼Œæ”¯æŒå¤šç¨®headé…ç½® (1, 2, 3, 4, 6, 8, 9, 12, 16, 18, 24)
- æ¯”128ç•¥å¤§ï¼Œæä¾›æ›´å¤šæ¨¡å‹å®¹é‡

### 2. æ™ºèƒ½Headèª¿æ•´æ©Ÿåˆ¶
```python
class SpatialSelfAttention(nn.Module):
    def __init__(self, c, heads=24):
        # è‡ªå‹•èª¿æ•´headsåˆ°æœ€æ¥è¿‘çš„å› æ•¸
        if c % heads != 0:
            possible_heads = [i for i in range(1, c + 1) if c % i == 0]
            heads = min(possible_heads, key=lambda x: abs(x - heads))
            print(f"èª¿æ•´attention heads: {heads} (channels={c})")
```

### 3. å„ªåŒ–Attentioné »ç‡
```python
# æ›´é »ç¹ä½¿ç”¨attention (æ¯1-3å€‹block)
self.attn_every = max(1, min(int(attn_every), 3))
```

## æ–°æ¨¡å‹é…ç½®

### ğŸ“ **æ¶æ§‹åƒæ•¸**
- **Channels**: 144 (åŸ128)
- **Attention Heads**: 24
- **Head Dimension**: 6 (144 Ã· 24)
- **Attention Frequency**: æ¯1-3å€‹residual block

### ğŸ§  **Attentionå¢å¼·**
1. **æ›´å¤šé ­éƒ¨**: 24å€‹headsæä¾›æ›´è±å¯Œçš„ç‰¹å¾µè¡¨ç¤º
2. **æ›´é »ç¹**: attentionæ¨¡çµ„å‡ºç¾æ›´å¯†é›†
3. **è‡ªé©æ‡‰**: è‡ªå‹•èª¿æ•´åˆ°æœ€ä½³headé…ç½®

### âš¡ **æ€§èƒ½æå‡é æœŸ**
1. **æ›´å¼·è¡¨ç¤ºèƒ½åŠ›**: 24å€‹attention headsèƒ½æ•æ‰æ›´è¤‡é›œçš„ç©ºé–“é—œä¿‚
2. **æ›´å¥½çš„é•·ç¨‹ä¾è³´**: ConnectXéœ€è¦è­˜åˆ¥4å­é€£ç·šï¼Œattentionæœ‰åŠ©æ–¼æ­¤
3. **æ›´ç´°ç·»çš„ç‰¹å¾µ**: æ¯å€‹headå°ˆæ³¨æ–¼ä¸åŒçš„æ¨¡å¼(æ©«å‘ã€ç¸±å‘ã€å°è§’ç­‰)

## æ¸¬è©¦çµæœ âœ…

### æ¨¡å‹å‰µå»ºæ¸¬è©¦
```
âœ… æ¨¡å‹å‰µå»ºæˆåŠŸï¼
   Channels: 144
   Attentionæ¨¡çµ„æ•¸é‡: 2
   Headæ•¸é‡: 24
   Headç¶­åº¦: 6
```

### å‰å‘å‚³æ’­æ¸¬è©¦
```
âœ… å‰å‘å‚³æ’­æˆåŠŸï¼
   Policy shape: torch.Size([4, 7])
   Value shape: torch.Size([4, 1])
```

### è¨“ç·´æµç¨‹æ¸¬è©¦
```
âœ… ç­–ç•¥æ›´æ–°æˆåŠŸï¼
   ç¸½æå¤±: 0.5254
   ç†µå€¼: 0.3698
```

## å»ºè­°é…ç½®

### å°æ–¼ä¸åŒè¨ˆç®—è³‡æºçš„æ¨è–¦ï¼š

#### ğŸš€ **é«˜æ€§èƒ½é…ç½®** (æ¨è–¦)
```python
channels = 144
heads = 24
attn_every = 2
num_layers = 16-32
```

#### âš–ï¸ **å¹³è¡¡é…ç½®**
```python
channels = 144
heads = 12
attn_every = 3
num_layers = 12-16
```

#### ğŸ’¡ **è¼•é‡é…ç½®**
```python
channels = 144
heads = 6
attn_every = 4
num_layers = 8-12
```

## æ³¨æ„äº‹é …

1. **è¨˜æ†¶é«”ä½¿ç”¨**: 24å€‹headsæœƒå¢åŠ attentionè¨ˆç®—é‡ï¼Œä½†å°æ–¼6Ã—7çš„å°æ£‹ç›¤ä»ç„¶å¯æ§
2. **è¨“ç·´é€Ÿåº¦**: å¯èƒ½ç•¥å¾®é™ä½ï¼Œä½†attentionçš„ä¸¦è¡Œæ€§æœ‰åŠ©æ–¼åŠ é€Ÿ
3. **æ•ˆæœé©—è­‰**: å»ºè­°è¨“ç·´ä¸€æ®µæ™‚é–“å¾Œæ¯”è¼ƒå‹ç‡æå‡

## æœªä¾†å„ªåŒ–æ–¹å‘

1. **å‹•æ…‹Headæ•¸**: æ ¹æ“šè¨“ç·´éšæ®µèª¿æ•´headæ•¸é‡
2. **æ··åˆAttention**: çµåˆlocalå’Œglobal attention
3. **æ£‹é¡ç‰¹å®š**: é‡å°ConnectXè¨­è¨ˆå°ˆé–€çš„attentionæ¨¡å¼

ç¾åœ¨ä½ çš„æ¨¡å‹å·²ç¶“æˆåŠŸæ”¯æŒ24å€‹attention headsï¼Œæ‡‰è©²èƒ½å¤ æä¾›æ›´å¼·çš„ç‰¹å¾µå­¸ç¿’èƒ½åŠ›ï¼ğŸ¯
