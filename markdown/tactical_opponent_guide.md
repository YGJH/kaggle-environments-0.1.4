# ConnectX 戰術增強自對弈功能說明

## 功能概述

我們已經成功實現了戰術增強的自對弈功能，解決了你提出的問題：現在的自對弈訓練中，只有玩家1會更新權重和蒐集訓練資料，而玩家2（對手）使用了戰術增強函數來提供更強的對抗。

## 核心改進

### 1. 原本的自對弈機制
- ✅ **確認**：只有玩家1會儲存訓練轉換（training transitions）
- ✅ **確認**：兩個玩家都使用相同的神經網路，但只有玩家1從經驗中學習

### 2. 新的戰術增強對手
現在玩家2使用戰術函數增強，具有以下優先級：

#### 戰術決策優先級：
1. **立即獲勝** - 如果可以直接連成四子，立即執行
2. **阻擋對手** - 如果對手下一步能獲勝，立即阻擋
3. **避免危險動作** - 避免讓對手在未來2步內獲勝的動作
4. **神經網路決策** - 在安全動作中使用神經網路選擇最佳動作

#### 使用的戰術函數：
- `if_i_can_finish()` - 檢測是否能立即獲勝
- `if_i_will_lose()` - 檢測是否需要阻擋對手獲勝
- `is_dangerous_move()` - 檢測動作是否會讓對手在未來幾步獲勝

## 訓練配置

### 配置檔案設定 (`config.yaml`)
```yaml
training:
  use_tactical_opponent: true  # 啟用戰術增強對手
  opponent_diversity: false    # 可與多樣性對手訓練配合使用
```

### 混合訓練策略
- **每3個回合中有1個** 使用戰術增強對手
- **其餘回合** 使用標準自對弈
- 這樣可以平衡訓練難度和多樣性

## 程式碼架構

### 新增的核心方法

1. **`create_tactical_opponent()`**
   - 創建戰術增強的對手函數
   - 結合規則和神經網路決策

2. **`play_game_with_different_agents()`**
   - 支持兩個不同智能體函數的遊戲
   - 只有玩家1儲存訓練資料

3. **`enhanced_self_play_episode()`**
   - 使用戰術對手的增強自對弈
   - 提供更具挑戰性的訓練對手

4. **`decode_state()`**
   - 將神經網路狀態解碼為棋盤格式
   - 供戰術函數使用

## 訓練效果預期

### 優勢：
1. **更強的對手** - 戰術增強對手能做出更合理的決策
2. **更快的收斂** - 面對更強對手，訓練中的智能體學習更有效
3. **減少過度適應** - 避免智能體只會對抗弱對手
4. **戰術意識提升** - 學習如何應對真正的威脅和機會

### 測試結果：
- ✅ 所有功能正常運作
- ✅ 戰術函數正確檢測獲勝和危險動作
- ✅ 增強自對弈平均遊戲長度較短（6步 vs 11.3步）
- ✅ 表示對手確實更加高效和有挑戰性

## 使用方法

### 1. 啟動訓練
```bash
uv run train_connectx_rl_robust.py
```

### 2. 測試戰術功能
```bash
uv run test_tactical_opponent.py
```

### 3. 調整設定
在 `config.yaml` 中修改：
- `use_tactical_opponent: true/false` - 開啟/關閉戰術對手
- 可以與其他訓練選項組合使用

## 未來可能的擴展

1. **動態難度調整** - 根據訓練進度調整戰術對手強度
2. **更多戰術規則** - 添加更複雜的棋局評估
3. **多種對手策略** - 實現不同風格的戰術對手
4. **對手學習記錄** - 追蹤對手的決策模式

## 總結

✅ **問題解決**：現在自對弈訓練中，玩家1專心學習，玩家2使用戰術增強提供更好的對抗
✅ **功能完整**：所有戰術函數正常工作，提供智能的遊戲對手
✅ **配置靈活**：可以輕鬆開啟/關閉，與其他訓練策略結合
✅ **效果顯著**：測試顯示戰術對手確實更加高效和具挑戰性

這個改進將大大提升你的 ConnectX AI 的訓練效果！🎯
