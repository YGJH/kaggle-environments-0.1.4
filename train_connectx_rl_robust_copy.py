#!/usr/bin/env python3

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import math
import re
import glob
import shutil
from collections import deque
import logging
from datetime import datetime
import argparse
import traceback
import multiprocessing as mp  # NEW: multiprocessing for parallel rollouts
import ray, numpy as np, torch, time, random
from copy import deepcopy  # NEW: for KL anchor

try:
    import yaml
except ImportError:
    print("PyYAML not found. Installing...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "PyYAML"])
    import yaml

try:
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    from matplotlib.animation import FuncAnimation
    import matplotlib.colors as mcolors
    
    # é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans', 'Bitstream Vera Sans']
    plt.rcParams['axes.unicode_minus'] = False
    # è®¾ç½®å­—ä½“å¤§å°ï¼Œé¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
    plt.rcParams['font.size'] = 10
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['xtick.labelsize'] = 9
    plt.rcParams['ytick.labelsize'] = 9
    
    VISUALIZATION_AVAILABLE = True
except ImportError:
    print("Matplotlib not found. Installing for visualization...")
    try:
        import subprocess
        subprocess.check_call(["uv", "add","matplotlib"])
        import matplotlib.pyplot as plt
        import matplotlib.patches as patches
        from matplotlib.animation import FuncAnimation
        import matplotlib.colors as mcolors
        VISUALIZATION_AVAILABLE = True
    except Exception:
        print("âš ï¸ ç„¡æ³•å®‰è£matplotlibï¼Œå°‡è·³éå¯è¦–åŒ–åŠŸèƒ½")
        VISUALIZATION_AVAILABLE = False

from kaggle_environments import make, evaluate

# Set up logging with proper format
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # Console output
        logging.FileHandler('training.log')  # File output
    ]
)
logger = logging.getLogger(__name__)

# ç¢ºä¿loggerè¨Šæ¯ä¸è¢«æŠ‘åˆ¶
logger.setLevel(logging.INFO)

# ===== Shared Tactical Functions =====
# These functions are used by both worker processes and the main trainer

def flat_to_2d(board_flat):
    """Convert flat board to 2D grid format."""
    try:
        if isinstance(board_flat[0], list):
            return board_flat
    except Exception:
        pass
    return [list(board_flat[r*7:(r+1)*7]) for r in range(6)]

def find_drop_row(grid, col):
    """Find the lowest empty row in a column."""
    for r in range(5, -1, -1):
        if grid[r][col] == 0:
            return r
    return None

def is_win_from(grid, r, c, mark):
    """Check if placing a mark at (r,c) creates a winning line."""
    dirs = [(1,0), (0,1), (1,1), (-1,1)]
    for dr, dc in dirs:
        cnt = 1
        for s in (1, -1):
            rr, cc = r + dr*s, c + dc*s
            while 0 <= rr < 6 and 0 <= cc < 7 and grid[rr][cc] == mark:
                cnt += 1
                rr += dr*s
                cc += dc*s
        if cnt >= 4:
            return True
    return False

def is_winning_move(board_flat, col, mark):
    """Check if a move in col is a winning move for mark."""
    grid = flat_to_2d(board_flat)
    if col < 0 or col > 6:
        return False
    r = find_drop_row(grid, col)
    if r is None:
        return False
    grid[r][col] = mark
    return is_win_from(grid, r, col, mark)

def apply_move(board_flat, col, mark):
    """Apply a move and return the new board state."""
    grid = flat_to_2d(board_flat)
    if col < 0 or col > 6:
        return None
    r = find_drop_row(grid, col)
    if r is None:
        return None
    grid[r][col] = mark
    return [grid[i][j] for i in range(6) for j in range(7)]

def if_i_can_win(board_flat, mark, agent):
    """Find a winning move for mark, if any."""
    for c in agent.get_valid_actions(board_flat):
        if is_winning_move(board_flat, c, mark):
            return c
    return None

def if_i_will_lose(board_flat, mark, agent):
    """Find a move to block opponent's immediate win."""
    opp = 3 - mark
    for c in agent.get_valid_actions(board_flat):
        if is_winning_move(board_flat, c, opp):
            return c
    return None

def if_i_will_lose_at_next(board_flat, move_col, mark, agent):
    """Check if making move_col gives opponent an immediate winning reply."""
    grid = flat_to_2d(board_flat)
    r = find_drop_row(grid, move_col) if 0 <= move_col <= 6 else None
    if r is None:
        return True
    grid[r][move_col] = mark
    opp = 3 - mark
    # Check if opponent has immediate winning reply
    new_board = [grid[i][j] for i in range(6) for j in range(7)]
    for c in agent.get_valid_actions(new_board):
        if is_winning_move(new_board, c, opp):
            return True
    return False

def safe_moves(board_flat, mark, valid_actions, agent):
    """Return moves that don't give opponent immediate win."""
    return [a for a in valid_actions if not if_i_will_lose_at_next(board_flat, a, mark, agent)]

# ===== Shared Opponent Strategies =====

def random_opponent_strategy(board_flat, mark, valid_actions, agent):
    """Random opponent with basic tactics."""
    c = if_i_can_win(board_flat, mark, agent)
    if c is not None:
        return c
    c = if_i_will_lose(board_flat, mark, agent)
    if c is not None:
        return c
    safe = safe_moves(board_flat, mark, valid_actions, agent)
    if safe:
        return random.choice(safe)
    return random.choice(valid_actions)

def minimax_opponent_strategy(board_flat, mark, valid_actions, agent, depth=3):
    """Minimax opponent implementation."""
    def score_window(window, mark):
        opp = 3 - mark
        cnt_self = window.count(mark)
        cnt_opp = window.count(opp)
        cnt_empty = window.count(0)
        if cnt_self > 0 and cnt_opp > 0:
            return 0
        score = 0
        if cnt_self == 4:
            score += 10000
        elif cnt_self == 3 and cnt_empty == 1:
            score += 100
        elif cnt_self == 2 and cnt_empty == 2:
            score += 10
        if cnt_opp == 3 and cnt_empty == 1:
            score -= 120
        return score

    def evaluate_board(board_flat, mark):
        grid = flat_to_2d(board_flat)
        score = 0
        # center preference
        center_col = [grid[r][3] for r in range(6)]
        score += center_col.count(mark) * 3
        # Horizontal
        for r in range(6):
            row = grid[r]
            for c in range(4):
                window = row[c:c+4]
                score += score_window(window, mark)
        # Vertical
        for c in range(7):
            col = [grid[r][c] for r in range(6)]
            for r in range(3):
                window = col[r:r+4]
                score += score_window(window, mark)
        # Diagonals
        for r in range(3):
            for c in range(4):
                window = [grid[r+i][c+i] for i in range(4)]
                score += score_window(window, mark)
        for r in range(3, 6):
            for c in range(4):
                window = [grid[r-i][c+i] for i in range(4)]
                score += score_window(window, mark)
        return score

    def has_winner(board_flat, mark):
        grid = flat_to_2d(board_flat)
        for r in range(6):
            for c in range(7):
                if grid[r][c] != mark:
                    continue
                if is_win_from(grid, r, c, mark):
                    return True
        return False

    def minimax(board_flat, depth, alpha, beta, current_mark, maximizing_mark):
        valid_moves = agent.get_valid_actions(board_flat)
        if depth == 0 or not valid_moves:
            return evaluate_board(board_flat, maximizing_mark), None
        
        best_move = None
        if current_mark == maximizing_mark:
            value = -float('inf')
            for c in valid_moves:
                nb = apply_move(board_flat, c, current_mark)
                if nb is None:
                    continue
                if has_winner(nb, current_mark):
                    return 1e6 - (5 - depth), c
                child_val, _ = minimax(nb, depth-1, alpha, beta, 3-current_mark, maximizing_mark)
                if child_val > value:
                    value, best_move = child_val, c
                alpha = max(alpha, value)
                if alpha >= beta:
                    break
            return value, best_move
        else:
            value = float('inf')
            for c in valid_moves:
                nb = apply_move(board_flat, c, current_mark)
                if nb is None:
                    continue
                if has_winner(nb, current_mark):
                    return -1e6 + (5 - depth), c
                child_val, _ = minimax(nb, depth-1, alpha, beta, 3-current_mark, maximizing_mark)
                if child_val < value:
                    value, best_move = child_val, c
                beta = min(beta, value)
                if alpha >= beta:
                    break
            return value, best_move

    # Check tactical moves first
    c = if_i_can_win(board_flat, mark, agent)
    if c is not None:
        return c
    c = if_i_will_lose(board_flat, mark, agent)
    if c is not None:
        return c
    
    # Use minimax
    safe = safe_moves(board_flat, mark, valid_actions, agent)
    moves = safe if safe else valid_actions
    best_score = -float('inf')
    best_move = random.choice(moves)
    
    for a in moves:
        nb = apply_move(board_flat, a, mark)
        if nb is None:
            continue
        score, _ = minimax(nb, depth-1, -float('inf'), float('inf'), 3-mark, mark)
        if score > best_score:
            best_score, best_move = score, a
    return best_move
# ===== æ¼¸é€²å¼å°æ‰‹ç­–ç•¥ (å¾å¼±åˆ°å¼·) =====

def pure_random_opponent_strategy(board_flat, mark, valid_actions, agent):
    """ç´”éš¨æ©Ÿå°æ‰‹ï¼Œä¸ä½¿ç”¨ä»»ä½•æˆ°è¡“"""
    return random.choice(valid_actions)

def basic_win_only_opponent_strategy(board_flat, mark, valid_actions, agent):
    """åªæœƒè´æ£‹çš„åŸºç¤å°æ‰‹ï¼Œä¸æœƒé˜²å®ˆ"""
    # åªæª¢æŸ¥èƒ½å¦ç²å‹
    c = if_i_can_win(board_flat, mark, agent)
    if c is not None:
        return c
    # å…¶ä»–æƒ…æ³ç´”éš¨æ©Ÿ
    return random.choice(valid_actions)

def defensive_only_opponent_strategy(board_flat, mark, valid_actions, agent):
    """åªæœƒé˜²å®ˆçš„å°æ‰‹ï¼Œä¸æœƒä¸»å‹•ç²å‹"""
    # åªæª¢æŸ¥æ˜¯å¦éœ€è¦é˜»æ­¢å°æ‰‹ç²å‹
    c = if_i_will_lose(board_flat, mark, agent)
    if c is not None:
        return c
    # å…¶ä»–æƒ…æ³ç´”éš¨æ©Ÿ
    return random.choice(valid_actions)

def center_bias_opponent_strategy(board_flat, mark, valid_actions, agent):
    """åå¥½ä¸­å¤®åˆ—çš„å¼±å°æ‰‹"""
    # åŸºæœ¬æˆ°è¡“ï¼šè´å’Œé˜²å®ˆ
    c = if_i_can_win(board_flat, mark, agent)
    if c is not None:
        return c
    c = if_i_will_lose(board_flat, mark, agent)
    if c is not None:
        return c
    
    # åå¥½ä¸­å¤®åˆ— (3, 4, 2, 5, 1, 6, 0)
    center_preference = [3, 4, 2, 5, 1, 6, 0]
    for col in center_preference:
        if col in valid_actions:
            return col
    return random.choice(valid_actions)

def weak_tactical_opponent_strategy(board_flat, mark, valid_actions, agent):
    """å¼±åŒ–çš„æˆ°è¡“å°æ‰‹ï¼Œæœ‰50%æ©Ÿç‡å¿½ç•¥æˆ°è¡“"""
    # 50%æ©Ÿç‡ä½¿ç”¨æˆ°è¡“
    if random.random() < 0.5:
        c = if_i_can_win(board_flat, mark, agent)
        if c is not None:
            return c
        c = if_i_will_lose(board_flat, mark, agent)
        if c is not None:
            return c
    
    # å…¶ä»–æƒ…æ³éš¨æ©Ÿé¸æ“‡
    return random.choice(valid_actions)

def mistake_prone_opponent_strategy(board_flat, mark, valid_actions, agent):
    """å®¹æ˜“çŠ¯éŒ¯çš„å°æ‰‹ï¼Œæœƒåšå‡ºå±éšªç§»å‹•"""
    # åŸºæœ¬æˆ°è¡“
    c = if_i_can_win(board_flat, mark, agent)
    if c is not None:
        return c
    c = if_i_will_lose(board_flat, mark, agent)
    if c is not None:
        return c
    
    # 30%æ©Ÿç‡æ•…æ„é¸æ“‡å±éšªç§»å‹•
    if random.random() < 0.3:
        dangerous_moves = []
        for action in valid_actions:
            if if_i_will_lose_at_next(board_flat, action, mark, agent):
                dangerous_moves.append(action)
        if dangerous_moves:
            return random.choice(dangerous_moves)
    
    # å…¶ä»–æƒ…æ³éš¨æ©Ÿ
    return random.choice(valid_actions)

def self_play_opponent_strategy(board_flat, mark, valid_actions, agent):
    """Self-play opponent using current policy network."""
    state = agent.encode_state(board_flat, mark)
    action, _, _ = agent.select_action(state, valid_actions, training=False)
    return int(action)

_WORKER = {
    "agent": None,
    "env": None,
    "policy_version": -1,  # å°šæœªè¼‰å…¥ä»»ä½•ç‰ˆæœ¬
}

def count_open_threats(board_flat, mark, threat_length):
    """Count open threat patterns of given length for a player.
    
    An 'open' threat means the pattern can potentially be extended to 4-in-a-row.
    For example: [0, mark, mark, 0] is an open 2-threat.
    """
    grid = flat_to_2d(board_flat)
    count = 0
    directions = [(1,0), (0,1), (1,1), (-1,1)]  # horizontal, vertical, diagonal
    
    for r in range(6):
        for c in range(7):
            for dr, dc in directions:
                # Check if we can fit a 4-length pattern starting from (r,c)
                if not (0 <= r + 3*dr < 6 and 0 <= c + 3*dc < 7):
                    continue
                
                # Extract 4-cell window
                window = []
                for i in range(4):
                    rr, cc = r + i*dr, c + i*dc
                    window.append(grid[rr][cc])
                
                # Count marks in window
                mark_count = window.count(mark)
                opp_count = window.count(3 - mark)
                empty_count = window.count(0)
                
                # Check if it's an open threat of desired length
                if mark_count == threat_length and opp_count == 0 and empty_count == (4 - threat_length):
                    count += 1
    
    return count

def count_center_control(board_flat, mark):
    """Count pieces in center and adjacent columns with weighted scoring."""
    grid = flat_to_2d(board_flat)
    center_count = sum(1 for r in range(6) if grid[r][3] == mark)  # center column
    adjacent_count = 0
    for col in [2, 4]:  # adjacent to center
        adjacent_count += sum(1 for r in range(6) if grid[r][col] == mark)
    return center_count, adjacent_count

def has_immediate_win(board_flat, mark):
    """Check if player has an immediate winning move available."""
    mock_agent = type('MockAgent', (), {
        'get_valid_actions': lambda self, board: [c for c in range(7) if flat_to_2d(board)[0][c] == 0]
    })()
    return if_i_can_win(board_flat, mark, mock_agent) is not None

def compute_potential_function(board_flat, mark, gamma=0.99):
    """Compute potential function Î¦(s) for potential-based reward shaping.
    
    Î¦(s) = 0.02Â·center_diff + 0.05Â·(my_open2 - opp_open2) + 0.12Â·(my_open3 - opp_open3)
           + 0.40Â·I(my_immediate_win) - 0.40Â·I(opp_immediate_win)
    """
    opp_mark = 3 - mark
    
    # Center control difference
    my_center, my_adjacent = count_center_control(board_flat, mark)
    opp_center, opp_adjacent = count_center_control(board_flat, opp_mark)
    center_diff = (my_center + 0.5 * my_adjacent) - (opp_center + 0.5 * opp_adjacent)
    
    # Open threat differences
    my_open2 = count_open_threats(board_flat, mark, 2)
    opp_open2 = count_open_threats(board_flat, opp_mark, 2)
    open2_diff = my_open2 - opp_open2
    
    my_open3 = count_open_threats(board_flat, mark, 3)
    opp_open3 = count_open_threats(board_flat, opp_mark, 3)
    open3_diff = my_open3 - opp_open3
    
    # Immediate win indicators
    my_immediate_win = 1.0 if has_immediate_win(board_flat, mark) else 0.0
    opp_immediate_win = 1.0 if has_immediate_win(board_flat, opp_mark) else 0.0
    
    # Combine into potential function
    potential = (0.02 * center_diff + 
                0.05 * open2_diff + 
                0.12 * open3_diff + 
                0.40 * my_immediate_win - 
                0.40 * opp_immediate_win)
    
    return potential

def calculate_custom_reward_global(prev_board, action, new_board, mark, valid_actions, game_over=False, winner=None, move_count=0, debug=False, gamma=0.99):
    """Enhanced reward function with potential-based shaping for PPO training.
    
    Combines sparse terminal rewards with dense tactical shaping using potential-based
    reward shaping to maintain policy invariance.
    
    Terminal rewards:
    - Win: +1.0 - 0.01 * move_count (prefer faster wins)
    - Loss: -1.0 + 0.01 * move_count (prefer prolonging when losing)  
    - Draw: -0.05 (push for decisive results)
    - Illegal move: -1.0
    
    Dense shaping via potential function:
    - Center control, open threats, immediate wins/losses
    - Applied as: Î³Â·Î¦(s') - Î¦(s) to maintain optimality
    
    Step penalties:
    - Small time penalty: -0.001 per move
    - Block opponent win: +0.20
    - Create blunder: -0.30
    
    Args:
        prev_board: Board state before action (42-length list)
        action: Action taken (0-6)
        new_board: Board state after action (42-length list)  
        mark: Current player mark (1 or 2)
        valid_actions: Legal actions before move
        game_over: Whether game ended this step
        winner: Winner (1, 2, or None for draw)
        move_count: Current move number
        debug: Whether to print debug info
        gamma: Discount factor for potential shaping
        
    Returns:
        float: Total shaped reward clipped to [-1, 1]
    """
    # Start with sparse reward
    sparse_reward = 0.0
    opp_mark = 3 - mark
    
    # 1. Illegal move penalty
    if action not in valid_actions:
        if debug:
            print(f"[DEBUG] Illegal move penalty: action={action}, valid={valid_actions}")
        return -1.0  # Terminal penalty, no need for other calculations
    
    # 2. Terminal rewards (game over)
    if game_over:
        if winner == mark:
            # Win bonus, with preference for faster wins
            sparse_reward = 1.0 - 0.01 * move_count
            if debug:
                print(f"[DEBUG] Win reward: +{sparse_reward:.3f} (move {move_count})")
        elif winner == opp_mark:
            # Loss penalty, slightly less harsh for longer games (reward prolonging)
            sparse_reward = -1.0 + 0.01 * move_count
            if debug:
                print(f"[DEBUG] Loss penalty: {sparse_reward:.3f} (move {move_count})")
        else:
            # Draw - slightly negative to encourage decisive play
            sparse_reward = -0.05
            if debug:
                print(f"[DEBUG] Draw penalty: -0.05")
    
    # 3. Step-based tactical rewards (non-terminal)
    step_reward = 0.0
    
    # Small time penalty to prevent dithering
    step_reward -= 0.001
    
    # Check if we blocked an opponent's immediate win
    try:
        prev_grid = flat_to_2d(prev_board)
        prev_valid_actions = [c for c in range(7) if prev_grid[0][c] == 0]
        
        blocked_win = False
        for opp_action in prev_valid_actions:
            if is_winning_move(prev_board, opp_action, opp_mark) and opp_action == action:
                step_reward += 0.20
                blocked_win = True
                if debug:
                    print(f"[DEBUG] Blocked opponent win: +0.20 (column {action})")
                break
    except Exception:
        pass
    
    # Check if we created a blunder (opponent can win immediately after our move)
    try:
        if not game_over:
            new_grid = flat_to_2d(new_board)
            new_valid_actions = [c for c in range(7) if new_grid[0][c] == 0]
            
            for opp_action in new_valid_actions:
                if is_winning_move(new_board, opp_action, opp_mark):
                    step_reward -= 0.30
                    if debug:
                        print(f"[DEBUG] Created blunder: -0.30 (opponent can win at {opp_action})")
                    break
    except Exception:
        pass
    
    # 4. Potential-based shaping (the main tactical component)
    shaping_reward = 0.0
    try:
        phi_prev = compute_potential_function(prev_board, mark, gamma)
        phi_curr = compute_potential_function(new_board, mark, gamma)
        shaping_reward = gamma * phi_curr - phi_prev
        
        if debug:
            print(f"[DEBUG] Potential shaping: {shaping_reward:.3f} (Î¦_prev={phi_prev:.3f}, Î¦_curr={phi_curr:.3f})")
    except Exception as e:
        if debug:
            print(f"[DEBUG] Potential function failed: {e}")
        shaping_reward = 0.0
    
    # 5. Combine all components
    total_reward = sparse_reward + step_reward + shaping_reward
    
    # 6. Clip to reasonable range to maintain PPO stability
    total_reward = max(-1.0, min(1.0, total_reward))
    
    if debug:
        print(f"[DEBUG] Final reward: {total_reward:.3f} = sparse({sparse_reward:.3f}) + step({step_reward:.3f}) + shaping({shaping_reward:.3f})")
    
    return total_reward

def _worker_init_persistent(agent_cfg):
    # é—œ GPUã€é™åŸ·è¡Œç·’ï¼Œé¿å… CPU oversubscription
    os.environ.setdefault('CUDA_VISIBLE_DEVICES', '')
    os.environ.setdefault('OMP_NUM_THREADS', '1')
    os.environ.setdefault('MKL_NUM_THREADS', '1')
    try:
        torch.set_num_threads(1)
    except Exception:
        pass

    # åˆå§‹åŒ–ä¸€æ¬¡ Agentï¼ˆCPUï¼‰èˆ‡ Env
    agent = PPOAgent(agent_cfg)            # ä½ çš„é¡åˆ¥
    agent.device = torch.device('cpu')
    agent.policy_net.to(agent.device)
    agent.policy_net.eval()

    from kaggle_environments import make    # æ”¾é€™è£¡é¿å…ä¸»é€²ç¨‹ import å½±éŸ¿
    env = make('connectx', debug=False)

    _WORKER["agent"] = agent
    _WORKER["env"] = env
    _WORKER["policy_version"] = -1

def _worker_play_one(args):
    """
    å–®å±€å°æˆ°ï¼›æŒä¹…åŒ– agent/envï¼Œä¸é‡å»ºã€‚
    åƒ…åœ¨æ”¶åˆ°æ›´é«˜ policy_version ä¸”å¤¾å¸¶æ¬Šé‡æ™‚æ‰ load_state_dictã€‚
    """
    try:
        agent = _WORKER["agent"]
        env = _WORKER["env"]

        # æ¬Šé‡æ›´æ–°ï¼ˆå¿…è¦æ™‚ï¼‰
        pv = int(args.get("policy_version", -1))
        weights_np = args.get("policy_state", None)  # åªæœ‰ç‰ˆæœ¬å‰›å‡æ™‚æ‰æœƒå¸¶
        if pv > _WORKER["policy_version"] and weights_np is not None:
            state_dict = {k: torch.from_numpy(v.copy()) if isinstance(v, np.ndarray) else v
                          for k, v in weights_np.items()}
            agent.policy_net.load_state_dict(state_dict, strict=True)
            agent.policy_net.eval()
            _WORKER["policy_version"] = pv

        # æ¯å±€çš„éš¨æ©Ÿæ€§
        seed = args.get('seed', None)
        if seed is not None:
            random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)

        # é–‹å±€
        env.reset()
        
        # ç²å–ç•¶å‰è¨“ç·´é€²åº¦ä¾†é¸æ“‡å°æ‰‹é›£åº¦
        current_win_rate = args.get('current_win_rate', 0.0)
        
        # æ ¹æ“šå‹ç‡é¸æ“‡å°æ‰‹ç­–ç•¥
        if current_win_rate < 0.2:
            # åˆæœŸï¼šç´”éš¨æ©Ÿå’Œåªæœƒè´çš„å°æ‰‹
            opponent_type = random.choice(['pure_random', 'win_only'])
        elif current_win_rate < 0.4:
            # é€²éšï¼šåŠ å…¥åªæœƒé˜²å®ˆçš„å°æ‰‹å’Œæˆ°è¡“é–‹å±€
            opponent_type = random.choice(['pure_random', 'win_only', 'defensive_only', 'tactical_opening'])
        elif current_win_rate < 0.55:
            # ä¸­æœŸï¼šåŠ å…¥åå¥½ä¸­å¤®å’Œå¼±æˆ°è¡“å°æ‰‹ï¼Œå¢åŠ æˆ°è¡“é–‹å±€æ¯”ä¾‹
            opponent_type = random.choice(['win_only', 'defensive_only', 'center_bias', 'weak_tactical', 'tactical_opening', 'tactical_opening'])
        elif current_win_rate < 0.7:
            # å¾ŒæœŸï¼šåŠ å…¥å®¹æ˜“çŠ¯éŒ¯çš„å°æ‰‹ï¼Œå¼·åŒ–æˆ°è¡“é–‹å±€
            opponent_type = random.choice(['center_bias', 'weak_tactical', 'mistake_prone', 'tactical_opening', 'tactical_opening'])
        else:
            # é«˜éšï¼šä½¿ç”¨å®Œæ•´æˆ°è¡“å°æ‰‹ï¼Œä¸»è¦ä½¿ç”¨æˆ°è¡“é–‹å±€
            opponent_type = random.choice(['random', 'minimax', 'self', 'tactical_opening', 'tactical_opening', 'tactical_opening'])
        
        player2_prob = float(args.get('player2_training_prob', 0.5))
        
        # ç‰¹æ®Šè™•ç†ï¼šå¦‚æœä½¿ç”¨æˆ°è¡“é–‹å±€å°æ‰‹ï¼Œå¼·åˆ¶å°æ‰‹ç‚º player 1ï¼Œè¨“ç·´ç©å®¶ç‚º player 2
        if opponent_type == 'tactical_opening':
            training_player = 2  # è¨“ç·´ç©å®¶å¼·åˆ¶ç‚ºå¾Œæ‰‹
        else:
            training_player = int(np.random.choice([1, 2], p=[1.0 - player2_prob, player2_prob]))

        transitions = []
        move_count, max_moves = 0, 50

        with torch.no_grad():
            while not env.done and move_count < max_moves:
                actions = []
                # è¨˜éŒ„æœ¬å›åˆé–‹å§‹æ™‚çš„æ£‹ç›¤ç‹€æ…‹
                round_start_board = None
                for player_idx in range(2):
                    if env.state[player_idx]['status'] == 'ACTIVE':
                        board, current_player = agent.extract_board_and_mark(env.state, player_idx)
                        valid_actions = agent.get_valid_actions(board)
                        
                        # è¨˜éŒ„è¨“ç·´ç©å®¶å‹•ä½œå‰çš„æ£‹ç›¤ç‹€æ…‹
                        if current_player == training_player:
                            round_start_board = board.copy()
                        
                        if current_player == training_player:
                            state = agent.encode_state(board, current_player)
                            action, prob, value = agent.select_action(state, valid_actions, training=True)
                            
                            # è¨˜éŒ„å®Œæ•´ä¿¡æ¯ä»¥ä¾¿è¨ˆç®—è‡ªå®šç¾©çå‹µ
                            transition_data = {
                                'state': state,
                                'action': int(action),
                                'prob': float(prob),
                                'value': float(value),
                                'board_before': board.copy(),  # å‹•ä½œå‰æ£‹ç›¤
                                'valid_actions': valid_actions.copy(),  # åˆæ³•å‹•ä½œ
                                'mark': current_player,  # ç©å®¶æ¨™è¨˜
                                'training_player': training_player,
                                'opponent_type': opponent_type,
                                'is_dangerous': bool(if_i_will_lose_at_next(board, int(action), current_player, agent)),
                                'move_index': move_count,  # å›åˆç´¢å¼•
                            }
                            transitions.append(transition_data)
                        else:
                            # æ ¹æ“šé¸å®šçš„å°æ‰‹é¡å‹é¸æ“‡ç­–ç•¥
                            if opponent_type == 'pure_random':
                                action = pure_random_opponent_strategy(board, current_player, valid_actions, agent)
                            elif opponent_type == 'win_only':
                                action = basic_win_only_opponent_strategy(board, current_player, valid_actions, agent)
                            elif opponent_type == 'defensive_only':
                                action = defensive_only_opponent_strategy(board, current_player, valid_actions, agent)
                            elif opponent_type == 'center_bias':
                                action = center_bias_opponent_strategy(board, current_player, valid_actions, agent)
                            elif opponent_type == 'weak_tactical':
                                action = weak_tactical_opponent_strategy(board, current_player, valid_actions, agent)
                            elif opponent_type == 'mistake_prone':
                                action = mistake_prone_opponent_strategy(board, current_player, valid_actions, agent)
                            elif opponent_type == 'tactical_opening':
                                # æˆ°è¡“é–‹å±€å°æ‰‹ï¼šå¼·åˆ¶å…ˆæ‰‹ä¸¦ä½¿ç”¨ 3->4->2 é–‹å±€ï¼Œç„¶å¾Œæˆ°è¡“é‚è¼¯
                                # åªæœ‰ç•¶æ­¤å°æ‰‹æ˜¯ player 1 (mark==1) æ™‚æ‰ä½¿ç”¨é–‹å±€ç­–ç•¥
                                if current_player == 1:
                                    action = if_i_can_win(board, current_player, agent)
                                    if action is None:
                                        action = if_i_will_lose(board, current_player, agent)
                                    if action is None:
                                        # æª¢æŸ¥é–‹å±€åºåˆ— 3->4->2
                                        grid = flat_to_2d(board)
                                        my_tokens = sum(1 for r in range(6) for c in range(7) if grid[r][c] == 1)
                                        if my_tokens == 0 and 3 in valid_actions:
                                            action = 3
                                        elif my_tokens == 1 and 4 in valid_actions:
                                            action = 4
                                        elif my_tokens == 2 and 2 in valid_actions:
                                            action = 2
                                        else:
                                            # é–‹å±€å®Œæˆå¾Œä½¿ç”¨å®‰å…¨ç­–ç•¥
                                            safe = safe_moves(board, current_player, valid_actions, agent)
                                            action = random.choice(safe) if safe else random.choice(valid_actions)
                                else:
                                    # å¦‚æœä¸æ˜¯å…ˆæ‰‹ï¼Œä½¿ç”¨æ™®é€šæˆ°è¡“ç­–ç•¥
                                    action = random_opponent_strategy(board, current_player, valid_actions, agent)
                            elif opponent_type == 'random':
                                action = random_opponent_strategy(board, current_player, valid_actions, agent)
                            elif opponent_type == 'minimax':
                                action = minimax_opponent_strategy(board, current_player, valid_actions, agent, depth=3)
                            else:  # self
                                action = self_play_opponent_strategy(board, current_player, valid_actions, agent)
                        actions.append(int(action))
                    else:
                        actions.append(0)
                
                # åŸ·è¡Œå‹•ä½œ
                try:
                    env.step(actions)
                except Exception:
                    break
                
                # è¨˜éŒ„å‹•ä½œå¾Œçš„æ£‹ç›¤ç‹€æ…‹ï¼ˆé‡å°è¨“ç·´ç©å®¶çš„æœ€å¾Œä¸€å€‹ transitionï¼‰
                if transitions and round_start_board is not None:
                    try:
                        # ç²å–å‹•ä½œå¾Œçš„æ£‹ç›¤ç‹€æ…‹
                        post_board, _ = agent.extract_board_and_mark(env.state, 0)  # ä½¿ç”¨player 0è¦–è§’ç²å–æ£‹ç›¤
                        transitions[-1]['board_after'] = post_board.copy()
                    except Exception:
                        transitions[-1]['board_after'] = round_start_board.copy()  # é€€åŒ–åˆ°å‹•ä½œå‰ç‹€æ…‹
                
                move_count += 1

        try:
            player_result = env.state[0]['reward'] if training_player == 1 else env.state[1]['reward']
        except Exception:
            player_result = 0

        try:
            player_result = env.state[0]['reward'] if training_player == 1 else env.state[1]['reward']
        except Exception:
            player_result = 0

        # è¨ˆç®—è‡ªå®šç¾©çå‹µä¸¦åˆ†é…çµ¦æ‰€æœ‰ transitions
        final_transitions = []
        for i, transition in enumerate(transitions):
            # è¨ˆç®—åŸºç¤ç’°å¢ƒçå‹µ
            base_reward = float(player_result)
            
            # è¨ˆç®—è‡ªå®šç¾©çå‹µï¼ˆä½¿ç”¨æ–°çš„å…¨å±€å‡½æ•¸ï¼‰
            custom_reward = 0.0
            if 'board_before' in transition and 'board_after' in transition:
                try:
                    board_before = transition['board_before']
                    board_after = transition['board_after']
                    action = transition['action']
                    valid_actions = transition['valid_actions']
                    mark = transition['mark']
                    move_index = transition['move_index']
                    total_moves = len(transitions)
                    
                    # åˆ¤æ–·éŠæˆ²æ˜¯å¦åœ¨æ­¤æ­¥çµæŸ
                    is_last_move = (i == len(transitions) - 1)
                    game_over = is_last_move
                    winner = None
                    if game_over:
                        if player_result == 1:
                            winner = mark  # æˆ‘æ–¹å‹åˆ©
                        elif player_result == -1:
                            winner = 3 - mark  # å°æ–¹å‹åˆ©
                        # player_result == 0 æ™‚ winner ä¿æŒ None (å¹³å±€)
                    
                    # ä½¿ç”¨æ–°çš„å…¨å±€è‡ªå®šç¾©çå‹µå‡½æ•¸
                    custom_reward = calculate_custom_reward_global(
                        prev_board=board_before,
                        action=action,
                        new_board=board_after,
                        mark=mark,
                        valid_actions=valid_actions,
                        game_over=game_over,
                        winner=winner,
                        move_count=i+1,
                        gamma=getattr(agent, 'gamma', 0.99),  # Use agent's gamma if available
                        debug=False  # Set to True for detailed reward logging
                    )
                    
                except Exception as e:
                    # å¦‚æœè‡ªå®šç¾©çå‹µè¨ˆç®—å¤±æ•—ï¼Œä½¿ç”¨é»˜èªå€¼
                    custom_reward = 0.0
            
            # åˆä½µåŸºç¤çå‹µå’Œè‡ªå®šç¾©çå‹µ
            final_reward = base_reward + custom_reward
            
            # ç‚ºæ¯å€‹transitionæ·»åŠ æœ€çµ‚çå‹µ
            transition['reward'] = float(final_reward)
            transition['custom_reward'] = float(custom_reward)
            transition['base_reward'] = float(base_reward)
            final_transitions.append(transition)

        return {
            'transitions': final_transitions,
            'training_player': training_player,
            'player_result': int(player_result),
            'game_length': len(final_transitions),
            'opponent_type': opponent_type,
            'policy_version_used': _WORKER["policy_version"],
            'custom_rewards_applied': len([t for t in final_transitions if 'custom_reward' in t])
        }
    except Exception as e:
        return {
            'transitions': [],
            'training_player': 1,
            'player_result': 0,
            'game_length': 0,
            'opponent_type': 'unknown',
            'error': str(e),
            'policy_version_used': _WORKER["policy_version"],
        }


class DropPath(nn.Module):
    """Stochastic Depth per sample (when training only)."""
    def __init__(self, drop_prob: float = 0.0):
        super().__init__()
        self.drop_prob = float(max(0.0, drop_prob))
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.drop_prob == 0.0 or not self.training:
            return x
        keep_prob = 1.0 - self.drop_prob
        # shape: (N, 1, 1, 1) for broadcasting
        mask = torch.empty((x.shape[0], 1, 1, 1), dtype=x.dtype, device=x.device).bernoulli_(keep_prob)
        return x.div(keep_prob) * mask


class ConnectXNet(nn.Module):
    """Advanced ConnectX policy-value network.
    Enhancements:
      - Coordinate embedding (row/col planes)
      - Wider channel width (128)
      - Bottleneck residual blocks with Squeeze-and-Excitation (SE)
      - Stochastic Depth (DropPath) per block
      - Periodic lightweight spatial self-attention (every attn_every blocks)
      - Shared trunk then dual heads (policy softmax, value tanh)
    Input remains flat 126 (3x6x7); coord planes added internally (->5 channels) for compatibility.
    """

    def __init__(self, input_size=126, hidden_size=192, num_layers=256, drop_path_rate: float = 0.08, attn_every: int = 4):
        super().__init__()

        print(f'input_size: {input_size}, hidden_size: {hidden_size}')
        # Clamp depth
        max_blocks = 32
        blocks = int(num_layers) if isinstance(num_layers, int) else max_blocks
        if blocks > max_blocks:
            try:
                logger.warning(f"num_layers={blocks} éæ·±ï¼Œç¸®æ¸›ç‚º {max_blocks}")
            except Exception:
                pass
            blocks = max_blocks
        self.blocks = blocks
        # èª¿æ•´channelsä»¥é…åˆattention heads (24çš„å€æ•¸)
        # 24çš„æœ€è¿‘å€æ•¸: 120, 144, 168, 192
        self.channels = 144  # 144 = 24 * 6ï¼Œç¢ºä¿headsèƒ½æ•´é™¤
        self.drop_path_rate = float(max(0.0, drop_path_rate))
        # ç”±æ–¼å¢åŠ äº†attention headsï¼Œå¯ä»¥æ›´é »ç¹åœ°ä½¿ç”¨attention
        self.attn_every = max(1, min(int(attn_every), 3))  # é™åˆ¶åœ¨1-3ä¹‹é–“ï¼Œæ›´é »ç¹çš„attention

        # Coordinate embedding (2,6,7)
        row = torch.linspace(-1, 1, steps=6).unsqueeze(1).repeat(1, 7)
        col = torch.linspace(-1, 1, steps=7).unsqueeze(0).repeat(6, 1)
        coord = torch.stack([row, col], dim=0)  # (2,6,7)
        self.register_buffer('coord_embed', coord)

        in_ch = 3 + 2  # original planes + coord planes
        self.stem = nn.Sequential(
            nn.Conv2d(in_ch, self.channels, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(8, self.channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(p=0.05),
        )

        # --- Building blocks ---
        class BottleneckSE(nn.Module):
            def __init__(self, c, drop_path=0.0, reduction=4):
                super().__init__()
                mid = max(32, c // 2)
                self.conv1 = nn.Conv2d(c, mid, 1, bias=False)
                self.gn1 = nn.GroupNorm(8, mid)
                self.conv2 = nn.Conv2d(mid, mid, 3, padding=1, bias=False)
                self.gn2 = nn.GroupNorm(8, mid)
                self.conv3 = nn.Conv2d(mid, c, 1, bias=False)
                self.gn3 = nn.GroupNorm(8, c)
                # SE
                se_hidden = max(8, c // reduction)
                self.se_fc1 = nn.Linear(c, se_hidden)
                self.se_fc2 = nn.Linear(se_hidden, c)
                self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()
                self.act = nn.ReLU(inplace=True)
            def forward(self, x):
                identity = x
                out = self.act(self.gn1(self.conv1(x)))
                out = self.act(self.gn2(self.conv2(out)))
                out = self.gn3(self.conv3(out))
                # Squeeze Excitation
                w = out.mean(dim=[2,3])          # (B,C)
                w = self.act(self.se_fc1(w))
                w = torch.sigmoid(self.se_fc2(w)).unsqueeze(-1).unsqueeze(-1)  # (B,C,1,1)
                out = out * w
                out = self.drop_path(out)
                out = self.act(out + identity)
                return out

        class SpatialSelfAttention(nn.Module):
            def __init__(self, c, heads=24):
                super().__init__()
                # ç¢ºä¿headsèƒ½æ•´é™¤cï¼Œå¦‚æœä¸èƒ½å°±èª¿æ•´åˆ°æœ€æ¥è¿‘çš„å› æ•¸
                if c % heads != 0:
                    # æ‰¾åˆ°æœ€æ¥è¿‘çš„å› æ•¸
                    possible_heads = [i for i in range(1, c + 1) if c % i == 0]
                    heads = min(possible_heads, key=lambda x: abs(x - heads))
                    print(f"èª¿æ•´attention heads: {heads} (channels={c})")
                
                self.heads = heads
                self.head_dim = c // heads
                self.scale = self.head_dim ** -0.5
                self.qkv = nn.Conv2d(c, c * 3, 1, bias=False)
                self.proj = nn.Conv2d(c, c, 1, bias=False)
                
            def forward(self, x):  # x: (B,C,H,W)
                B, C, H, W = x.shape
                qkv = self.qkv(x).reshape(B, 3, self.heads, self.head_dim, H * W)
                q, k, v = qkv[:,0], qkv[:,1], qkv[:,2]  # (B,heads,head_dim,HW)
                attn = (q.transpose(-2,-1) @ k) * self.scale  # (B,heads,HW,HW)
                attn = torch.softmax(attn, dim=-1)
                out = (attn @ v.transpose(-2,-1)).transpose(-2,-1)  # (B,heads,head_dim,HW)
                out = out.reshape(B, C, H, W)
                return self.proj(out)

        # Assemble trunk
        dprates = torch.linspace(0, self.drop_path_rate, steps=self.blocks).tolist() if self.drop_path_rate > 0 else [0.0]*self.blocks
        self.trunk = nn.ModuleList()
        for i in range(self.blocks):
            self.trunk.append(BottleneckSE(self.channels, drop_path=dprates[i]))
            if self.attn_every > 0 and (i+1) % self.attn_every == 0:
                self.trunk.append(SpatialSelfAttention(self.channels))

        # Head
        self.head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.channels * 6 * 7, hidden_size),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
        )
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_size // 2, 7),
            nn.Softmax(dim=-1),
        )
        self.value_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_size // 2, 1),
            nn.Tanh(),
        )

        # Init
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.GroupNorm):
                if m.weight is not None:
                    nn.init.ones_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        B = x.size(0)
        x = x.view(B, 3, 6, 7)
        # concat coord embeddings
        coord = self.coord_embed.unsqueeze(0).expand(B, -1, -1, -1)
        x = torch.cat([x, coord], dim=1)  # (B,5,6,7)
        y = self.stem(x)
        for block in self.trunk:
            y = block(y)
        feat = self.head(y)
        policy = self.policy_head(feat)
        value = self.value_head(feat)
        return policy, value

class PPOAgent:
    """PPO å¼·åŒ–å­¸ç¿’æ™ºèƒ½é«”"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ç¥ç¶“ç¶²è·¯
        self.policy_net = ConnectXNet(
            input_size=config['input_size'],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers']
        ).to(self.device)

        # è¼‰å…¥é è¨“ç·´æ¨¡å‹ (å¦‚æœæŒ‡å®š)
        pretrained_path = config.get('pretrained_model_path')
        if pretrained_path:
            self.load_pretrained_model(pretrained_path)
        # KL regularizer anchor/net + hyperparams
        kl_cfg = {}
        try:
            if isinstance(config, dict):
                kl_cfg = config.get('kl_regularizer', config.get('kl', {})) or {}
        except Exception:
            kl_cfg = {}
        self.kl_coef = float(kl_cfg.get('coef', 0.02))  # small anchor to imitation
        self.kl_decay = float(kl_cfg.get('decay', 0.999))
        self.kl_min_coef = float(kl_cfg.get('min_coef', 0.001))
        self.kl_mask_invalid = bool(kl_cfg.get('mask_invalid', True))
        self.anchor_net: nn.Module | None = None

        # å„ªåŒ–å™¨
        self.optimizer = optim.Adam(
            self.policy_net.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )

        # å­¸ç¿’ç‡èª¿åº¦å™¨ - æ ¹æ“šæ€§èƒ½å‹•æ…‹èª¿æ•´
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',  # ç›£æ§å‹ç‡æœ€å¤§åŒ–
            factor=0.7,  # å­¸ç¿’ç‡é™ä½å› å­
            patience=1000,  # ç­‰å¾…å›åˆæ•¸
            min_lr=1e-8
        )

        # è¨“ç·´åƒæ•¸
        self.gamma = config['gamma']
        self.eps_clip = config['eps_clip']
        self.k_epochs = config['k_epochs']
        self.entropy_coef = config['entropy_coef']
        self.value_coef = config['value_coef']

        # ç¶“é©—ç·©è¡å€
        self.memory = deque(maxlen=config['buffer_size'])

        # --- Stuck detection / partial reset settings ---
        reset_cfg = config.get('reset', {}) if isinstance(config, dict) else {}
        from collections import deque as _dq
        self.entropy_window = _dq(maxlen=int(reset_cfg.get('entropy_window', 30)))
        self.entropy_threshold = float(reset_cfg.get('entropy_threshold', 0.9))  # ln(7)â‰ˆ1.95, 0.9 is fairly sharp
        self.low_entropy_patience = int(reset_cfg.get('low_entropy_patience', 30))
        self.reset_cooldown_updates = int(reset_cfg.get('cooldown_updates', 200))
        self.partial_reset_fraction = float(reset_cfg.get('fraction', 0.25))  # reset fraction of residual blocks
        self.update_step = 0
        self.last_partial_reset_update = -10**9
        # Ensure numpy alias
        self._np = np

    # --- KL anchor helpers ---
    def set_anchor_from_current(self):
        """Freeze a copy of current policy as KL anchor (CPU, eval)."""
        try:
            self.anchor_net = deepcopy(self.policy_net).to('cpu')
            for p in self.anchor_net.parameters():
                p.requires_grad_(False)
            self.anchor_net.eval()
            try:
                logger.info("ğŸ”’ KL anchor set from current policy")
            except Exception:
                pass
        except Exception as e:
            try:
                logger.warning(f"Failed to set KL anchor: {e}")
            except Exception:
                pass

    def _compute_valid_mask_from_states(self, states_tensor: torch.Tensor) -> torch.Tensor:
        """Derive legal-action mask [B,7] from encoded states (3x6x7 planes)."""
        try:
            B = states_tensor.size(0)
            planes = states_tensor.view(B, 3, 6, 7)
            occ = (planes[:, 0] + planes[:, 1])  # (B,6,7)
            top_occ = occ[:, 0, :]  # (B,7), 1 if occupied
            mask = (top_occ <= 0).to(states_tensor.dtype)
            return mask
        except Exception:
            # Fallback: all legal
            return torch.ones((states_tensor.size(0), 7), dtype=states_tensor.dtype, device=states_tensor.device)

    def _kl_loss(self, curr_probs: torch.Tensor, states_tensor: torch.Tensor) -> torch.Tensor:
        """Compute mean KL(anchor || current) over batch, optionally masking invalid cols.
        curr_probs: [B,7] softmax outputs from current policy.
        """
        if self.anchor_net is None or self.kl_coef <= 0:
            return torch.tensor(0.0, device=states_tensor.device, dtype=states_tensor.dtype)
        with torch.no_grad():
            # Anchor on CPU; run in no-grad and move to device for math
            anchor_probs, _ = self.anchor_net(states_tensor.detach().to('cpu'))
            anchor_probs = anchor_probs.to(states_tensor.device)
        p = anchor_probs.clamp_min(1e-8)
        q = curr_probs.clamp_min(1e-8)
        if self.kl_mask_invalid:
            mask = self._compute_valid_mask_from_states(states_tensor)  # [B,7]
            # re-normalize on masked support
            p = p * mask
            q = q * mask
            # avoid degenerate all-zero rows -> uniform over all actions
            p = p / (p.sum(dim=1, keepdim=True) + 1e-8)
            q = q / (q.sum(dim=1, keepdim=True) + 1e-8)
        kl = (p * (p.add(1e-8).log() - q.add(1e-8).log())).sum(dim=1)
        return kl.mean()

    def _decay_kl_coef(self):
        try:
            if self.kl_coef > 0:
                self.kl_coef = max(self.kl_min_coef, self.kl_coef * self.kl_decay)
        except Exception:
            pass

    def load_pretrained_model(self, model_path: str):
        """
        è¼‰å…¥é è¨“ç·´çš„æ¨¡ä»¿å­¸ç¿’æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘
        """
        if not os.path.exists(model_path):
            logger.warning(f"é è¨“ç·´æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
            return False
        
        try:
            # è¼‰å…¥æ¨¡å‹
            checkpoint = torch.load(model_path, map_location=self.device)
            
            if 'model_state_dict' in checkpoint:
                # å®Œæ•´çš„checkpointæ ¼å¼
                model_state = checkpoint['model_state_dict']
                epoch = checkpoint.get('epoch', 0)
                loss = checkpoint.get('loss', 0.0)
                logger.info(f"è¼‰å…¥é è¨“ç·´æ¨¡å‹: {model_path} (epoch={epoch}, loss={loss:.4f})")
            else:
                # åƒ…æ¨¡å‹ç‹€æ…‹æ ¼å¼
                model_state = checkpoint
                logger.info(f"è¼‰å…¥é è¨“ç·´æ¨¡å‹ç‹€æ…‹: {model_path}")
            
            # è¼‰å…¥æ¬Šé‡
            self.policy_net.load_state_dict(model_state)
            
            logger.info("âœ… æˆåŠŸè¼‰å…¥é è¨“ç·´æ¨¡å‹ï¼æ¨¡å‹å·²æº–å‚™å¥½é€²è¡ŒRLè¨“ç·´")
            # Set KL anchor to this imitation policy
            self.set_anchor_from_current()
            return True
            
        except Exception as e:
            logger.error(f"è¼‰å…¥é è¨“ç·´æ¨¡å‹å¤±æ•—: {e}")
            logger.info("å°‡ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–çš„æ¨¡å‹ç¹¼çºŒè¨“ç·´")
            return False

    # === Kaggle env helpers used by trainer ===
    def extract_board_and_mark(self, env_state, player_idx):
        """Return (board_flat_list, mark) from kaggle_environments state list.
        board: list[int] length 42, row-major 6x7, 0 empty, 1/2 marks.
        mark: 1 or 2 for the given player index.
        """
        try:
            obs = env_state[player_idx]['observation']
            board = obs.get('board') or obs.get('board_state')
            mark = obs.get('mark') or (1 if player_idx == 0 else 2)
            return list(board), int(mark)
        except Exception:
            # Fallback: try top-level
            try:
                board = env_state['board']
                mark = env_state.get('mark', 1)
                return list(board), int(mark)
            except Exception:
                return [0] * 42, 1

    def encode_state(self, board, mark):
        """Encode board (len=42) and mark (1/2) into 126-d float state (3x6x7).
        Planes: [my pieces, opp pieces, ones].
        """
        try:
            grid = [board[r * 7:(r + 1) * 7] for r in range(6)]
            my = np.zeros((6, 7), dtype=np.float32)
            opp = np.zeros((6, 7), dtype=np.float32)
            for r in range(6):
                for c in range(7):
                    v = grid[r][c]
                    if v == mark:
                        my[r, c] = 1.0
                    elif v != 0:
                        opp[r, c] = 1.0
            ones = np.ones((6, 7), dtype=np.float32)
            stacked = np.stack([my, opp, ones], axis=0)
            return stacked.reshape(-1)
        except Exception:
            return np.zeros(126, dtype=np.float32)

    def get_valid_actions(self, board):
        """Valid columns where top cell is empty."""
        try:
            grid_top = [board[c] for c in range(7)]  # row 0
            return [c for c in range(7) if grid_top[c] == 0]
        except Exception:
            # Generic fallback
            valid = []
            try:
                for c in range(7):
                    col_full = True
                    for r in range(6):
                        if board[r * 7 + c] == 0:
                            col_full = False
                            break
                    if not col_full:
                        valid.append(c)
            except Exception:
                valid = list(range(7))
            return valid

    def select_action(self, state, valid_actions, training=True, temperature=1.0, exploration_bonus=0.0):
        """Return (action, prob, value). Masks invalid actions and samples during training.
        temperature <= 0 or not training -> greedy.
        """
        self.policy_net.eval()  # eval OK for GroupNorm with small batch; sampling unaffected
        with torch.no_grad():
            s = torch.as_tensor(state, dtype=torch.float32, device=self.device).view(1, -1)
            probs, value = self.policy_net(s)
            probs = probs.squeeze(0).clamp_min(1e-8)
            mask = torch.zeros_like(probs)
            if not valid_actions:
                valid_actions = list(range(7))
            mask[valid_actions] = 1.0
            masked = probs * mask
            if masked.sum() <= 0:
                # fallback to uniform over valid
                masked = mask / mask.sum()
            else:
                masked = masked / masked.sum()
            if training and temperature is not None and temperature > 1e-6:
                logits = torch.log(masked + 1e-8) / float(temperature)
                dist = torch.distributions.Categorical(logits=logits)
            else:
                dist = torch.distributions.Categorical(probs=masked)
            action = int(dist.sample().item())
            prob = float(masked[action].item())
            return action, prob, float(value.item())

    def get_action_scores(self, state, valid_actions):
        """å–å¾—ç›®å‰ç­–ç•¥å°æ¯å€‹å‹•ä½œçš„æ‰“åˆ† / æ¦‚ç‡èˆ‡ valueã€‚

        Args:
            state: 126/flattened state (æˆ– list/np.array)
            valid_actions: å¯è½å­åˆ—è¡¨

        Returns dict:
            {
              'valid_actions': [...],
              'raw_policy': list[7],          # åŸå§‹ policy_net softmax è¼¸å‡º
              'masked_policy': list[7],       # åªåœ¨ valid ä¸Šé‡æ–°æ­£è¦åŒ–å¾Œçš„åˆ†ä½ˆ
              'logits': list[7],              # å°æ‡‰ masked_policy çš„ log(prob)
              'value': float,                 # ç‹€æ…‹åƒ¹å€¼ V(s)
              'entropy': float,               # masked distribution entropy
              'action_ranking': [(action, prob), ...]  # ä¾ masked prob ç”±é«˜åˆ°ä½
            }
        """
        self.policy_net.eval()
        with torch.no_grad():
            s = torch.as_tensor(state, dtype=torch.float32, device=self.device).view(1, -1)
            probs, value = self.policy_net(s)  # probs: (1,7)
            probs = probs.squeeze(0).clamp_min(1e-8)
            if not valid_actions:
                valid_actions = list(range(7))
            mask = torch.zeros_like(probs)
            mask[valid_actions] = 1.0
            masked = probs * mask
            if masked.sum() <= 0:
                masked = mask / mask.sum()  # uniform on valid
            else:
                masked = masked / masked.sum()
            entropy = float(-(masked * (masked.add(1e-8).log())).sum().item())
            ranking = sorted([(int(a), float(masked[a].item())) for a in range(7)], key=lambda x: x[1], reverse=True)
            return {
                'valid_actions': list(map(int, valid_actions)),
                'raw_policy': [float(p) for p in probs.tolist()],
                'masked_policy': [float(p) for p in masked.tolist()],
                'logits': [float(math.log(p + 1e-8)) for p in masked.tolist()],
                'value': float(value.item()),
                'entropy': entropy,
                'action_ranking': ranking,
            }

    def debug_print_action_scores(self, board, mark):
        """ä¾¿åˆ©å‡½å¼ï¼šç›´æ¥è¼¸å‡ºæŸç›¤é¢ä¸‹æ¯å€‹å‹•ä½œæ¦‚ç‡/æ’åã€‚"""
        try:
            state = self.encode_state(board, mark)
        except Exception:
            return
        valid = self.get_valid_actions(board)
        info = self.get_action_scores(state, valid)
        logger.info(
            "[ActionScores] mark=%s value=%.4f entropy=%.3f ranking=%s", 
            mark, info['value'], info['entropy'], info['action_ranking']
        )
        return info

    def store_transition(self, state, action, prob, reward, done):
        self.memory.append((state, action, prob, reward, done))

    def compute_gae(self, rewards, values, dones, next_value, gamma=0.99, lam=0.95):
        T = len(rewards)
        adv = np.zeros(T, dtype=np.float32)
        last_gae = 0.0
        for t in reversed(range(T)):
            v_next = values[t + 1] if t + 1 < T else next_value
            mask = 1.0 - float(dones[t])
            delta = rewards[t] + gamma * v_next * mask - values[t]
            last_gae = delta + gamma * lam * mask * last_gae
            adv[t] = last_gae
        returns = adv + np.array(values[:T], dtype=np.float32)
        return adv.tolist(), returns.tolist()

    def partial_reset(self, which: str = 'res_blocks_and_head', fraction: float | None = None):
        """Reinitialize a subset of the network to escape local minima.
        which: 'res_blocks', 'head', 'res_blocks_and_head'
        fraction: portion of residual blocks to reset (default from config)
        """
        fraction = self.partial_reset_fraction if fraction is None else float(fraction)
        net: ConnectXNet = self.policy_net  # type: ignore
        reset_count = 0
        
        # Reset a random subset of trunk blocks
        if which in ('res_blocks', 'res_blocks_and_head'):
            trunk_blocks = list(net.trunk)
            import random as _rnd
            k = max(1, int(len(trunk_blocks) * max(0.0, min(1.0, fraction))))
            idxs = _rnd.sample(range(len(trunk_blocks)), k)
            for i in idxs:
                block = trunk_blocks[i]
                # Reinitialize parameters for this block
                for m in block.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
                    elif isinstance(m, nn.Linear):
                        nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
                    elif isinstance(m, nn.GroupNorm):
                        if m.weight is not None:
                            nn.init.ones_(m.weight)
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
                reset_count += 1
        
        # Optionally reset head and policy head
        if which in ('head', 'res_blocks_and_head'):
            for m in net.head.modules():
                if isinstance(m, nn.Linear):
                    nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
            for m in net.policy_head.modules():
                if isinstance(m, nn.Linear):
                    nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
            # Value head left intact to preserve value estimates
        
        try:
            logger.info(f"ğŸ”„ Partial reset applied: which={which}, fraction={fraction:.2f}, reset_blocks={reset_count}")
        except Exception:
            pass
        self.last_partial_reset_update = self.update_step
   
    def update_policy_from_batch(self, states, actions, old_action_probs, rewards, dones, is_weights):
        """
        ä½¿ç”¨çµ¦å®šçš„æ‰¹æ¬¡æ•¸æ“šé€²è¡ŒPPOæ›´æ–°ï¼Œæ”¯æŒé‡è¦æ€§åŠ æ¬Šï¼ˆç”¨æ–¼PERï¼‰
        
        Args:
            states: list[tensor or np], æœƒåœ¨å…§éƒ¨stackåˆ°device
            actions: np[int64] shape [B]
            old_action_probs: np[float32] shape [B]
            rewards: np[float32] shape [B]
            dones: np[bool] shape [B]
            is_weights: torch.float32 shape [B]  # é‡è¦æ€§åŠ æ¬Š

        Returns:
            dict(total_loss=..., td_errors_abs=np.ndarray[B])
        """
        batch_size = len(states)
        
        # 1) å°‡statesè½‰æ›ç‚ºå¼µé‡ä¸¦ç§»åˆ°device
        try:
            # è™•ç†ä¸åŒé¡å‹çš„statesè¼¸å…¥
            if isinstance(states[0], np.ndarray):
                states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
            elif isinstance(states[0], torch.Tensor):
                states_tensor = torch.stack(states).to(self.device)
            else:
                # å‡è¨­æ˜¯list of float lists
                states_tensor = torch.FloatTensor(states).to(self.device)
        except Exception:
            # å¾Œå‚™æ–¹æ¡ˆ
            states_tensor = torch.FloatTensor(np.array(states)).to(self.device)

        # 2) è¨ˆç®—ç•¶å‰ç­–ç•¥ç¶²è·¯çš„å€¼
        with torch.no_grad():
            _, values_tensor = self.policy_net(states_tensor)
            values = values_tensor.cpu().numpy().flatten()

        # 3) è¨ˆç®—å„ªå‹¢å’Œå›å ±ä½¿ç”¨GAE
        advantages, returns = self.compute_gae(
            rewards.tolist(), values.tolist(), dones.tolist(), 0,
            self.gamma, self.config['gae_lambda']
        )

        # 4) æ­£è¦åŒ–å„ªå‹¢
        advantages = np.array(advantages)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # 5) è½‰æ›ç‚ºå¼µé‡
        actions_tensor = torch.LongTensor(actions).to(self.device)
        old_probs_tensor = torch.FloatTensor(old_action_probs).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        returns_tensor = torch.FloatTensor(returns).to(self.device)

        # 6) PPOæ›´æ–°å¤šå€‹epochs
        total_loss = 0.0
        entropy_sum = 0.0
        policy_loss_sum = 0.0
        value_loss_sum = 0.0
        
        for epoch in range(self.k_epochs):
            # å‰å‘å‚³æ’­
            new_probs, values = self.policy_net(states_tensor)

            # è¨ˆç®—å‹•ä½œæ¦‚ç‡æ¯”ç‡
            new_action_probs = new_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze()
            ratio = new_action_probs / (old_probs_tensor + 1e-8)

            # PPO clipæå¤±
            surr1 = ratio * advantages_tensor
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages_tensor
            policy_loss = -torch.min(surr1, surr2)
            
            # åƒ¹å€¼æå¤±
            value_loss = nn.MSELoss(reduction='none')(values.squeeze(), returns_tensor)

            # ç†µæå¤±ï¼ˆé¼“å‹µæ¢ç´¢ï¼‰
            entropy = -(new_probs * torch.log(new_probs + 1e-8)).sum(dim=1)

            # KL(anchor || current) regularization (masked by legality)
            kl_term = self._kl_loss(new_probs, states_tensor)
            
            # æ‡‰ç”¨é‡è¦æ€§åŠ æ¬Šï¼ˆPERçš„ä¿®æ­£ï¼‰
            weighted_policy_loss = (policy_loss * is_weights).mean()
            weighted_value_loss = (value_loss * is_weights).mean()
            weighted_entropy = (entropy * is_weights).mean()

            # ç¸½æå¤±
            loss = weighted_policy_loss + self.value_coef * weighted_value_loss - self.entropy_coef * weighted_entropy
            if self.kl_coef > 0:
                loss = loss + float(self.kl_coef) * kl_term
            
            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
            self.optimizer.step()

            # ç´¯ç©çµ±è¨ˆ
            total_loss += loss.item()
            policy_loss_sum += weighted_policy_loss.item()
            value_loss_sum += weighted_value_loss.item()
            entropy_sum += weighted_entropy.item()

        # 7) è¨ˆç®—TD errorçµ•å°å€¼ä½œç‚ºæ–°çš„priority
        # optional KL decay per update
        self._decay_kl_coef()
        with torch.no_grad():
            _, current_values = self.policy_net(states_tensor)
            current_values = current_values.squeeze().cpu().numpy()

            # è¨ˆç®—TD error: |r + Î³V(s') - V(s)|
            next_values = np.zeros_like(current_values)
            for i in range(batch_size):
                if not dones[i] and i < batch_size - 1:
                    next_values[i] = current_values[i + 1] if i + 1 < len(current_values) else 0
                else:
                    next_values[i] = 0

            td_errors = rewards + self.gamma * next_values * (1 - dones.astype(float)) - current_values
            td_errors_abs = np.abs(td_errors) + 1e-3

        # 8) æ›´æ–°ç†µæª¢æ¸¬
        self.update_step += 1
        avg_entropy = entropy_sum / max(1, self.k_epochs)
        self.entropy_window.append(avg_entropy)

        if len(self.entropy_window) >= max(5, self.entropy_window.maxlen or 5):
            try:
                mean_ent = float(np.mean(self.entropy_window))
                if mean_ent < self.entropy_threshold and (self.update_step - self.last_partial_reset_update) >= self.reset_cooldown_updates:
                    logger.info(f"ğŸ” ä½ç†µè§¸ç™¼éƒ¨åˆ†é‡ç½®: mean_entropy={mean_ent:.3f} < thr={self.entropy_threshold:.3f}")
                    self.partial_reset('res_blocks_and_head')
            except Exception as e:
                try:
                    logger.debug(f"entropy reset check failed: {e}")
                except Exception:
                    pass

        return {
            "total_loss": total_loss / self.k_epochs,
            "policy_loss": policy_loss_sum / self.k_epochs,
            "value_loss": value_loss_sum / self.k_epochs,
            "entropy": avg_entropy,
            "td_errors_abs": td_errors_abs
        }

    def update_policy(self, use_batch_method=False):
        """ä½¿ç”¨ PPO æ›´æ–°ç­–ç•¥
        
        Args:
            use_batch_method: å¦‚æœç‚ºTrueï¼Œä½¿ç”¨update_policy_from_batchæ–¹æ³•
        """
        # é‡‹æ”¾æœªä½¿ç”¨çš„ CUDA å¿«å–ï¼ˆæ¸›å°‘ç¢ç‰‡åŒ–ï¼‰
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception:
            pass
        if len(self.memory) < self.config['min_batch_size']:
            return None

        # æº–å‚™è¨“ç·´æ•¸æ“š
        states = []
        actions = []
        old_probs = []
        rewards = []
        dones = []

        for transition in self.memory:
            state, action, prob, reward, done = transition
            states.append(state)
            actions.append(action)
            old_probs.append(prob)
            rewards.append(reward)
            dones.append(done)

        if use_batch_method:
            # ä½¿ç”¨æ–°çš„æ‰¹æ¬¡æ›´æ–°æ–¹æ³•
            states_array = np.array(states)
            actions_array = np.array(actions, dtype=np.int64)
            old_probs_array = np.array(old_probs, dtype=np.float32)
            rewards_array = np.array(rewards, dtype=np.float32)
            dones_array = np.array(dones, dtype=bool)
            
            # å‰µå»ºçµ±ä¸€çš„é‡è¦æ€§æ¬Šé‡ï¼ˆå‚³çµ±æ–¹æ³•ä¸­éƒ½æ˜¯1.0ï¼‰
            is_weights = torch.ones(len(states), dtype=torch.float32, device=self.device)
            
            # èª¿ç”¨æ–°çš„æ‰¹æ¬¡æ›´æ–°æ–¹æ³•
            result = self.update_policy_from_batch(
                states=states_array,
                actions=actions_array,
                old_action_probs=old_probs_array,
                rewards=rewards_array,
                dones=dones_array,
                is_weights=is_weights
            )
            
            # æ¸…ç©ºè¨˜æ†¶é«”
            self.memory.clear()
            return result
        
        # åŸå§‹çš„æ›´æ–°æ–¹æ³•
        # è¨ˆç®—æ‰€æœ‰ç‹€æ…‹çš„åƒ¹å€¼
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        with torch.no_grad():
            _, values_tensor = self.policy_net(states_tensor)
            values = values_tensor.cpu().numpy().flatten()

        # è¨ˆç®—å„ªå‹¢å’Œå›å ±
        advantages, returns = self.compute_gae(
            rewards, values, dones, 0,
            self.gamma, self.config['gae_lambda']
        )

        # æ­£è¦åŒ–å„ªå‹¢
        advantages = np.array(advantages)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # è½‰æ›ç‚ºå¼µé‡
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        actions_tensor = torch.LongTensor(actions).to(self.device)
        old_probs_tensor = torch.FloatTensor(old_probs).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        returns_tensor = torch.FloatTensor(returns).to(self.device)

        # PPO æ›´æ–°
        total_loss = 0
        entropy_sum = 0.0
        for _ in range(self.k_epochs):
            # å‰å‘å‚³æ’­
            new_probs, values = self.policy_net(states_tensor)

            # è¨ˆç®—æ¯”ç‡
            new_action_probs = new_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze()
            ratio = new_action_probs / (old_probs_tensor + 1e-8)

            # PPO æå¤±
            surr1 = ratio * advantages_tensor
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages_tensor
            policy_loss = -torch.min(surr1, surr2).mean()

            # åƒ¹å€¼æå¤±
            value_loss = nn.MSELoss()(values.squeeze(), returns_tensor)

            # ç†µæå¤±ï¼ˆæ¢ç´¢ï¼‰
            entropy = -(new_probs * torch.log(new_probs + 1e-8)).sum(dim=1).mean()
            entropy_sum += float(entropy.item())

            # ç¸½æå¤±
            # KL(anchor || current) regularization
            kl_term = self._kl_loss(new_probs, states_tensor)
            loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
            if self.kl_coef > 0:
                loss = loss + float(self.kl_coef) * kl_term
            total_loss += loss.item()

            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
            self.optimizer.step()

        # æ¸…ç©ºè¨˜æ†¶é«”
        self.memory.clear()
        # å†æ¬¡å˜—è©¦é‡‹æ”¾å¿«å–ï¼Œç‚ºä¸‹æ¬¡æ›´æ–°æº–å‚™
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception:
            pass

        # --- Stuck detection on entropy ---
        self.update_step += 1
        avg_entropy = entropy_sum / max(1, self.k_epochs)
        self.entropy_window.append(avg_entropy)
        if len(self.entropy_window) >= max(5, self.entropy_window.maxlen or 5):
            try:
                mean_ent = float(np.mean(self.entropy_window))
                if mean_ent < self.entropy_threshold and (self.update_step - self.last_partial_reset_update) >= self.reset_cooldown_updates:
                    logger.info(f"ğŸ” ä½ç†µè§¸ç™¼éƒ¨åˆ†é‡ç½®: mean_entropy={mean_ent:.3f} < thr={self.entropy_threshold:.3f}")
                    self.partial_reset('res_blocks_and_head')
            except Exception:
                pass
        # optional KL decay per update
        self._decay_kl_coef()
        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': avg_entropy,
            'total_loss': total_loss / self.k_epochs
        }







@ray.remote(num_cpus=1)
class RolloutActor:
    def __init__(self, agent_cfg):
        # éš”é›¢ CUDA + é™åˆ¶ BLAS åŸ·è¡Œç·’
        os.environ.setdefault('CUDA_VISIBLE_DEVICES', '')
        os.environ.setdefault('OMP_NUM_THREADS', '1')
        os.environ.setdefault('MKL_NUM_THREADS', '1')
        try:
            torch.set_num_threads(1)
        except Exception:
            pass

        # åˆå§‹åŒ–ä¸€æ¬¡ Agentï¼ˆCPUï¼‰èˆ‡ Env
        self.agent = PPOAgent(agent_cfg)
        self.agent.device = torch.device('cpu')
        self.agent.policy_net.to(self.agent.device)
        self.agent.policy_net.eval()
        from kaggle_environments import make
        self.env = make('connectx', debug=False)
        self.policy_version = -1

    def set_weights(self, weights_np, version: int):
        # åªæœ‰ç‰ˆæœ¬æå‡æ‰è¼‰å…¥
        if version > self.policy_version and weights_np is not None:
            state_dict = {k: torch.from_numpy(v.copy()) if isinstance(v, np.ndarray) else v
                          for k, v in weights_np.items()}
            self.agent.policy_net.load_state_dict(state_dict, strict=True)
            self.agent.policy_net.eval()
            self.policy_version = version
        return self.policy_version

    def rollout(self, n_episodes: int, player2_prob: float, seed: int = None):
        if seed is not None:
            random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)

        all_transitions = []
        meta = {"episodes": 0, "policy_version_used": self.policy_version, "wins":0,"losses":0,"draws":0}
        max_moves = 50

        with torch.no_grad():
            for _ in range(n_episodes):
                self.env.reset()
                opponent_type = random.choice(['random', 'minimax', 'self'])
                training_player = int(np.random.choice([1,2], p=[1.0 - player2_prob, player2_prob]))
                ep_transitions = []
                move_count = 0

                while not self.env.done and move_count < max_moves:
                    actions = []
                    for player_idx in range(2):
                        if self.env.state[player_idx]['status'] == 'ACTIVE':
                            board, current_player = self.agent.extract_board_and_mark(self.env.state, player_idx)
                            valid_actions = self.agent.get_valid_actions(board)
                            if current_player == training_player:
                                state = self.agent.encode_state(board, current_player)
                                action, prob, value = self.agent.select_action(state, valid_actions, training=True)
                                ep_transitions.append({
                                    'state': state,
                                    'action': int(action),
                                    'prob': float(prob),
                                    'value': float(value),
                                    'is_dangerous': bool(if_i_will_lose_at_next(board, int(action), current_player, self.agent)),
                                })
                            else:
                                if opponent_type == 'random':
                                    action = random_opponent_strategy(board, current_player, valid_actions, self.agent)
                                elif opponent_type == 'minimax':
                                    action = minimax_opponent_strategy(board, current_player, valid_actions, self.agent, depth=3)
                                else:
                                    action = self_play_opponent_strategy(board, current_player, valid_actions, self.agent)
                            actions.append(int(action))
                        else:
                            actions.append(0)
                    try:
                        self.env.step(actions)
                    except Exception:
                        break
                    move_count += 1

                # å›åˆçµæŸï¼šæ±ºå®šçµæœ
                try:
                    r = self.env.state[0]['reward'] if training_player == 1 else self.env.state[1]['reward']
                except Exception:
                    r = 0
                if r == 1: meta["wins"] += 1
                elif r == -1: meta["losses"] += 1
                else: meta["draws"] += 1

                all_transitions.append((ep_transitions, int(r)))
                meta["episodes"] += 1

        return all_transitions, meta

class PERBuffer:
    def __init__(self, capacity=200_000, alpha=0.6, beta_start=0.4, beta_frames=200_000, danger_boost=2.0, eps=1e-3):
        """
        alpha: æ±ºå®šå„ªå…ˆåº¦å°æŠ½æ¨£æ©Ÿç‡çš„å½±éŸ¿ï¼ˆ0=å‡å‹», 1=å®Œå…¨ä¾è³´å„ªå…ˆåº¦ï¼‰
        beta: é‡è¦æ€§åŠ æ¬Šä¿®æ­£ï¼Œå¾ beta_start ç·©å‡åˆ° 1.0
        danger_boost: å±éšªæ­¥å„ªå…ˆåº¦ä¹˜å­
        """
        self.capacity = capacity
        self.alpha = float(alpha)
        self.beta_start = float(beta_start)
        self.beta_frames = int(beta_frames)
        self.frame = 1
        self.eps = float(eps)
        self.danger_boost = float(danger_boost)

        self.data = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
        self.is_danger = deque(maxlen=capacity)
        self.max_priority = 1.0

    def __len__(self):
        return len(self.data)

    def beta_by_frame(self):
        # ç·©å‡åˆ° 1.0ï¼Œé¿å…ä¸€é–‹å§‹æ¬Šé‡å¤ªé‡
        t = min(1.0, self.frame / max(1, self.beta_frames))
        return self.beta_start + t * (1.0 - self.beta_start)

    def add(self, transition, td_error_abs=None, is_dangerous=False):
        # åˆå§‹å„ªå…ˆåº¦ï¼šmax_priorityï¼›è‹¥æœ‰ td_error_abs ç”¨å®ƒ
        p = float(td_error_abs) if td_error_abs is not None else self.max_priority
        if is_dangerous:
            p *= self.danger_boost
        self.data.append(transition)
        self.priorities.append(p)
        self.is_danger.append(bool(is_dangerous))
        self.max_priority = max(self.max_priority, p)

    def sample(self, batch_size, danger_fraction=0.25):
        n = len(self.data)
        if n == 0:
            return [], [], []

        # æ©Ÿç‡åˆ†ä½ˆ
        prios = np.array(self.priorities, dtype=np.float64)
        probs = prios ** self.alpha
        probs_sum = probs.sum()
        if probs_sum <= 0:
            probs = np.ones_like(probs) / len(probs)
        else:
            probs /= probs_sum

        # å±éšªå­é›†
        danger_mask = np.array(self.is_danger, dtype=bool)
        k_danger = int(round(batch_size * danger_fraction))
        k_rest = batch_size - k_danger

        idxs = []
        if k_danger > 0 and danger_mask.any():
            probs_d = probs.copy()
            probs_d[~danger_mask] = 0.0
            s = probs_d.sum()
            probs_d = probs_d / s if s > 0 else np.ones_like(probs_d) / probs_d.size
            idxs_danger = np.random.choice(np.arange(n), size=min(k_danger, danger_mask.sum()), replace=False, p=probs_d)
            idxs.extend(idxs_danger.tolist())

        # å…¶é¤˜å¾å…¨é«”æŒ‰æ©Ÿç‡æŠ½ï¼ˆé¿å…é‡è¦†ï¼‰
        if k_rest > 0:
            probs_rest = probs.copy()
            if idxs:
                probs_rest[idxs] = 0.0
                s = probs_rest.sum()
                probs_rest = probs_rest / s if s > 0 else np.ones_like(probs_rest) / probs_rest.size
            idxs_rest = np.random.choice(np.arange(n), size=min(k_rest, n - len(idxs)), replace=False, p=probs_rest)
            idxs.extend(idxs_rest.tolist())

        # é‡è¦æ€§åŠ æ¬Šï¼ˆIS weightsï¼‰
        probs_used = probs[idxs]
        beta = self.beta_by_frame()
        weights = (n * probs_used) ** (-beta)
        weights = weights / (weights.max() + 1e-8)
        self.frame += 1

        batch = [self.data[i] for i in idxs]
        return batch, idxs, weights

    def update_priorities(self, idxs, td_errors_abs, is_dangerous_flags=None):
        for j, i in enumerate(idxs):
            p = float(td_errors_abs[j]) + self.eps
            if is_dangerous_flags is not None and is_dangerous_flags[j]:
                p *= self.danger_boost
            self.priorities[i] = p
            self.max_priority = max(self.max_priority, p)

class ConnectXTrainer:
    """ConnectX è¨“ç·´å™¨"""

    def __init__(self, config_path_or_dict="config.yaml"):
        # è¼‰å…¥é…ç½®
        if isinstance(config_path_or_dict, dict):
            self.config = config_path_or_dict
        else:
            with open(config_path_or_dict, 'r') as f:
                self.config = yaml.safe_load(f)

        # åˆå§‹åŒ–æ™ºèƒ½é«”
        self.agent = PPOAgent(self.config['agent'])
        # å¦‚æœå…¨åŸŸé…ç½®ä¸­æä¾›äº†é è¨“ç·´æ¨¡å‹ï¼Œè¼‰å…¥ä¸¦è¨­ç½® KL éŒ¨é»
        try:
            pre_cfg = self.config.get('pretrained', {}) if isinstance(self.config, dict) else {}
            if bool(pre_cfg.get('use_pretrained', False)):
                pth = pre_cfg.get('pretrained_model_path') or pre_cfg.get('path')
                if isinstance(pth, str) and pth:
                    ok = self.agent.load_pretrained_model(pth)
                    if ok and hasattr(self.agent, 'set_anchor_from_current'):
                        self.agent.set_anchor_from_current()
        except Exception:
            pass

        # è¨“ç·´çµ±è¨ˆ
        self.episode_rewards = []
        self.win_rates = []
        self.training_losses = []
        
        # æ–°å¢ï¼šæ­·å²æ¨¡å‹ä¿å­˜ï¼ˆç”¨æ–¼èª²ç¨‹åŒ–å­¸ç¿’ï¼‰
        self.historical_models = []
        self.model_save_frequency = 1000  # æ¯1000å›åˆä¿å­˜ä¸€å€‹æ­·å²æ¨¡å‹
        
        # æ¢ç´¢å¢å¼·åƒæ•¸
        self.exploration_decay = 0.995
        self.min_exploration = 0.1
        self.current_exploration = 1.0

        # è¨“ç·´ç©å®¶é¸æ“‡æ©Ÿç‡è¨­å®š
        training_config = self.config.get('training', {})
        self.player2_training_prob = training_config.get('player2_training_probability', 0.7)  # é»˜èª70%æ©Ÿç‡é¸æ“‡player2
        # å¼·åŒ–å¾Œæ‰‹è¨“ç·´æ¯”ä¾‹ï¼Œæå‡å¾Œæ‰‹å‹ç‡
        # boosted_p2 = max(self.player2_training_prob, 0.85)
        # if boosted_p2 != self.player2_training_prob:
        #     logger.info(f"æé«˜å¾Œæ‰‹è¨“ç·´æ¯”ä¾‹: {self.player2_training_prob:.2f} -> {boosted_p2:.2f}")
        #     self.player2_training_prob = boosted_p2
        
        # æŒçºŒå­¸ç¿’æ•¸æ“š
        self.continuous_learning_data = None
        self.continuous_learning_targets = None

        # å‰µå»ºç›®éŒ„
        os.makedirs('checkpoints', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        # --- æ–°å¢ï¼šåœæ»¯åµæ¸¬ç‹€æ…‹ ---
        from collections import deque as _dq
        self._stagnation_scores = _dq(maxlen=50)  # æœ€è¿‘è©•ä¼°åˆ†æ•¸ç·©è¡
        self._best_score_seen = -1.0
        self._best_score_episode = 0
        self._last_stagnation_handle_ep = -10**9

    # --- Board helpers (6x7) - using shared functions ---
    def _flat_to_2d(self, board_flat):
        return flat_to_2d(board_flat)

    def _find_drop_row(self, grid, col):
        return find_drop_row(grid, col)

    def _apply_move(self, board_flat, col, mark):
        return apply_move(board_flat, col, mark)

    def _is_win_from(self, grid, r, c, mark):
        return is_win_from(grid, r, c, mark)

    def _is_winning_move(self, board_flat, col, mark):
        return is_winning_move(board_flat, col, mark)

    def _any_winning_moves(self, board_flat, mark):
        wins = []
        for c in self.agent.get_valid_actions(board_flat):
            if self._is_winning_move(board_flat, c, mark):
                wins.append(c)
        return wins

    # --- Tactical utilities - using shared functions ---
    def if_i_can_win(self, board_flat, mark):
        return if_i_can_win(board_flat, mark, self.agent)

    def if_i_will_lose(self, board_flat, mark):
        return if_i_will_lose(board_flat, mark, self.agent)

    def if_i_will_lose_at_next(self, board_flat, move_col, mark):
        return if_i_will_lose_at_next(board_flat, move_col, mark, self.agent)

    # --- Debug / Analysis ---
    def get_action_score_report(self, board_flat, mark):
        """å›å‚³ç›®å‰ PPO å°æ­¤ç›¤é¢ (board_flat) åœ¨ mark è¦–è§’ä¸‹å„å‹•ä½œåˆ†å¸ƒèˆ‡åƒ¹å€¼ã€‚
        Returns dict (åŒ PPOAgent.get_action_scores)ã€‚"""
        try:
            return self.agent.debug_print_action_scores(board_flat, mark)
        except Exception as e:
            logger.warning(f"å–å¾—å‹•ä½œåˆ†æ•¸å ±å‘Šå¤±æ•—: {e}")
            return None

    # --- Custom Reward System ---
    def calculate_custom_reward(self, prev_board, action, new_board, mark, valid_actions, game_over=False, winner=None, move_count=0):
        """è¨ˆç®—è‡ªå®šç¾©çå‹µ/æ‡²ç½°ç³»çµ± - çµ±ä¸€èª¿ç”¨å…¨åŸŸå‡½æ•¸
        
        é€™å€‹æ–¹æ³•ç¾åœ¨æ˜¯å…¨åŸŸ calculate_custom_reward_global å‡½æ•¸çš„åŒ…è£å™¨ï¼Œ
        ç¢ºä¿æ‰€æœ‰çå‹µè¨ˆç®—é‚è¼¯çµ±ä¸€ï¼Œé¿å…é‡è¤‡ä»£ç¢¼ã€‚
        
        çå‹µè¦å‰‡ï¼š
        1. éæ³•å‹•ä½œæ‡²ç½°: -1.0
        2. å‹åˆ©çå‹µ: +2.0  
        3. é˜²å®ˆçå‹µ (æ“‹ä½å°æ‰‹å³å°‡å‹åˆ©): +1.0
        4. å¨è„…æ‡²ç½° (å¿½ç•¥å°æ‰‹å³å°‡å‹åˆ©): -0.3
        5. éŒ¯éå‹åˆ©æ©Ÿæœƒæ‡²ç½°: -0.5
        6. æ‹–æ™‚é–“çå‹µ: +0.01 * (å›åˆæ•¸-20), æœ€å¤š+0.2
        
        Args:
            prev_board: å‹•ä½œå‰çš„æ£‹ç›¤ç‹€æ…‹ (42-length list)
            action: åŸ·è¡Œçš„å‹•ä½œ (0-6)
            new_board: å‹•ä½œå¾Œçš„æ£‹ç›¤ç‹€æ…‹ (42-length list)  
            mark: ç•¶å‰ç©å®¶æ¨™è¨˜ (1 or 2)
            valid_actions: å‹•ä½œå‰çš„åˆæ³•å‹•ä½œåˆ—è¡¨
            game_over: éŠæˆ²æ˜¯å¦çµæŸ
            winner: ç²å‹è€… (1, 2, or None for draw)
            move_count: ç•¶å‰å›åˆæ•¸
            
        Returns:
            float: ç¸½çå‹µ/æ‡²ç½°å€¼
        """
        # èª¿ç”¨å…¨åŸŸå‡½æ•¸é€²è¡Œçµ±ä¸€è¨ˆç®—
        total_reward = calculate_custom_reward_global(
            prev_board=prev_board,
            action=action,
            new_board=new_board,
            mark=mark,
            valid_actions=valid_actions,
            game_over=game_over,
            winner=winner,
            move_count=move_count,
            gamma=self.gamma
        )
        
        # æ·»åŠ è©³ç´°çš„èª¿è©¦æ—¥èªŒï¼ˆåªåœ¨é¡æ–¹æ³•ä¸­æä¾›ï¼‰
        if total_reward != 0.0:
            opp_mark = 3 - mark
            debug_info = []
            
            # æª¢æŸ¥å„ç¨®çå‹µ/æ‡²ç½°çš„å…·é«”åŸå› 
            if action not in valid_actions:
                debug_info.append(f"éæ³•å‹•ä½œæ‡²ç½°: action={action}, valid={valid_actions}")
            elif game_over and winner == mark:
                debug_info.append(f"å‹åˆ©çå‹µ: +2.0")
            else:
                # æª¢æŸ¥é˜²å®ˆçå‹µ
                try:
                    prev_grid = flat_to_2d(prev_board)
                    prev_valid_actions = [c for c in range(7) if prev_grid[0][c] == 0]
                    for opp_action in prev_valid_actions:
                        if is_winning_move(prev_board, opp_action, opp_mark) and opp_action == action:
                            debug_info.append(f"é˜²å®ˆæˆåŠŸçå‹µ: +1.0 (æ“‹ä½åˆ—{action})")
                            break
                except Exception:
                    pass
                
                # æª¢æŸ¥å¨è„…æ‡²ç½°
                try:
                    opponent_winning_moves = []
                    for opp_action in prev_valid_actions:
                        if is_winning_move(prev_board, opp_action, opp_mark):
                            opponent_winning_moves.append(opp_action)
                    if opponent_winning_moves and action not in opponent_winning_moves:
                        debug_info.append(f"å¿½ç•¥å°æ‰‹å¨è„…æ‡²ç½°: -0.3 (å°æ‰‹å¯å‹åˆ©æ–¼{opponent_winning_moves}, æˆ‘é¸æ“‡{action})")
                except Exception:
                    pass
                
                # æª¢æŸ¥éŒ¯éå‹åˆ©æ©Ÿæœƒæ‡²ç½°
                try:
                    winning_move = self.if_i_can_win(prev_board, mark)
                    if winning_move is not None and action != winning_move:
                        debug_info.append(f"éŒ¯éå‹åˆ©æ©Ÿæœƒæ‡²ç½°: -0.5 (å¯å‹åˆ©æ–¼åˆ—{winning_move}, å»é¸æ“‡{action})")
                except Exception:
                    pass
                
                # æª¢æŸ¥æ‹–æ™‚é–“çå‹µ
                if move_count > 20:
                    drag_reward = min(0.2, (move_count - 20) * 0.01)
                    debug_info.append(f"æ‹–æ™‚é–“çå‹µ: +{drag_reward:.3f} (å›åˆ{move_count})")
            
            # è¼¸å‡ºèª¿è©¦ä¿¡æ¯
            for info in debug_info:
                logger.debug(info)
        
        return total_reward

    def apply_custom_rewards_to_transitions(self, transitions, game_result, move_count):
        """å°‡è‡ªå®šç¾©çå‹µæ‡‰ç”¨åˆ° transition åºåˆ—ä¸­
        
        Args:
            transitions: list of dict, æ¯å€‹åŒ…å« {'state', 'action', 'prob', 'board_before', 'board_after', 'mark', 'valid_actions'}
            game_result: éŠæˆ²çµæœ (1=æˆ‘æ–¹å‹, -1=æˆ‘æ–¹æ•—, 0=å¹³å±€)
            move_count: ç¸½å›åˆæ•¸
            
        Returns:
            list: æ›´æ–°å¾Œçš„ transitionsï¼Œæ¯å€‹æ·»åŠ äº† 'custom_reward' å­—æ®µ
        """
        if not transitions:
            return transitions
            
        updated_transitions = []
        for i, tr in enumerate(transitions):
            # åŸºæœ¬è³‡è¨Š
            prev_board = tr.get('board_before', [0]*42)
            action = tr.get('action', 0)
            new_board = tr.get('board_after', [0]*42)
            mark = tr.get('mark', 1)
            valid_actions = tr.get('valid_actions', list(range(7)))
            
            # åˆ¤æ–·éŠæˆ²æ˜¯å¦åœ¨æ­¤æ­¥çµæŸ
            is_last_move = (i == len(transitions) - 1)
            game_over = is_last_move
            winner = None
            if game_over:
                if game_result == 1:
                    winner = mark  # æˆ‘æ–¹å‹åˆ©
                elif game_result == -1:
                    winner = 3 - mark  # å°æ–¹å‹åˆ©
                # game_result == 0 æ™‚ winner ä¿æŒ None (å¹³å±€)
            
            # è¨ˆç®—è‡ªå®šç¾©çå‹µ - ä½¿ç”¨å¯¦ä¾‹æ–¹æ³•ç‰ˆæœ¬
            custom_reward = self.calculate_custom_reward(
                prev_board=prev_board,
                action=action, 
                new_board=new_board,
                mark=mark,
                valid_actions=valid_actions,
                game_over=game_over,
                winner=winner,
                move_count=i+1
            )
            
            # è¤‡è£½ transition ä¸¦æ·»åŠ è‡ªå®šç¾©çå‹µ
            updated_tr = tr.copy()
            updated_tr['custom_reward'] = custom_reward
            updated_transitions.append(updated_tr)
            
        return updated_transitions

    # NEW: list all moves that do NOT give opponent an immediate winning reply
    def _safe_moves(self, board_flat, mark, valid_actions):
        return safe_moves(board_flat, mark, valid_actions, self.agent)

    # Tactical-aware random opponent move - using shared function
    def _random_with_tactics(self, board_flat, mark, valid_actions):
        return random_opponent_strategy(board_flat, mark, valid_actions, self.agent)

    # --- Random opening tactic for first player (mark==1) ---
    def _random_opening_move(self, board_flat, mark, valid_actions):
        """If this random player is the starter (mark==1), follow opening: 3 -> 4 -> 2 -> then 5 or 1 depending on top row empty.
        Returns a move or None if not applicable."""
        try:
            if int(mark) != 1:
                return None
            grid = self._flat_to_2d(board_flat)
            # Count own tokens on board to determine which turn for this player
            my_tokens = 0
            for r in range(6):
                for c in range(7):
                    if grid[r][c] == 1:
                        my_tokens += 1
            # Opening sequence by own move index (before placing this move)
            if my_tokens == 0 and 3 in valid_actions:
                return 3
            if my_tokens == 1 and 4 in valid_actions:
                return 4
            if my_tokens == 2 and 2 in valid_actions:
                return 2
            if my_tokens == 3:
                top5_empty = (grid[0][5] == 0)
                top1_empty = (grid[0][1] == 0)
                if top5_empty and 5 in valid_actions:
                    return 5
                if top1_empty and 1 in valid_actions:
                    return 1
            return None
        except Exception:
            return None

    def _random_with_opening(self, board_flat, mark, valid_actions):
        """Opening when starting; after choosing, ensure it is safe, else pick a safe move, else random."""
        move = self._random_opening_move(board_flat, mark, valid_actions)
        if move is not None:
            # If opening move would immediately allow opponent to win, try to pick a safe alternative
            if self.if_i_will_lose_at_next(board_flat, move, mark):
                safe = self._safe_moves(board_flat, mark, valid_actions)
                if safe:
                    return random.choice(safe)
                # no safe moves, keep original (or random)
                return random.choice(valid_actions)
            return move
        # No opening move; prefer safe moves
        safe = self._safe_moves(board_flat, mark, valid_actions)
        if safe:
            return random.choice(safe)
        return random.choice(valid_actions)

    # --- Unified tactical + opening random opponent - using shared functions ---
    def _tactical_random_opening_agent(self, board_flat, mark, valid_actions):
        """Priority: win -> block -> (if starter opening) -> safe move -> random.
        Adds safe-move layer to avoid handing immediate wins to opponent."""
        # Win if possible
        c = if_i_can_win(board_flat, mark, self.agent)
        if c is not None:
            return c
        # Block if necessary
        c = if_i_will_lose(board_flat, mark, self.agent)
        if c is not None:
            return c
        # If starter, try opening move (but reject if unsafe)
        move = self._random_opening_move(board_flat, mark, valid_actions)
        if move is not None and not if_i_will_lose_at_next(board_flat, move, mark, self.agent):
            return move
        # Safe moves filter
        safe = safe_moves(board_flat, mark, valid_actions, self.agent)
        if safe:
            return random.choice(safe)
        # Fallback to random
        return random.choice(valid_actions)

    def _play_one_game_vs_random(self):
        env = make('connectx', debug=False)
        env.reset()
        move_count = 0
        max_moves = 50
        with torch.no_grad():
            while not env.done and move_count < max_moves:
                actions = []
                for p in range(2):
                    if env.state[p]['status'] == 'ACTIVE':
                        board, mark = self.agent.extract_board_and_mark(env.state, p)
                        state = self.agent.encode_state(board, mark)
                        valid = self.agent.get_valid_actions(board)
                        # Force tactical random opponent to be player 0 (first/red player)
                        # Agent is always player 1 (second/yellow player)
                        if p == 0:
                            a = self._tactical_random_opening_agent(board, mark, valid)
                        else:
                            a, _, _ = self.agent.select_action(state, valid, training=False)
                        actions.append(int(a))
                    else:
                        actions.append(0)
                try:
                    env.step(actions)
                except Exception:
                    break
                move_count += 1
        # å›å‚³ç©å®¶2çµæœï¼ˆæˆ‘æ–¹ï¼Œç¾åœ¨æ˜¯å¾Œæ‰‹ï¼‰
        try:
            return 1 if env.state[1]['reward'] == 1 else (0 if env.state[1]['reward'] == 0 else -1)
        except Exception:
            return 0

    # --- Minimax opponent for evaluation ---
    def _choose_minimax_move(self, board_flat, mark, max_depth):
        # Use shared minimax strategy
        valid = self.agent.get_valid_actions(board_flat)
        return minimax_opponent_strategy(board_flat, mark, valid, self.agent, depth=max_depth)

    def evaluate_against_random(self, games: int = 50):
        wins = 0
        draws = 0
        total_games = max(1, int(games))
        
        for game_idx in range(total_games):
            try:
                r = self._play_one_game_vs_random()
                if r == 1:
                    wins += 1
                elif r == 0:
                    draws += 1
            except Exception as e:
                logger.warning(f"Game {game_idx} vs random failed: {e}")
                continue
                
        win_rate = wins / total_games
        logger.info(f"evaluate_against_random: {wins}/{total_games} wins, win_rate={win_rate:.3f}")
        return win_rate

    def evaluate_against_minimax(self, games: int = 20):
        wins = 0
        draws = 0
        total_games = max(1, int(games))
        
        for game_idx in range(total_games):
            try:
                env = make('connectx', debug=False)
                env.reset()
                moves = 0
                with torch.no_grad():
                    while not env.done and moves < 50:
                        actions = []
                        for p in range(2):
                            if env.state[p]['status'] == 'ACTIVE':
                                board, mark = self.agent.extract_board_and_mark(env.state, p)
                                state = self.agent.encode_state(board, mark)
                                valid = self.agent.get_valid_actions(board)
                                if p == 0:
                                    a, _, _ = self.agent.select_action(state, valid, training=False)
                                else:
                                    a = self._choose_minimax_move(board, mark, max_depth=int(self.config.get('evaluation', {}).get('minimax_depth', 3)))
                                actions.append(int(a))
                            else:
                                actions.append(0)
                        try:
                            env.step(actions)
                        except Exception:
                            break
                        moves += 1
                try:
                    res = env.state[0]['reward']
                    if res == 1:
                        wins += 1
                    elif res == 0:
                        draws += 1
                except Exception:
                    draws += 1
            except Exception as e:
                logger.warning(f"Game {game_idx} vs minimax failed: {e}")
                continue
                
        win_rate = wins / total_games
        logger.info(f"evaluate_against_minimax: {wins}/{total_games} wins, win_rate={win_rate:.3f}")
        return win_rate

    def evaluate_with_metrics(self, games: int = 50):
        wins = 0
        losses = 0
        draws = 0
        lengths = []
        for _ in range(max(1, int(games))):
            env = make('connectx', debug=False)
            env.reset()
            moves = 0
            with torch.no_grad():
                while not env.done and moves < 50:
                    actions = []
                    for p in range(2):
                        if env.state[p]['status'] == 'ACTIVE':
                            board, mark = self.agent.extract_board_and_mark(env.state, p)
                            state = self.agent.encode_state(board, mark)
                            valid = self.agent.get_valid_actions(board)
                            # Force tactical random opponent to be player 0 (first/red player)
                            # Agent is always player 1 (second/yellow player)
                            if p == 0:
                                a = self._tactical_random_opening_agent(board, mark, valid)
                            else:
                                a, _, _ = self.agent.select_action(state, valid, training=False)
                            actions.append(int(a))
                        else:
                            actions.append(0)
                    try:
                        env.step(actions)
                    except Exception:
                        break
                    moves += 1
            try:
                res = env.state[1]['reward']  # Check player 1 (agent) result
                if res == 1:
                    wins += 1
                elif res == -1:
                    losses += 1
                else:
                    draws += 1
            except Exception:
                draws += 1
            lengths.append(moves)
        total = max(1, int(games))
        win_rate = wins / total
        avg_len = float(np.mean(lengths)) if lengths else 0.0
        return {
            'win_rate': win_rate,
            'avg_game_length': avg_len,
            'wins': wins,
            'losses': losses,
            'draws': draws,
            'quick_wins': 0,
            'comeback_wins': 0,
        }

    def _choose_policy_with_tactics(self, board_flat, mark, valid_actions, training=False):
        # å…ˆçœ‹æ˜¯å¦èƒ½ç›´æ¥è´
        c = if_i_can_win(board_flat, mark, self.agent)
        if c is not None:
            return c
        # å†çœ‹æ˜¯å¦å¿…é ˆæ“‹ä¸‹å°æ–¹
        c = if_i_will_lose(board_flat, mark, self.agent)
        if c is not None:
            return c
        # ç”¨ç­–ç•¥ç¶²è·¯é¸æ“‡
        state = self.agent.encode_state(board_flat, mark)
        a, _, _ = self.agent.select_action(state, valid_actions, training=training)
        a = int(a)
        # é¿å…ç«‹å³é€çµ¦å°æ‰‹è‡´å‹
        if if_i_will_lose_at_next(board_flat, a, mark, self.agent):
            safe = safe_moves(board_flat, mark, valid_actions, self.agent)
            if safe:
                return random.choice(safe)
        return a

    def _play_one_game_self_tactical(self, main_as_player1=True):
        """è‡ªæˆ‘å°æˆ°ï¼šå°æ‰‹ä½¿ç”¨æˆ°è¡“è¦å‰‡(if_i_can_win/if_i_will_lose)ï¼Œä¸»æ§æ–¹ç”¨ç­–ç•¥ç¶²è·¯ã€‚
        main_as_player1 æ±ºå®šæˆ‘æ–¹æ˜¯å…ˆæ‰‹é‚„æ˜¯å¾Œæ‰‹ï¼Œå›å‚³æˆ‘æ–¹å‹/å’Œ/è²  (1/0/-1)ã€‚"""
        env = make('connectx', debug=False)
        env.reset()
        moves = 0
        with torch.no_grad():
            while not env.done and moves < 50:
                actions = []
                for p in range(2):
                    if env.state[p]['status'] != 'ACTIVE':
                        actions.append(0)
                        continue
                    board, mark = self.agent.extract_board_and_mark(env.state, p)
                    valid = self.agent.get_valid_actions(board)
                    # æ±ºå®šå“ªä¸€æ–¹æ˜¯æˆ‘æ–¹
                    is_main = (p == 0 and main_as_player1) or (p == 1 and not main_as_player1)
                    if is_main:
                        # æˆ‘æ–¹ç”¨ç­–ç•¥ + å®‰å…¨æª¢æŸ¥
                        a = self._choose_policy_with_tactics(board, mark, valid, training=False)
                    else:
                        # å°æ‰‹ä½¿ç”¨åˆä½µç‰ˆï¼šæˆ°è¡“å„ªå…ˆ + å…ˆæ‰‹é–‹å±€ + éš¨æ©Ÿ
                        a = self._tactical_random_opening_agent(board, mark, valid)
                    actions.append(int(a))
                try:
                    env.step(actions)
                except Exception:
                    break
                moves += 1
        try:
            if main_as_player1:
                res = env.state[0]['reward']
            else:
                res = env.state[1]['reward']
            return 1 if res == 1 else (0 if res == 0 else -1)
        except Exception:
            return 0

    def evaluate_self_play_tactical(self, games: int = 50):
        wins = 0
        draws = 0
        for _ in range(max(1, int(games))):
            # Force agent to be player 1 (second player), tactical opponent always player 0
            r = self._play_one_game_self_tactical(main_as_player1=False)
            if r == 1:
                wins += 1
            elif r == 0:
                draws += 1
        return wins / max(1, int(games))
    def train_with_ray(self):
        tcfg = self.config.get('training', {})
        num_actors = int(tcfg.get('num_workers',  max(1, (os.cpu_count() or 2) - 1)))
        episodes_per_task = int(tcfg.get('episodes_per_update', 8))  # æ¯å€‹ actor ä¸€æ¬¡æ‰“å¹¾å±€
        max_episodes = int(tcfg.get('max_episodes', 100_000))
        eval_frequency = int(self.config.get('evaluation', {}).get('frequency', 200))
        eval_games = int(self.config.get('evaluation', {}).get('games', 30))
        win_scale = float(tcfg.get('win_reward_scaling', 1.0))
        loss_scale = float(tcfg.get('loss_penalty_scaling', 1.0))
        danger_scale = float(self.config.get('agent', {}).get('tactical_bonus', 0.1))
        visualize_every = int(tcfg.get('visualize_every', 100))

        # PER è¶…åƒæ•¸
        capacity = int(tcfg.get('replay_capacity', 300_000))
        alpha = float(tcfg.get('per_alpha', 0.6))
        beta_start = float(tcfg.get('per_beta_start', 0.4))
        beta_frames = int(tcfg.get('per_beta_frames', 200_000))
        danger_boost = float(tcfg.get('danger_priority_boost', 2.0))
        danger_fraction = float(tcfg.get('danger_oversample_fraction', 0.25))
        min_batch_size = int(self.agent.config.get('min_batch_size', 512))
        sgd_batch_size = int(self.agent.config.get('sgd_batch_size', 256))

        rng = random.Random()

        # å•Ÿå‹• Ray
        if not ray.is_initialized():
            ray.init(
                ignore_reinit_error=True,
                include_dashboard=False,
                runtime_env={
                    'excludes': [
                        '.git/',
                        '/pycache/',
                        'checkpoints/',
                        'videos/',
                        'game_videos/',
                        'game_visualizations/**'
                    ]
                }
            )


        # å»º actors
        actors = [RolloutActor.remote(self.config['agent']) for _ in range(num_actors)]

        # åˆå§‹æ¬Šé‡ -> å»£æ’­
        def export_weights_numpy():
            return {k: v.detach().cpu().numpy() for k, v in self.agent.policy_net.state_dict().items()}

        policy_version = 0
        weights_np = export_weights_numpy()
        ray.get([a.set_weights.remote(weights_np, policy_version) for a in actors])

        # PER ç·©è¡
        per = PERBuffer(capacity=capacity, alpha=alpha, beta_start=beta_start, beta_frames=beta_frames, danger_boost=danger_boost)

        # å»ºç«‹åˆå§‹ in-flight ä»»å‹™
        in_flight = []
        def submit(actor):
            return actor.rollout.remote(n_episodes=episodes_per_task, player2_prob=self.player2_training_prob, seed=rng.randrange(2**31-1))
        in_flight = [submit(a) for a in actors]

        episodes_done_total = len(self.episode_rewards)
        best_score = -1.0
        # Track recent win rate for difficulty reporting
        recent_results = []  # keep last ~1000 results (1=win, 0=non-win)
        current_win_rate = 0.0
        # Next scheduled checkpoints for evaluation/visualization (align to next multiple)
        if eval_frequency > 0:
            if episodes_done_total > 0:
                next_eval_at = ((episodes_done_total // eval_frequency) + 1) * eval_frequency
            else:
                next_eval_at = eval_frequency
        else:
            next_eval_at = None
        if visualize_every > 0:
            if episodes_done_total > 0:
                next_viz_at = ((episodes_done_total // visualize_every) + 1) * visualize_every
            else:
                next_viz_at = visualize_every
        else:
            next_viz_at = None

        logger.info(f"ğŸš€ Ray Actor å¹³è¡Œè¨“ç·´å•Ÿå‹•ï¼šactors={num_actors}, episodes_per_task={episodes_per_task}")
        best_win_rate = 0.0
        try:
            while episodes_done_total < max_episodes:
                # ç­‰å¾…ä»»ä¸€ actor å®Œæˆ
                done_ids, in_flight = ray.wait(in_flight, num_returns=1, timeout=1.0)
                if not done_ids:
                    continue

                # ç«‹åˆ»è£œä»¶
                finished_id = done_ids[0]
                # åƒ…è£œä¸€å€‹æ–°çš„ä»»å‹™ï¼Œç¶­æŒèˆ‡ actors æ•¸é‡ä¸€è‡´ï¼Œé¿å…éé‡æäº¤
                if len(in_flight) < len(actors):
                    try:
                        in_flight.append(submit(rng.choice(actors)))
                    except Exception:
                        in_flight.append(submit(actors[0]))

                # å–çµæœ
                all_transitions, meta = ray.get(finished_id)
                # é€å›åˆå¯«å…¥ PER
                for ep_transitions, result in all_transitions:
                    ep_reward_sum = 0.0
                    last_idx = len(ep_transitions) - 1
                    if last_idx < 0:
                        self.episode_rewards.append(0.0)
                        episodes_done_total += 1
                        continue

                    for idx, tr in enumerate(ep_transitions):
                        reward = 0.0
                        if idx == last_idx:
                            if result == 1:
                                reward += 1.0 * win_scale
                            elif result == -1:
                                reward += -1.0 * loss_scale
                        if tr.get('is_dangerous', False):
                            reward += -10.0 * danger_scale
                        ep_reward_sum += reward

                        transition = {
                            'state': tr['state'],
                            'action': tr['action'],
                            'prob': tr['prob'],
                            'reward': reward,
                            'done': (idx == last_idx),
                            'is_dangerous': tr.get('is_dangerous', False),
                        }

                        # åˆå§‹ priorityï¼šç”¨ |reward| ç•¶è¿‘ä¼¼ï¼ˆç­‰æœƒå…’æ›´æ–°å¾Œå†è¦†è“‹ï¼‰
                        init_prio = abs(reward) + 1e-3
                        if transition['is_dangerous']:
                            init_prio *= danger_boost
                        per.add(transition, td_error_abs=init_prio, is_dangerous=transition['is_dangerous'])

                    self.episode_rewards.append(ep_reward_sum)
                    episodes_done_total += 1

                    # Update moving win-rate (requires at least minimal stability window)
                    try:
                        recent_results.append(1 if int(result) == 1 else 0)
                        if len(recent_results) > 1000:
                            recent_results.pop(0)
                        if len(recent_results) >= 100:
                            current_win_rate = sum(recent_results) / float(len(recent_results))
                    except Exception:
                        pass

                # åªè¦è³‡æ–™å¤ ï¼Œå°±å¤šæ¬¡ SGD æ›´æ–°ï¼ˆè®“ Actor ä¸åœæ‰“å±€ï¼‰
                while len(per) >= min_batch_size:
                    batch, idxs, is_weights = per.sample(sgd_batch_size, danger_fraction=danger_fraction)
                    if not batch:
                        break

                    # å°è£çµ¦ agentï¼ˆå…©ç¨®é¸æ“‡ï¼š1) ä¸Ÿå› self.agent.memory ç„¶å¾Œç”¨ä½ åŸæœ‰çš„ updateï¼›
                    # 2) ç›´æ¥å¯«ä¸€å€‹ update_policy_from_batchï¼‰
                    # é€™è£¡ç¤ºç¯„ 2)ï¼Œç”¨ä¸€å€‹ä½ å¯ä»¥åœ¨ agent å…§å¯¦ä½œçš„ä»‹é¢ï¼š
                    states = [b['state'] for b in batch]
                    actions = np.array([b['action'] for b in batch], dtype=np.int64)
                    old_probs = np.array([b['prob'] for b in batch], dtype=np.float32)
                    rewards = np.array([b['reward'] for b in batch], dtype=np.float32)
                    dones = np.array([b['done'] for b in batch], dtype=bool)
                    is_danger_flags = np.array([b['is_dangerous'] for b in batch], dtype=bool)
                    isw = torch.as_tensor(is_weights, dtype=torch.float32, device=self.agent.device)

                    # ä½ å¯ä»¥åœ¨ agent å…§éƒ¨æŠŠ states -> tensorã€è¨ˆç®—æ–° logprob / valueã€GAEã€PPO loss ç­‰
                    # ä¸¦å›å‚³æ¯ç­†æ¨£æœ¬çš„ |TD-error| æˆ– |Advantage| ç•¶ priority æ›´æ–°ä¾æ“š
                    info = self.agent.update_policy_from_batch(
                        states=states,
                        actions=actions,
                        old_action_probs=old_probs,
                        rewards=rewards,
                        dones=dones,
                        is_weights=isw
                    )
                    # æœŸæœ› info å›ä¾†æœ‰ 'td_errors_abs'ï¼ˆlen==batchï¼‰ï¼Œæ²’æœ‰å°±ç”¨ |rewards| ä»£æ›¿
                    if info is not None:
                        self.training_losses.append(float(info.get('total_loss', 0.0)))

                    td_err = info.get('td_errors_abs') if info is not None else np.abs(rewards) + 1e-3
                    per.update_priorities(idxs, td_err, is_dangerous_flags=is_danger_flags)

                    # æˆåŠŸæ›´æ–°å¾Œâ†’å»£æ’­æ–°æ¬Šé‡ï¼ˆåªåœ¨æœ‰å¯¦è³ªæ›´æ–°æ™‚ï¼‰
                    policy_version += 1
                    weights_np = {k: v.detach().cpu().numpy() for k, v in self.agent.policy_net.state_dict().items()}
                    # åªåœ¨ç‰ˆæœ¬å‡ç´šé»å»£æ’­ä¸€æ¬¡ï¼ˆRay æœƒä¸¦è¡Œ set_weightsï¼‰
                    ray.get([a.set_weights.remote(weights_np, policy_version) for a in actors])

                # é€±æœŸæ€§è©•ä¼°ï¼ˆä»¥é–€æª»è§¸ç™¼ï¼Œé¿å…æ¨¡æ•¸å°ä¸é½Šè€Œæ°¸ä¸è§¸ç™¼ï¼‰
                if next_eval_at is not None and episodes_done_total >= next_eval_at:
                    metrics = self.evaluate_comprehensive(games=eval_games)
                    score = float(metrics.get('comprehensive_score', 0.0))
                    score_incl = float(metrics.get('comprehensive_score_incl_self', score))
                    score_excl = float(metrics.get('comprehensive_score_excl_self', score))
                    use_self = bool(metrics.get('comprehensive_uses_self_play', False))
                    self.win_rates.append(score)
                    try:
                        self.agent.scheduler.step(score)
                    except Exception:
                        pass
                    if score > best_win_rate:
                        best_win_rate = score
                    # Difficulty label from moving win-rate
                    if best_win_rate < 0.2:
                        difficulty = "åˆç´š (ç´”éš¨æ©Ÿ+åªæœƒè´)"
                    elif best_win_rate < 0.4:
                        difficulty = "åˆéš (åŠ å…¥é˜²å®ˆ)"
                    elif best_win_rate < 0.55:
                        difficulty = "ä¸­éš (åå¥½ä¸­å¤®+å¼±æˆ°è¡“)"
                    elif best_win_rate < 0.7:
                        difficulty = "é€²éš (å®¹æ˜“çŠ¯éŒ¯)"
                    else:
                        difficulty = "é«˜éš (å®Œæ•´æˆ°è¡“)"

                    # Optional PPO diagnostics
                    ppo_diag = {}
                    try:
                        ppo_diag = self.evaluate_ppo_diagnostics(samples=max(3, eval_games // 10)) or {}
                    except Exception:
                        ppo_diag = {}

                    logger.info(
                        f"ğŸ“ˆ Eps={episodes_done_total} | Score={score:.3f} (incl={score_incl:.3f}, excl={score_excl:.3f}, use_self={use_self}) | "
                        f"win_rate={current_win_rate:.3f} | diff={difficulty} | "
                        f"self={metrics.get('self_play', 0):.3f} minimax={metrics.get('vs_minimax', 0):.3f} rand={metrics.get('vs_random', 0):.3f} | "
                        f"ppo(entropy={ppo_diag.get('avg_entropy','n/a')}, value={ppo_diag.get('avg_value','n/a')})"
                    )

                    # Stagnation detection and handling (parity with train_parallel)
                    try:
                        if self._detect_convergence_stagnation(episodes_done_total, score):
                            self._handle_convergence_stagnation(episodes_done_total)
                    except Exception:
                        pass
                    if score > best_score:
                        best_score = score
                        send_telegram(f"ğŸ‰ æ–°æœ€ä½³åˆ†æ•¸ Eps={episodes_done_total} | Score={best_score:.3f} | ")
                        self.save_checkpoint(f"best_model_wr_{best_score:.3f}.pt")
                    # æ›´æ–°ä¸‹ä¸€æ¬¡è©•ä¼°é–€æª»
                    next_eval_at += eval_frequency
                # é€±æœŸæ€§è¦–è¦ºåŒ–ï¼ˆåŒæ¨£ä½¿ç”¨é–€æª»è§¸ç™¼ï¼‰
                if next_viz_at is not None and episodes_done_total >= next_viz_at:
                    try:
                        quick_games = max(5, int(eval_games // 2))
                        metrics_v = self.evaluate_comprehensive(games=quick_games)
                        score_v = float(metrics_v.get('comprehensive_score', 0.0))
                        logger.info(
                            f"ğŸ¯ è¦–è¦ºåŒ–è©•ä¼° Eps={episodes_done_total} | Score={score_v:.3f} (incl={metrics_v.get('comprehensive_score_incl_self', score_v):.3f}, excl={metrics_v.get('comprehensive_score_excl_self', score_v):.3f}, use_self={metrics_v.get('comprehensive_uses_self_play', False)}) | "
                            f"self={metrics_v.get('self_play', 0):.3f} minimax={metrics_v.get('vs_minimax', 0):.3f} rand={metrics_v.get('vs_random', 0):.3f}"
                        )
                    except Exception as ee:
                        logger.warning(f"è¦–è¦ºåŒ–å‰è©•ä¼°å¤±æ•—ï¼š{ee}")
                    try:
                        self.visualize_training_game(episodes_done_total, save_dir='videos', opponent='tactical', fps=2)
                    except Exception as ve:
                        logger.warning(f"å¯è¦–è¦ºåŒ–å¤±æ•—ï¼š{ve}")
                    # æ›´æ–°ä¸‹ä¸€æ¬¡è¦–è¦ºåŒ–é–€æª»
                    next_viz_at += visualize_every

        finally:
            try:
                ray.shutdown()
            except Exception:
                pass

        logger.info("âœ… Ray Actor å¹³è¡Œè¨“ç·´å®Œæˆ")
        return self.agent

    def evaluate_self_play_player2_focus(self, games: int = 50):
        """è©•ä¼°æˆ‘æ–¹ä½œç‚ºå¾Œæ‰‹(ç©å®¶2)æ™‚çš„å‹ç‡ï¼Œå°æ‰‹æ¡æˆ°è¡“è¦å‰‡ã€‚"""
        wins = 0
        draws = 0
        total_games = max(1, int(games))
        
        for game_idx in range(total_games):
            try:
                r = self._play_one_game_self_tactical(main_as_player1=False)
                if r == 1:
                    wins += 1
                elif r == 0:
                    draws += 1
            except Exception as e:
                logger.warning(f"Game {game_idx} self_play_player2 failed: {e}")
                continue
                
        win_rate = wins / total_games
        logger.info(f"evaluate_self_play_player2_focus: {wins}/{total_games} wins, win_rate={win_rate:.3f}")
        return win_rate

    def evaluate_ppo_diagnostics(self, samples: int = 5):
        """Quick PPO diagnostics: average entropy and value on random legal states.
        Returns dict with avg_entropy and avg_value. Best-effort only; safe on CPU.
        """
        try:
            import numpy as np
            ent_list, val_list = [], []
            self.agent.policy_net.eval()
            # Create a few empty or near-empty states to probe policy/value
            for _ in range(max(1, int(samples))):
                # start from empty, optionally add a couple random moves
                board = [0] * 42
                moves = np.random.randint(0, 4)
                mark = 1
                for _m in range(moves):
                    valid = [c for c in range(7) if board[c] == 0]
                    if not valid:
                        break
                    c = int(np.random.choice(valid))
                    # drop piece
                    grid = flat_to_2d(board)
                    r = find_drop_row(grid, c)
                    if r is None:
                        continue
                    grid[r][c] = mark
                    board = [grid[i][j] for i in range(6) for j in range(7)]
                    mark = 3 - mark

                # Evaluate on current mark's perspective
                state = self.agent.encode_state(board, mark)
                valid = self.agent.get_valid_actions(board)
                info = self.agent.get_action_scores(state, valid)
                if info is None:
                    continue
                ent_list.append(float(info.get('entropy', 0.0)))
                val_list.append(float(info.get('value', 0.0)))
            if not ent_list:
                return {'avg_entropy': None, 'avg_value': None}
            return {
                'avg_entropy': round(float(np.mean(ent_list)), 4),
                'avg_value': round(float(np.mean(val_list)), 4),
            }
        except Exception:
            return {'avg_entropy': None, 'avg_value': None}

    def evaluate_comprehensive(self, games: int = 50):
        try:
            vs_random = self.evaluate_against_random(games)
            logger.info(f"vs_random result: {vs_random}")
        except Exception as e:
            logger.warning(f"evaluate_against_random failed: {e}")
            vs_random = 0.0
            
        try:
            vs_minimax = self.evaluate_against_minimax(max(1, games // 2))
            logger.info(f"vs_minimax result: {vs_minimax}")
        except Exception as e:
            logger.warning(f"evaluate_against_minimax failed: {e}")
            vs_minimax = 0.0
            
        try:
            # è‡ªæˆ‘å°æˆ°åˆ†å…©ç¨®ï¼šå…ˆæ‰‹èˆ‡å¾Œæ‰‹ç„¦é»ï¼Œé€™è£¡ä½¿ç”¨å¾Œæ‰‹ç„¦é»ä»¥é¼“å‹µå¾Œæ‰‹èƒ½åŠ›
            self_play = self.evaluate_self_play_player2_focus(max(1, games // 2))
            logger.info(f"self_play result: {self_play}")
        except Exception as e:
            logger.warning(f"evaluate_self_play_player2_focus failed: {e}")
            self_play = 0.0
            
        # æ¬Šé‡ï¼ˆè‹¥é…ç½®æœ‰æä¾›ï¼‰
        eval_cfg = self.config.get('evaluation', {}) if isinstance(self.config, dict) else {}
        weights = eval_cfg.get('weights', {}) if isinstance(eval_cfg, dict) else {}
        w_self = float(weights.get('self_play', 0.4))
        w_minimax = float(weights.get('vs_minimax', 0.4))
        w_random = float(weights.get('vs_random', 0.2))

        # æ˜¯å¦åœ¨ç¶œåˆè©•åˆ†ä¸­è¨ˆå…¥è‡ªå°å¼ˆï¼ˆé è¨­ï¼šä¸è¨ˆå…¥ï¼Œé¿å…è¢« â‰ˆ50% æ‹‰é«˜æˆ–æ©è“‹ï¼‰
        include_self_in_score = bool(eval_cfg.get('count_self_play_in_score', False))

        # åˆ†åˆ¥è¨ˆç®—ã€ŒåŒ…å«è‡ªå°å¼ˆã€èˆ‡ã€Œæ’é™¤è‡ªå°å¼ˆã€å…©ç¨®åˆ†æ•¸
        total_w_incl = max(1e-6, w_self + w_minimax + w_random)
        score_incl_self = (w_self * self_play + w_minimax * vs_minimax + w_random * vs_random) / total_w_incl

        # æ’é™¤è‡ªå°å¼ˆï¼šè‡ªå°å¼ˆæ¬Šé‡è¦–ç‚º 0ï¼Œä¸¦é‡æ–°æ­£è¦åŒ–
        total_w_excl = max(1e-6, (0.0) + w_minimax + w_random)
        score_excl_self = (w_minimax * vs_minimax + w_random * vs_random) / total_w_excl

        # æœ€çµ‚è¼¸å‡ºéµå¾ªé…ç½®
        comprehensive = score_incl_self if include_self_in_score else score_excl_self
        
        return {
            'vs_random': vs_random,
            'vs_minimax': vs_minimax,
            'self_play': self_play,
            'comprehensive_score': comprehensive,
            'comprehensive_score_incl_self': score_incl_self,
            'comprehensive_score_excl_self': score_excl_self,
            'comprehensive_uses_self_play': include_self_in_score,
        }

    # --- Stagnation detection / handling ---
    def _detect_convergence_stagnation(self, current_episode: int, current_score: float) -> bool:
        """åµæ¸¬æ˜¯å¦é€²å…¥æ”¶æ–‚åœæ»¯ç‹€æ…‹ã€‚
        æ¢ä»¶(æ»¿è¶³ä»»ä¸€æ ¸å¿ƒæ¢ä»¶ + åŸºæœ¬æ¢ä»¶)ï¼š
          1. æœ€è¿‘çª—å£(score_window)çš„åˆ†æ•¸æ³¢å‹•ç¯„åœéå° (range < range_threshold)
          2. å·²ç¶“è¶…é patience_episodes ä»æ²’æœ‰åˆ·æ–°æœ€ä½³åˆ†æ•¸
          3. åˆ†æ•¸é•·æœŸä½æ–¼ low_score_threshold (å¹³å‡å€¼ < è©²å€¼ ä¸” episode >= min_episode_start)
        è§¸ç™¼å¾Œç”± _handle_convergence_stagnation åŸ·è¡Œéƒ¨åˆ†æ¬Šé‡é‡ç½®èˆ‡æ¢ç´¢å¢å¼·ã€‚
        """
        cfg = self.config.get('stagnation', {}) if isinstance(self.config, dict) else {}
        window_size = int(cfg.get('window', 10))
        range_threshold = float(cfg.get('range_threshold', 0.02))
        patience_eps = int(cfg.get('patience_episodes', 800))
        low_score_threshold = float(cfg.get('low_score_threshold', 0.35))
        min_episode_start = int(cfg.get('min_episode_start', 500))
        cooldown_eps = int(cfg.get('cooldown_episodes', 800))

        # æ›´æ–°ç‹€æ…‹
        self._stagnation_scores.append(float(current_score))
        if current_score > self._best_score_seen + 1e-6:
            self._best_score_seen = float(current_score)
            self._best_score_episode = int(current_episode)

        # å†·å»æœªéä¸å†è§¸ç™¼
        if current_episode - self._last_stagnation_handle_ep < cooldown_eps:
            return False

        if len(self._stagnation_scores) < max(5, window_size):
            return False

        recent = list(self._stagnation_scores)[-window_size:]
        r_min, r_max = min(recent), max(recent)
        r_range = r_max - r_min
        avg_recent = sum(recent) / len(recent)

        no_improve_eps = current_episode - self._best_score_episode

        cond_small_range = (r_range < range_threshold)
        cond_patience = (no_improve_eps >= patience_eps and current_episode >= min_episode_start)
        cond_low = (avg_recent < low_score_threshold and current_episode >= min_episode_start)

        triggered = (cond_small_range or cond_patience or cond_low)
        if triggered:
            try:
                logger.info(
                    f"âš ï¸ åµæ¸¬åˆ°å¯èƒ½åœæ»¯: eps={current_episode} score={current_score:.3f} "
                    f"range={r_range:.4f} avg={avg_recent:.3f} no_improve={no_improve_eps} "
                    f"(range<{range_threshold}? {cond_small_range}, patience? {cond_patience}, low? {cond_low})"
                )
            except Exception:
                pass
        return triggered

    def _handle_convergence_stagnation(self, current_episode: int):
        """è™•ç†æ”¶æ–‚åœæ»¯ï¼šéƒ¨åˆ†é‡ç½®æ¨¡å‹æ¬Šé‡ã€æå‡æ¢ç´¢ã€é©åº¦èª¿æ•´å­¸ç¿’ç‡/æ¸…ç·©è¡ã€‚
        ç­–ç•¥ï¼š
          1. ä½¿ç”¨ agent.partial_reset é‡ç½®éƒ¨åˆ†æ®˜å·®å€å¡Šèˆ‡ head (ä¾ reset_fraction)
          2. æå‡ entropy_coefï¼ˆä¸Šé™ä¿è­·ï¼‰é¼“å‹µæ¢ç´¢
          3. å¯é¸ï¼šè¼•å¾®èª¿æ•´å­¸ç¿’ç‡(é™ä½æˆ–å›å½ˆ)ï¼›é€™è£¡æ¡ç”¨ *0.9 è®“é‡æ–°æœç´¢æ›´ç©©å®š
          4. æ¸…ç©º PPO è¨˜æ†¶é¿å…èˆŠç­–ç•¥åˆ†ä½ˆå¹²æ“¾
          5. é‡ç½®å°æ‡‰ optimizer state ä»¥é˜²éºç•™å‹•é‡
        """
        cfg = self.config.get('stagnation', {}) if isinstance(self.config, dict) else {}
        cooldown_eps = int(cfg.get('cooldown_episodes', 800))
        reset_fraction = float(cfg.get('reset_fraction', 0.30))
        entropy_boost = float(cfg.get('entropy_boost', 1.35))
        entropy_max = float(cfg.get('entropy_max', 0.02))  # entropy_coef é€šå¸¸å¾ˆå°ï¼Œè¨­å®šä¸Šé™
        lr_decay = float(cfg.get('lr_decay_factor', 0.9))
        
        min_gap_ok = (current_episode - self._last_stagnation_handle_ep) >= cooldown_eps
        if not min_gap_ok:
            return
        
        try:
            logger.info(
                f"ğŸ› ï¸ åŸ·è¡Œåœæ»¯è™•ç†: episode={current_episode} reset_fraction={reset_fraction} entropy_boost={entropy_boost}"
            )
        except Exception:
            pass
        
        # 1) éƒ¨åˆ†é‡ç½®ï¼ˆè‹¥ agent æä¾› partial_resetï¼‰
        try:
            if hasattr(self.agent, 'partial_reset'):
                self.agent.partial_reset('res_blocks_and_head', fraction=reset_fraction)
            else:
                # å¾Œå‚™ï¼šæ‰‹å‹•æŒ‘é¸éƒ¨åˆ†å±¤é‡æ–°åˆå§‹åŒ–
                import random
                import math
                modules = []
                for m in self.agent.policy_net.modules():  # type: ignore
                    if isinstance(m, (torch.nn.Conv2d, torch.nn.Linear)) and m.weight.requires_grad:
                        modules.append(m)
                random.shuffle(modules)
                k = max(1, int(len(modules) * reset_fraction))
                for m in modules[:k]:
                    if isinstance(m, torch.nn.Conv2d):
                        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                    else:
                        torch.nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                    if m.bias is not None:
                        torch.nn.init.zeros_(m.bias)
        except Exception as e:
            try:
                logger.warning(f"éƒ¨åˆ†é‡ç½®éç¨‹å‡ºéŒ¯: {e}")
            except Exception:
                pass
        
        # 2) å¢åŠ æ¢ç´¢: entropy_coef ä¹˜ä»¥ boost (æœ‰ä¸Šé™)
        try:
            if hasattr(self.agent, 'entropy_coef'):
                new_entropy = min(entropy_max, getattr(self.agent, 'entropy_coef', 0.0) * entropy_boost + 1e-12)
                self.agent.entropy_coef = new_entropy
                logger.info(f"ğŸ”„ entropy_coef -> {new_entropy:.6f}")
        except Exception:
            pass
        
        # 3) èª¿æ•´å­¸ç¿’ç‡ï¼ˆå…¨éƒ¨ param groupï¼‰
        try:
            for pg in self.agent.optimizer.param_groups:  # type: ignore
                old_lr = pg.get('lr', 0.0)
                pg['lr'] = max(1e-6, old_lr * lr_decay)
            logger.info("ğŸ“‰ å·²èª¿æ•´å­¸ç¿’ç‡ (ä¹˜ä»¥ lr_decay_factor)")
        except Exception:
            pass
        
        # 4) æ¸…ç©ºè¨˜æ†¶ / 5) æ¸… optimizer state (åªä¿ç•™å¿…è¦çµæ§‹)
        try:
            if hasattr(self.agent, 'memory'):
                self.agent.memory.clear()
            # é‡å»º optimizer ä¾†æ¸…å‹•é‡
            opt_cls = type(self.agent.optimizer)
            self.agent.optimizer = opt_cls(self.agent.policy_net.parameters(), **self.agent.optimizer.defaults)  # type: ignore
        except Exception:
            pass
        
        # 6) æ›´æ–°å†·å»æ¨™è¨˜
        self._last_stagnation_handle_ep = int(current_episode)
        try:
            logger.info("âœ… åœæ»¯è™•ç†å®Œæˆï¼Œé€²å…¥å†·å»éšæ®µã€‚")
        except Exception:
            pass
        return

    # ------------------------------
    # Checkpoint I/O utilities
    # ------------------------------
    def save_checkpoint(self, filename: str):
        """ä¿å­˜æª¢æŸ¥é»åˆ° checkpoints/ ç›®éŒ„"""
        try:
            os.makedirs("checkpoints", exist_ok=True)
            checkpoint = {
                'model_state_dict': self.agent.policy_net.state_dict(),
                'optimizer_state_dict': self.agent.optimizer.state_dict(),
                'episode_rewards': self.episode_rewards,
                'win_rates': self.win_rates,
                'config': self.config,
                'model_architecture': {
                    'input_size': self.config['agent']['input_size'],
                    'hidden_size': self.config['agent']['hidden_size'],
                    'num_layers': self.config['agent']['num_layers'],
                },
                'save_timestamp': datetime.now().isoformat(),
                'pytorch_version': str(torch.__version__),
            }
            path = f"checkpoints/{filename}"
            tmp_path = path + ".tmp"
            # å…ˆå¯«åˆ°è‡¨æ™‚æª”å†åŸå­æ›¿æ›ï¼Œé¿å…ç”¢ç”ŸåŠå¯«å…¥çš„å£æª”
            torch.save(checkpoint, tmp_path)
            os.replace(tmp_path, path)
            size_mb = os.path.getsize(path) / (1024 * 1024)
            logger.info(f"âœ… å·²ä¿å­˜æª¢æŸ¥é»: {path} ({size_mb:.2f} MB)")
        except Exception as e:
            logger.error(f"ä¿å­˜æª¢æŸ¥é»å¤±æ•—: {e}")

    def load_checkpoint(self, checkpoint_path: str) -> bool:
        """è¼‰å…¥æª¢æŸ¥é»ä¸¦æ¢å¾©è¨“ç·´ç‹€æ…‹"""
        try:
            if not os.path.exists(checkpoint_path):
                logger.error(f"æª¢æŸ¥é»ä¸å­˜åœ¨: {checkpoint_path}")
                return False

            logger.info(f"è¼‰å…¥æª¢æŸ¥é»: {checkpoint_path}")
            # å˜—è©¦æ¨™æº–è¼‰å…¥
            try:
                checkpoint = torch.load(checkpoint_path, map_location=self.agent.device)
            except Exception as e:
                logger.warning(f"æ¨™æº–è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦åªè¼‰å…¥æ¬Šé‡: {e}")
                checkpoint = torch.load(checkpoint_path, map_location='cpu')

            # è§£ææ¬Šé‡
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                # å…¼å®¹ç›´æ¥ä¿å­˜state_dictçš„æƒ…æ³
                state_dict = checkpoint if isinstance(checkpoint, dict) else {}

            missing, unexpected = self.agent.policy_net.load_state_dict(state_dict, strict=False)
            if missing:
                logger.warning(f"è¼‰å…¥æ™‚ç¼ºå°‘éµ(å‰è‹¥å¹²): {missing[:5]}")
            if unexpected:
                logger.warning(f"è¼‰å…¥æ™‚æœªä½¿ç”¨éµ(å‰è‹¥å¹²): {unexpected[:5]}")
            logger.info("âœ… æ¨¡å‹æ¬Šé‡è¼‰å…¥æˆåŠŸ")
            # Refresh KL anchor to loaded policy
            try:
                if hasattr(self.agent, 'set_anchor_from_current'):
                    self.agent.set_anchor_from_current()
            except Exception:
                pass

            # å„ªåŒ–å™¨
            if isinstance(checkpoint, dict) and 'optimizer_state_dict' in checkpoint:
                try:
                    self.agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    logger.info("âœ… å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥å¤±æ•—: {e}")

            # è¨“ç·´æ­·å²
            if isinstance(checkpoint, dict):
                self.episode_rewards = checkpoint.get('episode_rewards', self.episode_rewards)
                self.win_rates = checkpoint.get('win_rates', self.win_rates)

            logger.info("ğŸ‰ æª¢æŸ¥é»è¼‰å…¥å®Œæˆï¼")
            return True
        except Exception as e:
            logger.error(f"è¼‰å…¥æª¢æŸ¥é»æ™‚å‡ºéŒ¯: {e}")
            return False

    # =========================
    # [CHANGED] ä¸»è¨“ç·´æµç¨‹
    # =========================

    def train_parallel(self):
        cfg_t = self.config.get('training', {})
        num_workers = int(cfg_t.get('num_workers', max(1, (mp.cpu_count() or 2) - 1)))
        episodes_per_update = int(cfg_t.get('episodes_per_update', 16))
        max_episodes = int(cfg_t.get('max_episodes', 100000))
        eval_frequency = int(self.config.get('evaluation', {}).get('frequency', 200))
        eval_games = int(self.config.get('evaluation', {}).get('games', 30))
        win_scale = float(cfg_t.get('win_reward_scaling', 1.0))
        loss_scale = float(cfg_t.get('loss_penalty_scaling', 1.0))
        danger_scale = float(self.config.get('agent', {}).get('tactical_bonus', 0.1))
        min_batch_size = int(self.agent.config.get('min_batch_size', 512))
        visualize_every = int(cfg_t.get('visualize_every', 100))

        inflight_multiplier = int(cfg_t.get('inflight_multiplier', 2))
        target_inflight = max(1, num_workers * inflight_multiplier)

        try:
            mp_ctx = mp.get_context('spawn')
        except ValueError:
            mp_ctx = mp

        pool = mp_ctx.Pool(
            processes=num_workers,
            initializer=_worker_init_persistent,
            initargs=(self.config['agent'],)
        )

        rng = random.Random()
        best_score = -1.0
        best_win_rate = -1.0
        episodes_done_total = len(self.episode_rewards)
        current_win_rate = 0.0  # è¿½è¹¤ç•¶å‰å‹ç‡

        # ç‰ˆæœ¬æ§åˆ¶
        policy_version = 0
        pending_weight_tasks = 0

        def _make_args(send_weights: bool):
            weights_np = None
            if send_weights:
                weights_np = {k: v.detach().cpu().numpy()
                            for k, v in self.agent.policy_net.state_dict().items()}
            return {
                'policy_version': policy_version,
                'policy_state': weights_np if send_weights else None,
                'player2_training_prob': self.player2_training_prob,
                'current_win_rate': current_win_rate,  # å‚³éç•¶å‰å‹ç‡
                'seed': rng.randrange(2**31 - 1),
            }

        # å»ºç«‹åˆå§‹ in-flight ä»»å‹™
        in_flight = []
        pending_weight_tasks = num_workers
        for _ in range(target_inflight):
            send_w = pending_weight_tasks > 0
            if send_w:
                pending_weight_tasks -= 1
            ar = pool.apply_async(_worker_play_one, (_make_args(send_w),))
            in_flight.append(ar)

        logger.info(f"ğŸš€ æ¼¸é€²å¼é›£åº¦è¨“ç·´é–‹å§‹ï¼šworkers={num_workers}, target_inflight={target_inflight}")

        steps_since_update = 0
        recent_results = []  # è¿½è¹¤æœ€è¿‘çš„å°æˆ°çµæœ
        best_win_rate = 0.0
        try:
            while episodes_done_total < max_episodes:
                # è¼•é‡è¼ªè©¢å®Œæˆçš„ä»»å‹™
                i = 0
                while i < len(in_flight):
                    ar = in_flight[i]
                    if ar.ready():
                        # å–çµæœä¸¦è™•ç†
                        res = ar.get()
                        # å¾ in_flight ç§»é™¤
                        in_flight[i] = in_flight[-1]
                        in_flight.pop()
                        # ç«‹åˆ»è£œä¸Šä¸€å€‹æ–°ä»»å‹™
                        send_w = pending_weight_tasks > 0
                        if send_w:
                            pending_weight_tasks -= 1
                        in_flight.append(pool.apply_async(_worker_play_one, (_make_args(send_w),)))

                        # === consume result ===
                        err_msg = res.get('error') if isinstance(res, dict) else None
                        transitions = res.get('transitions', []) if isinstance(res, dict) else []
                        if not transitions:
                            self.episode_rewards.append(0.0)
                            recent_results.append(0)  # è¨˜éŒ„å¹³å±€
                            if err_msg:
                                logger.debug(f"worker episode returned empty transitions: {err_msg}")
                        else:
                            player_result = int(res.get('player_result', 0))
                            recent_results.append(1 if player_result == 1 else 0)  # è¨˜éŒ„å‹æ•—
                            
                            # åªä¿ç•™æœ€è¿‘1000å±€çš„çµæœä¾†è¨ˆç®—å‹ç‡
                            if len(recent_results) > 1000:
                                recent_results.pop(0)
                            
                            # æ›´æ–°ç•¶å‰å‹ç‡
                            if len(recent_results) >= 100:  # è‡³å°‘100å±€å¾Œæ‰é–‹å§‹è¨ˆç®—
                                current_win_rate = sum(recent_results) / len(recent_results)
                            
                            ep_reward_sum = 0.0
                            for idx, tr in enumerate(transitions):
                                state = tr['state']
                                action = int(tr['action'])
                                prob = float(tr['prob'])
                                # ä½¿ç”¨è¨ˆç®—å¥½çš„æœ€çµ‚çå‹µï¼ˆåŒ…å«è‡ªå®šç¾©çå‹µï¼‰
                                reward = tr.get('reward', 0.0)
                                
                                # å¦‚æœéœ€è¦é¡å¤–çš„å±éšªè¡Œç‚ºæ‡²ç½°ï¼Œå¯ä»¥æ·»åŠ 
                                # ç§»é™¤é¡å¤–çš„å±éšªæ­¥æ‡²ç½°ï¼Œé¿å…èˆ‡ shaping é‡è¤‡ï¼ˆshaping å·²åŒ…å« blunder æ‡²ç½°ï¼‰
                                # è‹¥éœ€è¦æ›´å¼·æ‡²ç½°ï¼Œè«‹æ–¼ calculate_custom_reward_global ä¸­èª¿æ•´ã€‚
                                    
                                ep_reward_sum += reward
                                done = (idx == len(transitions) - 1)
                                self.agent.store_transition(state, action, prob, reward, done)
                                steps_since_update += 1
                            self.episode_rewards.append(ep_reward_sum)

                            # æ›´æ–°ç­–ç•¥
                            if steps_since_update >= min_batch_size:
                                info = self.agent.update_policy()
                                if info is not None:
                                    self.training_losses.append(info.get('total_loss', 0.0))
                                    policy_version += 1
                                    pending_weight_tasks += num_workers
                                steps_since_update = 0

                        episodes_done_total += 1
                        if best_win_rate < current_win_rate:
                            best_win_rate = current_win_rate
                        # é€±æœŸæ€§è©•ä¼°
                        if eval_frequency > 0 and episodes_done_total % eval_frequency == 0:
                            metrics = self.evaluate_comprehensive(games=eval_games)
                            score = float(metrics.get('comprehensive_score'))

                            self.win_rates.append(score)
                            try:
                                self.agent.scheduler.step(score)
                            except Exception:
                                pass
                            
                            # é¡¯ç¤ºç•¶å‰å°æ‰‹é›£åº¦ç­‰ç´š
                            if best_win_rate < 0.2:
                                difficulty = "åˆç´š (ç´”éš¨æ©Ÿ+åªæœƒè´)"
                            elif best_win_rate < 0.4:
                                difficulty = "åˆéš (åŠ å…¥é˜²å®ˆ)"
                            elif best_win_rate < 0.55:
                                difficulty = "ä¸­éš (åå¥½ä¸­å¤®+å¼±æˆ°è¡“)"
                            elif best_win_rate < 0.7:
                                difficulty = "é€²éš (å®¹æ˜“çŠ¯éŒ¯)"
                            else:
                                difficulty = "é«˜éš (å®Œæ•´æˆ°è¡“)"
                            
                            logger.info(
                                f"ğŸ“ˆ Eps={episodes_done_total} | Score={score:.3f} | ç•¶å‰å‹ç‡={current_win_rate:.3f} | "
                                f"å°æ‰‹é›£åº¦={difficulty} | self={metrics.get('self_play', 0):.3f} minimax={metrics.get('vs_minimax', 0):.3f} rand={metrics.get('vs_random', 0):.3f}"
                            )
                            
                            try:
                                if episodes_done_total > 200 and self._detect_convergence_stagnation(episodes_done_total, score):
                                    self._handle_convergence_stagnation(episodes_done_total)
                            except Exception:
                                pass
                        
                            if score > best_score:
                                best_score = score
                                send_telegram(f"ğŸ‰ æ–°æœ€ä½³åˆ†æ•¸ Eps={episodes_done_total} | Score={best_score:.3f} | ")
                                self.save_checkpoint(f"best_model_wr_{best_score:.3f}.pt")
                            elif current_win_rate > best_win_rate:
                                best_win_rate = current_win_rate
                                send_telegram(f"ğŸ‰ æ–°æœ€ä½³we Eps={episodes_done_total} | Score={current_win_rate:.3f} | ")
                                self.save_checkpoint(f"best_model_we_{current_win_rate:.3f}.pt")
                            # è¦–è¦ºåŒ– (å®šæœŸ) ï¼šå¿«é€Ÿè©•ä¼° + ç”¢ç”Ÿå½±ç‰‡
                            if visualize_every > 0 and episodes_done_total > 0 and episodes_done_total % visualize_every == 0:
                                try:
                                    quick_games = max(5, int(eval_games // 2))
                                    metrics_v = self.evaluate_comprehensive(games=quick_games)
                                    score_v = float(metrics_v.get('comprehensive_score', 0.0))
                                    logger.info(
                                        f"ğŸ¯ è¦–è¦ºåŒ–è©•ä¼° Eps={episodes_done_total} | Score={score_v:.3f} | "
                                        f"self={metrics_v.get('self_play', 0):.3f} minimax={metrics_v.get('vs_minimax', 0):.3f} rand={metrics_v.get('vs_random', 0):.3f}"
                                    )
                                except Exception as ee:
                                    logger.warning(f"è¦–è¦ºåŒ–å‰å¿«é€Ÿè©•ä¼°å¤±æ•—ï¼š{ee}")
                                # ç”Ÿæˆè¨“ç·´å°å±€å½±ç‰‡
                                try:
                                    self.visualize_training_game(
                                        episodes_done_total,
                                        save_dir='videos',
                                        opponent='tactical',
                                        fps=2
                                    )
                                except Exception as ve:
                                    logger.warning(f"å¯è¦–è¦ºåŒ–å¤±æ•—ï¼š{ve}")
                
                try:
                    quick_games = max(5, int(eval_games // 2))
                    metrics_v = self.evaluate_comprehensive(games=quick_games)
                    score_v = float(metrics_v.get('comprehensive_score', 0.0))
                    logger.info(
                        f"ğŸ¯ è¦–è¦ºåŒ–è©•ä¼° Eps={episodes_done_total} | Score={score_v:.3f} | "
                        f"self={metrics_v.get('self_play', 0):.3f} minimax={metrics_v.get('vs_minimax', 0):.3f} rand={metrics_v.get('vs_random', 0):.3f}"
                    )
                except Exception as ee:
                    logger.warning(f"è¦–è¦ºåŒ–å‰è©•ä¼°å¤±æ•—ï¼š{ee}")
                try:
                    self.visualize_training_game(episodes_done_total, save_dir='videos', opponent='tactical', fps=2)
                except Exception as ve:
                    logger.warning(f"å¯è¦–è¦ºåŒ–å¤±æ•—ï¼š{ve}")

                # è£œè¶³ in_flight
                while len(in_flight) < target_inflight:
                    send_w = pending_weight_tasks > 0
                    if send_w:
                        pending_weight_tasks -= 1
                    in_flight.append(pool.apply_async(_worker_play_one, (_make_args(send_w),)))
                
                import time
                time.sleep(0.002)

        finally:
            try:
                pool.close(); pool.join()
            except Exception:
                pass

        logger.info("âœ… æ¼¸é€²å¼é›£åº¦è¨“ç·´å®Œæˆ")
        return self.agent


    def _record_game_frames(self, opponent: str = 'tactical', max_moves: int = 50):
        """éŠç©ä¸€å±€ä¸¦è¨˜éŒ„æ¯æ­¥çš„æ£‹ç›¤å½±æ ¼ï¼ˆ6x7æ•¸çµ„ï¼‰ã€‚"""
        frames = []
        try:
            env = make('connectx', debug=False)
            env.reset()
            moves = 0
            with torch.no_grad():
                # è¨˜éŒ„åˆå§‹æ£‹ç›¤ï¼ˆè‹¥å¯ï¼‰
                try:
                    board, _ = self.agent.extract_board_and_mark(env.state, 0)
                    frames.append(self._flat_to_2d(board))
                except Exception:
                    pass
                while not env.done and moves < max_moves:
                    actions = []
                    for p in range(2):
                        if env.state[p]['status'] != 'ACTIVE':
                            actions.append(0)
                            continue
                        board, mark = self.agent.extract_board_and_mark(env.state, p)
                        valid = self.agent.get_valid_actions(board)
                        if p == 0:
                            # æˆ‘æ–¹ç”¨ç­–ç•¥ + å®‰å…¨æª¢æŸ¥
                            a = self._choose_policy_with_tactics(board, mark, valid, training=False)
                        else:
                            if opponent == 'tactical':
                                a = self._tactical_random_opening_agent(board, mark, valid)
                            elif opponent == 'minimax':
                                a = self._choose_minimax_move(board, mark, max_depth=int(self.config.get('evaluation', {}).get('minimax_depth', 3)))
                            else:
                                # random family uses the same unified logic
                                a = self._tactical_random_opening_agent(board, mark, valid)
                        actions.append(int(a))
                    try:
                        env.step(actions)
                    except Exception:
                        break
                    moves += 1
                    try:
                        board, _ = self.agent.extract_board_and_mark(env.state, 0)
                        frames.append(self._flat_to_2d(board))
                    except Exception:
                        pass
        except Exception:
            pass
        return frames

    def visualize_training_game(self, episode_idx: int, save_dir: str = 'videos', opponent: str = 'tactical', fps: int = 2):
        """å°‡ä¸€å±€å¯è¦–åŒ–ä¸¦è¼¸å‡ºç‚ºå½±ç‰‡ï¼ˆå¼·åˆ¶ä½¿ç”¨FFmpeg MP4ï¼‰ã€‚è¿”å›è¼¸å‡ºè·¯å¾‘æˆ–Noneã€‚"""
        if not globals().get('VISUALIZATION_AVAILABLE', False):
            logger.info("æœªå®‰è£å¯è¦–åŒ–ä¾è³´ï¼Œç•¥éå½±ç‰‡è¼¸å‡ºã€‚")
            return None
        try:
            import shutil
            import matplotlib
            import matplotlib.pyplot as plt
            from matplotlib import animation
            from matplotlib.patches import Patch
            # Configure ffmpeg path from system PATH. Force MP4 via FFmpeg.
            ffmpeg_path = shutil.which('ffmpeg')
            if not ffmpeg_path:
                raise RuntimeError("ffmpeg not found in PATH. Please install ffmpeg to export MP4.")
            matplotlib.rcParams['animation.ffmpeg_path'] = ffmpeg_path
        except Exception as e:
            logger.warning(f"è¼‰å…¥matplotlib/ffmpegå¤±æ•—ï¼Œç•¥éå½±ç‰‡è¼¸å‡º: {e}")
            return None

        frames = self._record_game_frames(opponent=opponent)
        if not frames:
            logger.info("ç„¡å¯è¦–åŒ–å½±æ ¼ï¼Œç•¥éå½±ç‰‡è¼¸å‡ºã€‚")
            return None

        # è®“æœ€å¾Œä¸€å¹€å¤šåœç•™ä¸€æœƒï¼ˆç´„2ç§’ï¼‰
        try:
            hold_frames = max(1, int(fps * 2))  # åœç•™ 2 ç§’
            frames = frames + [frames[-1]] * hold_frames
        except Exception:
            pass

        # æº–å‚™è¼¸å‡ºç›®éŒ„èˆ‡æª”å
        os.makedirs(save_dir, exist_ok=True)
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        mp4_path = os.path.join(save_dir, f"episode_{episode_idx}_{ts}.mp4")
        # gif_path ä¿ç•™ä½†ä¸ä½¿ç”¨ï¼Œè‹¥éœ€å›é€€å¯å•Ÿç”¨
        gif_path = os.path.join(save_dir, f"episode_{episode_idx}_{ts}.gif")

        # å»ºç«‹ç•«å¸ƒèˆ‡åº§æ¨™è»¸
        fig, ax = plt.subplots(figsize=(5.6, 4.8))
        ax.set_xlim(-0.5, 6.5)
        ax.set_ylim(-0.5, 5.5)
        ax.set_xticks(range(7))
        ax.set_yticks(range(6))
        ax.grid(True)
        ax.set_title(f"Episode {episode_idx} vs {opponent}")

        # æ–°å¢åœ–ä¾‹èªªæ˜é¡è‰²: ç´…=Agent(P1), é‡‘=Opponent(P2)
        handles = [
            Patch(color='red', label='Agent (P1)'),
            Patch(color='gold', label='Opponent (P2)')
        ]
        ax.legend(handles=handles, loc='upper right', framealpha=0.9)

        # åˆå§‹åŒ–åœ“ç‰‡é›†åˆ
        discs = []
        def draw_board(grid):
            # æ¸…é™¤å…ˆå‰åœ“ç‰‡
            for d in discs:
                d.remove()
            discs.clear()
            # ç¹ªè£½æ£‹å­ï¼šgrid[r][c] 1->ç´…, 2->é»ƒ
            for r in range(6):
                for c in range(7):
                    v = grid[r][c]
                    if v == 0:
                        continue
                    color = 'red' if v == 1 else 'gold'
                    circle = plt.Circle((c, 5 - r), 0.4, color=color)
                    ax.add_patch(circle)
                    discs.append(circle)

        def init():
            draw_board(frames[0])
            return discs

        def update(i):
            draw_board(frames[i])
            return discs

        anim = animation.FuncAnimation(fig, update, init_func=init, frames=len(frames), interval=int(1000 / max(1, fps)), blit=False)

        out_path = None
        # å¼·åˆ¶ä½¿ç”¨ FFmpeg è¼¸å‡º MP4ï¼ˆå¦‚æœå¤±æ•—å‰‡ä¸å›é€€ GIFï¼‰
        try:
            writer = animation.FFMpegWriter(fps=fps, codec='mpeg4', bitrate=1800, extra_args=['-pix_fmt', 'yuv420p'])
            anim.save(mp4_path, writer=writer)
            out_path = mp4_path
        except Exception as e:
            logger.warning(f"ä½¿ç”¨FFmpegè¼¸å‡ºMP4å¤±æ•—: {e}")
            out_path = None
        finally:
            plt.close(fig)

        if out_path:
            logger.info(f"ğŸ¬ å·²è¼¸å‡ºè¨“ç·´å°å±€å½±ç‰‡: {out_path}")
        return out_path

def send_telegram(msg: str):
    token = os.getenv("TELEGRAM_BOT_TOKEN") or os.getenv("BOT_TOKEN")
    chat_id = os.getenv("TELEGRAM_CHAT_ID") or "6166024220"
    if not token or not chat_id:
        logger.info("æœªè¨­ç½® TELEGRAM_BOT_TOKEN/TELEGRAM_CHAT_IDï¼Œç•¥éè¨Šæ¯é€šçŸ¥ã€‚")
        return
    try:
        import requests
        base = f"https://api.telegram.org/bot{token}/sendMessage"
        payload = {"chat_id": chat_id, "text": msg}
        r = requests.post(base, data=payload, timeout=3.0)
        logger.info("å·²ç™¼é€ Telegram é€šçŸ¥ã€‚")
    except Exception as e:
        logger.warning(f"Telegram ç™¼é€å¤±æ•—: {e}")



def main():
    """Main entry: load config, resume from latest checkpoint, train, notify."""
    # Local imports to keep global scope clean
    from datetime import datetime

    # --- helpers ---
    def parse_ts_from_name(fname: str):
        FINAL_RE = re.compile(r"^final_(\d{8})_(\d{6})\.pt$")
        TS_TAIL_RE = re.compile(r"_(\d{8})_(\d{6})\.pt$")
        m = FINAL_RE.match(fname)
        if m:
            try:
                return datetime.strptime(m.group(1) + m.group(2), "%Y%m%d%H%M%S")
            except Exception:
                return None
        m = TS_TAIL_RE.search(fname)
        if m:
            try:
                return datetime.strptime(m.group(1) + m.group(2), "%Y%m%d%H%M%S")
            except Exception:
                return None
        return None

    def choose_latest_checkpoint_by_name(files):
        latest = None
        latest_time = None
        for p in files:
            fname = os.path.basename(p)
            ts = parse_ts_from_name(fname)
            try:
                t = ts.timestamp() if ts is not None else os.path.getmtime(p)
            except OSError:
                t = 0
            if latest is None or t > latest_time:
                latest = p
                latest_time = t
        return latest

    # --- æ–°å¢ï¼šæŒ‘é¸ best_model_wr_X.pt ä¸­ win rate æœ€é«˜è€… ---
    def choose_best_wr_checkpoint(files):
        """åœ¨æª”åä¸­å°‹æ‰¾ best_model_wr_*.ptï¼Œå– win rate æœ€å¤§è€…ï¼Œä¸¦é©—è­‰å¯è®€å–ã€‚
        ä¾‹å¦‚: best_model_wr_0.393.pt > best_model_wr_0.380.pt
        è‹¥è®€å–å¤±æ•—å‰‡è·³éï¼Œå…¨éƒ¨å¤±æ•—å›å‚³ Noneã€‚"""
        import re as _re
        pattern = _re.compile(r"best_model_wr_(\d+(?:\.\d+)?)\.pt$")
        # æ”¶é›† (wr, path)
        candidates = []
        for p in files:
            m = pattern.search(os.path.basename(p))
            if m:
                try:
                    wr = float(m.group(1))
                    candidates.append((wr, p))
                except ValueError:
                    continue
        if not candidates:
            return None
        # ä¾ win rate ç”±å¤§åˆ°å°æ’åº
        candidates.sort(key=lambda x: x[0], reverse=True)
        for wr, path in candidates:
            try:
                # å¿«é€Ÿé©—è­‰æª”æ¡ˆæ˜¯å¦å¯è¢« torch.loadï¼ˆä¸å¿…å¥—ç”¨ï¼‰
                _ = torch.load(path, map_location='cpu')
                logger.info(f"é¸æ“‡æœ€é«˜å‹ç‡æª¢æŸ¥é»: {os.path.basename(path)} (wr={wr:.3f})")
                return path
            except Exception as e:
                logger.warning(f"ç•¥éä¸å¯ç”¨å‹ç‡æª¢æŸ¥é» {path}: {e}")
                continue
        return None

    def find_working_checkpoint(files):
        """å›å‚³ç¬¬ä¸€å€‹å¯æˆåŠŸ torch.load çš„æª¢æŸ¥é»ï¼ˆä¾æ™‚é–“æ–°â†’èˆŠï¼‰ã€‚è‹¥æ²’æœ‰å‰‡å›å‚³ Noneã€‚"""
        def _mtime(p):
            try:
                ts = parse_ts_from_name(os.path.basename(p))
                return ts.timestamp() if ts else os.path.getmtime(p)
            except Exception:
                return 0
        for p in sorted(files, key=_mtime, reverse=True):
            try:
                _ = torch.load(p, map_location='cpu')
                logger.info(f"æª¢æŸ¥é»å¯ç”¨ï¼š{p}")
                return p
            except Exception as e:
                logger.warning(f"ç•¥éä¸å¯ç”¨æª¢æŸ¥é» {p}: {e}")
                continue
        return None
    # --- choose config ---
    cfg_path = "config.yaml"
    if not os.path.exists(cfg_path):
        alt_path = "connectx_config.yaml"
        if os.path.exists(alt_path):
            cfg_path = alt_path
        else:
            logger.error("æ‰¾ä¸åˆ° config.yaml æˆ– connectx_config.yamlï¼Œç„¡æ³•å•Ÿå‹•è¨“ç·´ã€‚")
            send_telegram("âš ï¸ æ‰¾ä¸åˆ°è¨­å®šæª”ï¼Œè¨“ç·´æœªå•Ÿå‹•ã€‚")
            return

    trainer = ConnectXTrainer(cfg_path)

    # --- resume from checkpoint ---
    resume_from_env = os.getenv("CHECKPOINT_PATH") or os.getenv("RESUME_FROM")
    ckpt_to_load = None
    if resume_from_env and os.path.exists(resume_from_env):
        ckpt_to_load = resume_from_env
    else:
        files = glob.glob(os.path.join('checkpoints', '*.pt'))
        # 1) å…ˆæ‰¾æœ€é«˜å‹ç‡ best_model_wr_*.pt
        ckpt_to_load = choose_best_wr_checkpoint(files)
        if ckpt_to_load is None:
            # 2) å†æ‰¾å¯ç”¨ (æ™‚é–“æ–°â†’èˆŠ) æª¢æŸ¥é»
            ckpt_to_load = find_working_checkpoint(files)
            if ckpt_to_load is None:
                # 3) é€€å›åŸæœ¬ä¾åç¨±/mtime çš„æœ€æ–°è€…
                ckpt_to_load = choose_latest_checkpoint_by_name(files)

    ckpt_to_load = 'perfect_imitation_model.pt'
    logger.info(f"ckpt_to_load: {ckpt_to_load}")
    send_telegram("ä½¿ç”¨: "+str(ckpt_to_load)+" modelä¾†ç¹¼çºŒ")
    if ckpt_to_load:
        loaded = trainer.load_checkpoint(ckpt_to_load)
        if not loaded:
            logger.warning(f"ç„¡æ³•è¼‰å…¥æª¢æŸ¥é»: {ckpt_to_load}ï¼Œå°‡ä»¥éš¨æ©Ÿåˆå§‹åŒ–é–‹å§‹ã€‚")
    else:
        logger.info("æœªæ‰¾åˆ°å¯ç”¨æª¢æŸ¥é»ï¼Œå°‡ä»¥éš¨æ©Ÿåˆå§‹åŒ–é–‹å§‹è¨“ç·´ã€‚")
    # --- train ---
    start_ts = datetime.now()
    err = None
    try:
        training_cfg = trainer.config.get('training', {})
        use_parallel = bool(training_cfg.get('parallel_rollout', True))
        if use_parallel:
            logger.info("ä½¿ç”¨å¹³è¡Œè’é›†é€²è¡Œè¨“ç·´ train_parallel()")
            trainer.train_parallel()
            # trainer.train_with_ray()
        else:
            logger.info("ä½¿ç”¨å–®åŸ·è¡Œç·’è¨“ç·´ train()")
           
            trainer.train()
    except Exception as e:
        err = e
        logger.error(f"è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}\n{traceback.format_exc()}")
    finally:
        pass
    # --- notify ---

    elapsed = datetime.now() - start_ts
    episodes_done = len(trainer.episode_rewards)
    last_wr = trainer.win_rates[-1] if trainer.win_rates else None
    if err is None:
        msg = f"ğŸ‰ è¨“ç·´å®Œç•¢ï¼\nå›åˆæ•¸: {episodes_done}\næœ€å¾Œå‹ç‡: {last_wr if last_wr is not None else 'N/A'}\nè€—æ™‚: {str(elapsed).split('.')[0]}"
    else:
        msg = f"âš ï¸ è¨“ç·´å¤±æ•—: {err}\nå›åˆæ•¸: {episodes_done}\nè€—æ™‚: {str(elapsed).split('.')[0]}"
    send_telegram(msg)

if __name__ == "__main__":
    main()
