#!/usr/bin/env python3
"""
ConnectX ç›£ç£å­¸ç¿’è¨“ç·´è…³æœ¬
ä½¿ç”¨ connectx-state-action-value.txt è³‡æ–™é›†é€²è¡Œè¨“ç·´
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import logging
import time
from datetime import datetime
from collections import deque
from kaggle_environments import make
from tqdm import tqdm

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ConnectXNet(nn.Module):
    """å¼ºåŒ–å­¦ä¹ ç”¨çš„ ConnectX æ·±åº¦ç¥ç»ç½‘ç»œ"""

    def __init__(self, input_size=126, hidden_size=150, num_layers=4):
        super(ConnectXNet, self).__init__()

        # ç¡®ä¿å‚æ•°æ˜¯æ­£ç¡®çš„Python intç±»å‹
        input_size = int(input_size)
        hidden_size = int(hidden_size)
        num_layers = int(num_layers)

        # è¾“å…¥å±‚
        print(f"Initializing ConnectXNet with input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}")
        self.input_layer = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # éšè—å±‚ï¼ˆæ®‹å·®è¿æ¥ + å±‚æ­£è§„åŒ–ä»£æ›¿æ‰¹é‡æ­£è§„åŒ–ï¼‰
        self.hidden_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.LayerNorm(hidden_size),  # ä½¿ç”¨ LayerNorm ä»£æ›¿ BatchNorm1d
                nn.ReLU(),
                nn.Dropout(0.15),  # ç¨å¾®å¢åŠ dropout
                nn.Linear(hidden_size, hidden_size),
                nn.LayerNorm(hidden_size)   # ä½¿ç”¨ LayerNorm ä»£æ›¿ BatchNorm1d
            ) for _ in range(num_layers)
        ])

        # ç­–ç•¥å¤´ï¼ˆåŠ¨ä½œæ¦‚ç‡ï¼‰
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),  # ä½¿ç”¨ LayerNorm ä»£æ›¿ BatchNorm1d
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 7),  # 7 åˆ—
            nn.Softmax(dim=-1)
        )

        # ä»·å€¼å¤´ï¼ˆçŠ¶æ€ä»·å€¼ï¼‰
        self.value_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),  # ä½¿ç”¨ LayerNorm ä»£æ›¿ BatchNorm1d
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 1),
            nn.Tanh()
        )

    def forward(self, x):
        # è¾“å…¥å¤„ç†
        x = self.input_layer(x)

        # æ®‹å·®è¿æ¥
        for hidden_layer in self.hidden_layers:
            residual = x
            x = hidden_layer(x)
            x = torch.relu(x + residual)

        # è¾“å‡ºå¤´
        policy = self.policy_head(x)
        value = self.value_head(x)

        return policy, value
class PPOAgent:
    """PPO å¼·åŒ–å­¸ç¿’æ™ºèƒ½é«”"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ç¥ç¶“ç¶²è·¯
        self.policy_net = ConnectXNet(
            input_size=config['input_size'],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers']
        ).to(self.device)

        # å„ªåŒ–å™¨
        self.optimizer = optim.Adam(
            self.policy_net.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )

        # å­¸ç¿’ç‡èª¿åº¦å™¨ - æ ¹æ“šæ€§èƒ½å‹•æ…‹èª¿æ•´
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',  # ç›£æ§å‹ç‡æœ€å¤§åŒ–
            factor=0.7,  # å­¸ç¿’ç‡é™ä½å› å­
            patience=1000,  # ç­‰å¾…å›åˆæ•¸
            min_lr=1e-8
        )

        # è¨“ç·´åƒæ•¸
        self.gamma = config['gamma']
        self.eps_clip = config['eps_clip']
        self.k_epochs = config['k_epochs']
        self.entropy_coef = config['entropy_coef']
        self.value_coef = config['value_coef']

        # ç¶“é©—ç·©è¡å€
        self.memory = deque(maxlen=config['buffer_size'])

    def extract_board_and_mark(self, env_state, player_idx):
        """å¾ç’°å¢ƒç‹€æ…‹ä¸­æå–æ£‹ç›¤å’Œç©å®¶æ¨™è¨˜"""
        try:
            # æª¢æŸ¥ç’°å¢ƒç‹€æ…‹åŸºæœ¬çµæ§‹
            if not env_state or len(env_state) <= player_idx:
                logger.warning(f"ç’°å¢ƒç‹€æ…‹ç„¡æ•ˆæˆ–ç©å®¶ç´¢å¼•è¶…å‡ºç¯„åœ: len={len(env_state) if env_state else 0}, player_idx={player_idx}")
                return [0] * 42, player_idx + 1

            # ç²å–ç©å®¶ç‹€æ…‹
            player_state = env_state[player_idx]
            if 'observation' not in player_state:
                logger.warning(f"ç©å®¶ {player_idx} ç‹€æ…‹ä¸­æ²’æœ‰ observation")
                return [0] * 42, player_idx + 1

            obs = player_state['observation']

            # é¦–å…ˆå˜—è©¦ç²å–æ¨™è¨˜
            mark = None
            if 'mark' in obs:
                mark = obs['mark']
            elif hasattr(obs, 'mark'):
                mark = obs.mark
            else:
                mark = player_idx + 1  # é»˜èªæ¨™è¨˜
                logger.warning(f"ç„¡æ³•ç²å–ç©å®¶æ¨™è¨˜ï¼Œä½¿ç”¨é»˜èªå€¼: {mark}")

            # å˜—è©¦ç²å–æ£‹ç›¤
            board = None

            # æ–¹æ³•1: ç›´æ¥å¾ç•¶å‰ç©å®¶è§€å¯Ÿç²å–
            if 'board' in obs:
                board = obs['board']
                logger.debug(f"å¾ç©å®¶ {player_idx} å­—å…¸æ–¹å¼ç²å–æ£‹ç›¤")
            elif hasattr(obs, 'board'):
                board = obs.board
                logger.debug(f"å¾ç©å®¶ {player_idx} å±¬æ€§æ–¹å¼ç²å–æ£‹ç›¤")

            # æ–¹æ³•2: å¦‚æœç•¶å‰ç©å®¶æ²’æœ‰æ£‹ç›¤ï¼Œå¾å…¶ä»–ç©å®¶ç²å–
            if board is None:
                logger.warning(f"ç©å®¶ {player_idx} è§€å¯Ÿä¸­æ²’æœ‰æ£‹ç›¤æ•¸æ“šï¼Œå¯ç”¨éµ: {list(obs.keys()) if hasattr(obs, 'keys') else 'N/A'}")
                logger.warning(f"ç©å®¶ {player_idx} ç‹€æ…‹: {player_state.get('status', 'Unknown')}")

                # å˜—è©¦å¾å…¶ä»–ç©å®¶ç²å–
                for other_idx in range(len(env_state)):
                    if other_idx != player_idx:
                        try:
                            other_state = env_state[other_idx]
                            if 'observation' in other_state:
                                other_obs = other_state['observation']
                                if 'board' in other_obs:
                                    board = other_obs['board']
                                    logger.info(f"å¾ç©å®¶ {other_idx} ç²å–æ£‹ç›¤ (å­—å…¸æ–¹å¼)")
                                    break
                                elif hasattr(other_obs, 'board'):
                                    board = other_obs.board
                                    logger.info(f"å¾ç©å®¶ {other_idx} ç²å–æ£‹ç›¤ (å±¬æ€§æ–¹å¼)")
                                    break
                        except Exception as e:
                            logger.debug(f"å¾ç©å®¶ {other_idx} ç²å–æ£‹ç›¤å¤±æ•—: {e}")
                            continue

            # æ–¹æ³•3: æœ€å¾Œå‚™ç”¨æ–¹æ¡ˆ
            if board is None:
                logger.warning("æ‰€æœ‰æ–¹æ³•éƒ½ç„¡æ³•ç²å–æ£‹ç›¤ï¼Œä½¿ç”¨ç©ºæ£‹ç›¤")
                board = [0] * 42

            # é©—è­‰æ£‹ç›¤æ•¸æ“š
            if not isinstance(board, (list, tuple)) or len(board) != 42:
                logger.warning(f"æ£‹ç›¤æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º: type={type(board)}, len={len(board) if hasattr(board, '__len__') else 'N/A'}")
                board = [0] * 42

            logger.debug(f"æˆåŠŸæå–ç©å®¶ {player_idx} çš„ç‹€æ…‹: æ£‹ç›¤é•·åº¦={len(board)}, æ¨™è¨˜={mark}")
            return list(board), mark

        except Exception as e:
            logger.error(f"æå–æ£‹ç›¤ç‹€æ…‹æ™‚å‡ºéŒ¯: {e}")
            logger.error(f"ç’°å¢ƒç‹€æ…‹é¡å‹: {type(env_state)}")

            # è©³ç´°èª¿è©¦ä¿¡æ¯
            try:
                if env_state and len(env_state) > player_idx:
                    player_state = env_state[player_idx]
                    logger.error(f"ç©å®¶ {player_idx} ç‹€æ…‹éµ: {list(player_state.keys()) if hasattr(player_state, 'keys') else 'N/A'}")
                    logger.error(f"ç©å®¶ {player_idx} ç‹€æ…‹: {player_state.get('status', 'Unknown')}")

                    if 'observation' in player_state:
                        obs = player_state['observation']
                        obs_keys = list(obs.keys()) if hasattr(obs, 'keys') else [attr for attr in dir(obs) if not attr.startswith('_')]
                        logger.error(f"è§€å¯Ÿéµ: {obs_keys}")

                        # æª¢æŸ¥è§€å¯Ÿçš„å…§å®¹
                        if hasattr(obs, 'keys'):
                            for key in obs.keys():
                                try:
                                    value = obs[key]
                                    logger.error(f"  {key}: {type(value)} = {value}")
                                except:
                                    logger.error(f"  {key}: ç„¡æ³•è¨ªå•")
            except Exception as debug_e:
                logger.error(f"èª¿è©¦ä¿¡æ¯æ”¶é›†å¤±æ•—: {debug_e}")

            # è¿”å›é»˜èªå€¼
            return [0] * 42, player_idx + 1

    def encode_state(self, board, mark):
        """ç·¨ç¢¼æ£‹ç›¤ç‹€æ…‹"""
        # ç¢ºä¿ board æ˜¯æœ‰æ•ˆçš„
        if not board:
            board = [0] * 42
        elif len(board) != 42:
            # å¦‚æœé•·åº¦ä¸å°ï¼Œèª¿æ•´æˆ–å¡«å……
            if len(board) < 42:
                board = list(board) + [0] * (42 - len(board))
            else:
                board = list(board)[:42]

        # è½‰æ›ç‚º 6x7 çŸ©é™£
        state = np.array(board).reshape(6, 7)

        # å‰µå»ºä¸‰å€‹ç‰¹å¾µé€šé“
        # é€šé“ 1: ç•¶å‰ç©å®¶çš„æ£‹å­
        player_pieces = (state == mark).astype(np.float32)
        # é€šé“ 2: å°æ‰‹çš„æ£‹å­
        opponent_pieces = (state == (3 - mark)).astype(np.float32)
        # é€šé“ 3: ç©ºä½
        empty_spaces = (state == 0).astype(np.float32)

        # æ‹‰å¹³ä¸¦é€£æ¥
        encoded = np.concatenate([
            player_pieces.flatten(),
            opponent_pieces.flatten(),
            empty_spaces.flatten()
        ])

        return encoded

    def get_valid_actions(self, board):
        """ç²å–æœ‰æ•ˆå‹•ä½œ"""
        # ç¢ºä¿ board æ˜¯æœ‰æ•ˆçš„
        if not board or len(board) != 42:
            board = [0] * 42

        # æª¢æŸ¥æ¯ä¸€åˆ—çš„é ‚éƒ¨æ˜¯å¦ç‚ºç©º
        valid_actions = []
        for col in range(7):
            if board[col] == 0:  # æª¢æŸ¥æ¯åˆ—çš„é ‚éƒ¨
                valid_actions.append(col)

        # å¦‚æœæ²’æœ‰æœ‰æ•ˆå‹•ä½œï¼Œè¿”å›æ‰€æœ‰åˆ—ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
        if not valid_actions:
            valid_actions = list(range(7))

        return valid_actions

    def select_action(self, state, valid_actions, training=True, temperature=1.0, exploration_bonus=0.0):
        """é¸æ“‡å‹•ä½œï¼ˆæ”¯æŒæº«åº¦æ¡æ¨£å’Œæ¢ç´¢çå‹µï¼‰"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        # æ ¹æ“š batch size æ±ºå®šæ˜¯å¦ä½¿ç”¨ BatchNorm
        use_eval_mode = state_tensor.size(0) == 1  # å–®å€‹æ¨£æœ¬æ™‚ä½¿ç”¨evalæ¨¡å¼

        if use_eval_mode:
            # å–®å€‹æ¨£æœ¬æ™‚ä½¿ç”¨è©•ä¼°æ¨¡å¼é¿å… BatchNorm å•é¡Œ
            self.policy_net.eval()

        with torch.no_grad():
            action_probs, state_value = self.policy_net(state_tensor)

        # æ¢å¾©è¨“ç·´æ¨¡å¼
        if training and use_eval_mode:
            self.policy_net.train()

        # é®ç½©ç„¡æ•ˆå‹•ä½œ
        action_probs = action_probs.cpu().numpy()[0]
        masked_probs = np.zeros_like(action_probs)
        masked_probs[valid_actions] = action_probs[valid_actions]

        # æ­£è¦åŒ–æ¦‚ç‡
        if masked_probs.sum() > 0:
            masked_probs /= masked_probs.sum()
        else:
            # å¾Œå‚™æ–¹æ¡ˆï¼šå‡å‹»åˆ†ä½ˆ
            masked_probs[valid_actions] = 1.0 / len(valid_actions)

        # æ‡‰ç”¨æº«åº¦å’Œæ¢ç´¢çå‹µ
        if training and (temperature != 1.0 or exploration_bonus > 0.0):
            # æº«åº¦æ¡æ¨£
            if temperature != 1.0:
                masked_probs = np.power(masked_probs + 1e-8, 1/temperature)
                masked_probs /= masked_probs.sum()

            # æ¢ç´¢çå‹µï¼ˆç‚ºä¸å¸¸é¸æ“‡çš„å‹•ä½œå¢åŠ æ¦‚ç‡ï¼‰
            if exploration_bonus > 0.0:
                uniform_dist = np.zeros_like(masked_probs)
                uniform_dist[valid_actions] = 1.0 / len(valid_actions)
                masked_probs = (1 - exploration_bonus) * masked_probs + exploration_bonus * uniform_dist

        if training:
            # è¨“ç·´æ™‚æ¡æ¨£å‹•ä½œ
            action = np.random.choice(7, p=masked_probs)
        else:
            # è©•ä¼°æ™‚é¸æ“‡æœ€ä½³å‹•ä½œ
            action = valid_actions[np.argmax(masked_probs[valid_actions])]

        # ç¢ºä¿è¿”å›çš„å‹•ä½œæ˜¯ Python int é¡å‹ï¼Œé¿å… numpy é¡å‹å•é¡Œ
        action = int(action)

        return action, action_probs[action], state_value.item()

    def store_transition(self, state, action, prob, reward, done):
        """å„²å­˜è½‰æ›"""
        self.memory.append((state, action, prob, reward, done))

    def compute_gae(self, rewards, values, dones, next_value, gamma=0.99, lam=0.95):
        """è¨ˆç®—å»£ç¾©å„ªå‹¢ä¼°è¨ˆï¼ˆGAEï¼‰"""
        advantages = []
        gae = 0

        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_val = next_value
            else:
                next_val = values[i + 1]

            delta = rewards[i] + gamma * next_val * (1 - dones[i]) - values[i]
            gae = delta + gamma * lam * (1 - dones[i]) * gae
            advantages.insert(0, gae)

        returns = [adv + val for adv, val in zip(advantages, values)]
        return advantages, returns

    def update_policy(self):
        """ä½¿ç”¨ PPO æ›´æ–°ç­–ç•¥"""
        if len(self.memory) < self.config['min_batch_size']:
            return None

        # æº–å‚™è¨“ç·´æ•¸æ“š
        states = []
        actions = []
        old_probs = []
        rewards = []
        dones = []

        for transition in self.memory:
            state, action, prob, reward, done = transition
            states.append(state)
            actions.append(action)
            old_probs.append(prob)
            rewards.append(reward)
            dones.append(done)

        # è¨ˆç®—æ‰€æœ‰ç‹€æ…‹çš„åƒ¹å€¼
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        with torch.no_grad():
            _, values_tensor = self.policy_net(states_tensor)
            values = values_tensor.cpu().numpy().flatten()

        # è¨ˆç®—å„ªå‹¢å’Œå›å ±
        advantages, returns = self.compute_gae(
            rewards, values, dones, 0,
            self.gamma, self.config['gae_lambda']
        )

        # æ­£è¦åŒ–å„ªå‹¢
        advantages = np.array(advantages)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # è½‰æ›ç‚ºå¼µé‡
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        actions_tensor = torch.LongTensor(actions).to(self.device)
        old_probs_tensor = torch.FloatTensor(old_probs).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        returns_tensor = torch.FloatTensor(returns).to(self.device)

        # PPO æ›´æ–°
        total_loss = 0
        for _ in range(self.k_epochs):
            # å‰å‘å‚³æ’­
            new_probs, values = self.policy_net(states_tensor)

            # è¨ˆç®—æ¯”ç‡
            new_action_probs = new_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze()
            ratio = new_action_probs / (old_probs_tensor + 1e-8)

            # PPO æå¤±
            surr1 = ratio * advantages_tensor
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages_tensor
            policy_loss = -torch.min(surr1, surr2).mean()

            # åƒ¹å€¼æå¤±
            value_loss = nn.MSELoss()(values.squeeze(), returns_tensor)

            # ç†µæå¤±ï¼ˆæ¢ç´¢ï¼‰
            entropy = -(new_probs * torch.log(new_probs + 1e-8)).sum(dim=1).mean()

            # ç¸½æå¤±
            loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
            total_loss += loss.item()

            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
            self.optimizer.step()

        # æ¸…ç©ºè¨˜æ†¶é«”
        self.memory.clear()

        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': entropy.item(),
            'total_loss': total_loss / self.k_epochs
        }


class ConnectXTrainer:
    """ConnectX è¨“ç·´å™¨"""

    def __init__(self, config_path_or_dict="config_supervised.yaml"):
        # è¼‰å…¥é…ç½®
        if isinstance(config_path_or_dict, dict):
            self.config = config_path_or_dict
        else:
            with open(config_path_or_dict, 'r') as f:
                self.config = yaml.safe_load(f)

        # åˆå§‹åŒ–æ™ºèƒ½é«”
        self.agent = PPOAgent(self.config['agent'])

        # è¨“ç·´çµ±è¨ˆ
        self.episode_rewards = []
        self.win_rates = []
        self.training_losses = []

        # æŒçºŒå­¸ç¿’æ•¸æ“š
        self.continuous_learning_data = None
        self.continuous_learning_targets = None

        # å‰µå»ºç›®éŒ„
        os.makedirs('checkpoints', exist_ok=True)
        os.makedirs('logs', exist_ok=True)

    def load_state_action_dataset(self, file_path="connectx-state-action-value.txt", max_lines=-1):
        """
        è¼‰å…¥ç‹€æ…‹-å‹•ä½œåƒ¹å€¼æ•¸æ“šé›†

        æ ¼å¼èªªæ˜ï¼š
        - æ¯è¡ŒåŒ…å«ï¼šæ£‹ç›¤ç‹€æ…‹(42å­—ç¬¦) + 7å€‹å‹•ä½œåƒ¹å€¼ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰
        - æ£‹ç›¤ç‹€æ…‹ï¼š0=ç©ºï¼Œ1=å…ˆæ‰‹ï¼Œ2=å¾Œæ‰‹ï¼ˆå¾å·¦åˆ°å³ï¼Œå¾ä¸Šåˆ°ä¸‹ï¼‰
        - å‹•ä½œåƒ¹å€¼ï¼šæ­£æ•¸=å…ˆæ‰‹è´ï¼ˆæ­¥æ•¸ï¼‰ï¼Œè² æ•¸=å¾Œæ‰‹è´ï¼Œ0=å¹³å±€ï¼Œç©º=ç„¡æ•ˆå‹•ä½œ
        """
        states = []
        action_values = []
        skipped_lines = 0

        try:
            logger.info(f"è¼‰å…¥è¨“ç·´æ•¸æ“šé›†: {file_path}")

            # å¦‚æœæ²’æœ‰é™åˆ¶è¡Œæ•¸ï¼Œå…ˆå¿«é€Ÿæƒææ–‡ä»¶ç¸½è¡Œæ•¸
            if max_lines == -1:
                logger.info("æ­£åœ¨æƒææ•¸æ“šé›†ç¸½è¡Œæ•¸...")
                with open(file_path, 'r') as f:
                    total_lines = sum(1 for _ in f)
                max_lines = total_lines
                logger.info(f"æ•¸æ“šé›†ç¸½è¡Œæ•¸: {total_lines}")
            else:
                logger.info(f"é™åˆ¶è¼‰å…¥è¡Œæ•¸: {max_lines}")

            # åªè®€å–éœ€è¦çš„è¡Œæ•¸ï¼Œé¿å…è¨˜æ†¶é«”å•é¡Œ
            lines = []
            with open(file_path, 'r') as f:
                for i, line in enumerate(f):
                    if i >= max_lines:
                        break
                    lines.append(line)

            logger.info(f"å¯¦éš›è¼‰å…¥è¡Œæ•¸: {len(lines)}")
            try:
                from tqdm.auto import tqdm
            except ImportError:
                from tqdm import tqdm

            with tqdm(total=len(lines), desc="read dataset") as pbar:
                for line_idx, line in enumerate(lines):
                    pbar.update(1)

                    line = line.strip()
                    if not line:
                        continue

                    try:
                        # æ‰¾åˆ°ç¬¬ä¸€å€‹é€—è™Ÿçš„ä½ç½®ï¼Œåˆ†å‰²æ£‹ç›¤ç‹€æ…‹å’Œå‹•ä½œåƒ¹å€¼
                        comma_idx = line.find(',')
                        if comma_idx == -1:
                            logger.warning(f"ç¬¬ {line_idx + 1} è¡Œæ ¼å¼éŒ¯èª¤ï¼Œæ‰¾ä¸åˆ°é€—è™Ÿåˆ†éš”ç¬¦")
                            skipped_lines += 1
                            continue

                        # è§£ææ£‹ç›¤ç‹€æ…‹ (é€—è™Ÿå‰çš„42å€‹å­—ç¬¦)
                        board_part = line[:comma_idx]
                        if len(board_part) != 42:
                            logger.warning(f"ç¬¬ {line_idx + 1} è¡Œæ£‹ç›¤ç‹€æ…‹é•·åº¦éŒ¯èª¤ï¼ŒæœŸæœ›42ï¼Œå¾—åˆ°{len(board_part)}")
                            skipped_lines += 1
                            continue

                        # é©—è­‰æ£‹ç›¤ç‹€æ…‹åªåŒ…å« 0, 1, 2
                        try:
                            board_state = []
                            for char in board_part:
                                if char not in '012':
                                    raise ValueError(f"ç„¡æ•ˆå­—ç¬¦: {char}")
                                board_state.append(int(char))
                        except ValueError as e:
                            logger.warning(f"ç¬¬ {line_idx + 1} è¡Œæ£‹ç›¤ç‹€æ…‹åŒ…å«ç„¡æ•ˆå­—ç¬¦: {e}")
                            skipped_lines += 1
                            continue

                        # è§£æå‹•ä½œåƒ¹å€¼ (é€—è™Ÿå¾Œçš„7å€‹å€¼)
                        action_part = line[comma_idx+1:]
                        action_parts = action_part.split(',')

                        if len(action_parts) != 7:
                            logger.warning(f"ç¬¬ {line_idx + 1} è¡Œå‹•ä½œåƒ¹å€¼æ•¸é‡éŒ¯èª¤ï¼ŒæœŸæœ›7å€‹ï¼Œå¾—åˆ°{len(action_parts)}å€‹")
                            skipped_lines += 1
                            continue

                        # è™•ç†å‹•ä½œåƒ¹å€¼ï¼ˆåŒ…æ‹¬ç©ºå€¼ï¼‰
                        action_vals = []
                        for i, val_str in enumerate(action_parts):
                            val_str = val_str.strip()

                            if val_str == '':
                                # ç©ºå€¼è¡¨ç¤ºè©²åˆ—å·²æ»¿ï¼Œè¨­ç‚ºæ¥µå¤§è² å€¼ï¼ˆä¸å¯ä¸‹ï¼‰
                                action_vals.append(-999.0)
                            else:
                                try:
                                    # å˜—è©¦è½‰æ›ç‚ºæ•¸å­—
                                    val = float(val_str)
                                    action_vals.append(val)
                                except ValueError:
                                    logger.warning(f"ç¬¬ {line_idx + 1} è¡Œåˆ— {i} çš„åƒ¹å€¼ç„¡æ³•è§£æ: '{val_str}'ï¼Œè¨­ç‚º0")
                                    action_vals.append(0.0)

                        # æ•¸æ“šè³ªé‡æª¢æŸ¥
                        valid_actions = [i for i, val in enumerate(action_vals) if val > -900]
                        if len(valid_actions) == 0:
                            logger.warning(f"ç¬¬ {line_idx + 1} è¡Œæ²’æœ‰æœ‰æ•ˆå‹•ä½œï¼Œè·³é")
                            skipped_lines += 1
                            continue

                        # å°‡æ£‹ç›¤ç‹€æ…‹è½‰æ›ç‚ºæˆ‘å€‘çš„ç·¨ç¢¼æ ¼å¼
                        encoded_state = self.agent.encode_state(board_state, 1)

                        states.append(encoded_state)
                        action_values.append(action_vals)

                    except (ValueError, IndexError) as e:
                        logger.warning(f"ç¬¬ {line_idx + 1} è¡Œè§£æéŒ¯èª¤: {e}")
                        skipped_lines += 1
                        continue

            logger.info(f"æ•¸æ“šè¼‰å…¥å®Œæˆ:")
            logger.info(f"  ç¸½è¡Œæ•¸: {len(lines)}")
            logger.info(f"  æˆåŠŸè§£æ: {len(states)} å€‹æ¨£æœ¬")
            logger.info(f"  è·³éè¡Œæ•¸: {skipped_lines}")
            logger.info(f"  æˆåŠŸç‡: {len(states)/(len(lines)-skipped_lines)*100:.1f}%")

            if len(states) == 0:
                logger.error("æ²’æœ‰æˆåŠŸè§£æä»»ä½•æ•¸æ“šæ¨£æœ¬ï¼")
                return None, None

            # è½‰æ›ç‚ºnumpyæ•¸çµ„ä¸¦æ¸…ç†è‡¨æ™‚è®Šé‡ä»¥ç¯€çœè¨˜æ†¶é«”
            states_array = np.array(states, dtype=np.float32)
            action_values_array = np.array(action_values, dtype=np.float32)

            # æ¸…ç†è‡¨æ™‚è®Šé‡
            del states, action_values, lines

            logger.info(f"æ•¸æ“šè¼‰å…¥å®Œæˆï¼Œè¨˜æ†¶é«”ä½¿ç”¨ç‹€æ…‹ï¼š")
            logger.info(f"  ç‹€æ…‹æ•¸çµ„å½¢ç‹€: {states_array.shape}")
            logger.info(f"  å‹•ä½œåƒ¹å€¼æ•¸çµ„å½¢ç‹€: {action_values_array.shape}")

            return states_array, action_values_array

        except FileNotFoundError:
            logger.error(f"æ‰¾ä¸åˆ°æ•¸æ“šé›†æ–‡ä»¶: {file_path}")
            return None, None
        except Exception as e:
            logger.error(f"è¼‰å…¥æ•¸æ“šé›†æ™‚å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            return None, None


    def create_connectx_environment(self):
        """å‰µå»ºConnectXéŠæˆ²ç’°å¢ƒ"""
        try:
            env = make("connectx", debug=False)
            logger.info("âœ… ConnectXç’°å¢ƒå‰µå»ºæˆåŠŸ")
            return env
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºConnectXç’°å¢ƒå¤±æ•—: {e}")
            return None

    def play_game(self, agent1_func, agent2_func, training=True):
        """åŸ·è¡Œä¸€å±€éŠæˆ²"""
        try:
            env = self.create_connectx_environment()
            if env is None:
                return 0, 0

            # é‡ç½®ç’°å¢ƒ
            env.reset()
            config = env.configuration
            
            # éŠæˆ²ç‹€æ…‹
            done = False
            step_count = 0
            max_steps = config.rows * config.columns  # æœ€å¤§æ­¥æ•¸
            
            # å„²å­˜éŠæˆ²æ­·å²
            game_states = []
            game_actions = []
            game_probs = []
            
            while not done and step_count < max_steps:
                # ç²å–ç•¶å‰ç‹€æ…‹
                current_state = env.state
                current_player = step_count % 2  # 0 or 1
                
                # æå–æ£‹ç›¤å’Œç©å®¶æ¨™è¨˜
                board, mark = self.agent.extract_board_and_mark(current_state, current_player)
                
                # ç·¨ç¢¼ç‹€æ…‹
                encoded_state = self.agent.encode_state(board, mark)
                
                # ç²å–æœ‰æ•ˆå‹•ä½œ
                valid_actions = self.agent.get_valid_actions(board)
                
                if not valid_actions:
                    logger.warning("æ²’æœ‰æœ‰æ•ˆå‹•ä½œï¼ŒéŠæˆ²çµæŸ")
                    break
                
                # é¸æ“‡æ™ºèƒ½é«”å‡½æ•¸
                if current_player == 0:
                    agent_func = agent1_func
                else:
                    agent_func = agent2_func
                
                # ç²å–å‹•ä½œ
                try:
                    action, prob, value, is_dangerous = agent_func(encoded_state, valid_actions, training)
                    action = int(action)  # ç¢ºä¿æ˜¯Python int
                    
                    if action not in valid_actions:
                        logger.warning(f"ç„¡æ•ˆå‹•ä½œ {action}ï¼Œé¸æ“‡éš¨æ©Ÿå‹•ä½œ")
                        action = np.random.choice(valid_actions)
                        action = int(action)
                    
                except Exception as e:
                    logger.error(f"æ™ºèƒ½é«”é¸æ“‡å‹•ä½œæ™‚å‡ºéŒ¯: {e}")
                    action = int(np.random.choice(valid_actions))
                    prob = 1.0 / len(valid_actions)
                    value = 0.0
                
                # å„²å­˜è¨“ç·´æ•¸æ“š
                if training and current_player == 0:  # åªå„²å­˜ç©å®¶1çš„æ•¸æ“š
                    game_states.append(encoded_state)
                    game_actions.append(action)
                    game_probs.append(prob)
                
                # åŸ·è¡Œå‹•ä½œ
                try:
                    env.step([action, None] if current_player == 0 else [None, action])
                except Exception as e:
                    logger.error(f"åŸ·è¡Œå‹•ä½œæ™‚å‡ºéŒ¯: {e}")
                    break
                
                # æª¢æŸ¥éŠæˆ²æ˜¯å¦çµæŸ
                if len(env.state) >= 2:
                    status_0 = env.state[0].get('status', 'ACTIVE')
                    status_1 = env.state[1].get('status', 'ACTIVE')
                    
                    if status_0 != 'ACTIVE' or status_1 != 'ACTIVE':
                        done = True
                
                step_count += 1
            
            # è¨ˆç®—çå‹µ
            reward = 0
            if len(env.state) >= 2:
                reward_0 = env.state[0].get('reward', 0)
                reward_1 = env.state[1].get('reward', 0)
                
                if reward_0 > reward_1:
                    reward = 1  # ç©å®¶1ç²å‹
                elif reward_1 > reward_0:
                    reward = -1  # ç©å®¶2ç²å‹
                else:
                    reward = 0  # å¹³å±€
            
            # å„²å­˜éŠæˆ²è½‰æ›
            if training and game_states:
                for i, (state, action, prob) in enumerate(zip(game_states, game_actions, game_probs)):
                    # è¨ˆç®—æŠ˜æ‰£çå‹µ
                    discounted_reward = reward * (self.agent.gamma ** (len(game_states) - i - 1))
                    self.agent.store_transition(state, action, prob, discounted_reward, i == len(game_states) - 1)
            
            return reward, step_count
            
        except Exception as e:
            logger.error(f"éŠæˆ²åŸ·è¡Œå‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            return 0, 0

    def create_agent_function(self, strategy='standard'):
        """å‰µå»ºæ™ºèƒ½é«”å‡½æ•¸"""
        def agent_func(state, valid_actions, training):
            try:
                action, prob, value = self.agent.select_action(state, valid_actions, training)
                return int(action), prob, value, False
            except Exception as e:
                logger.error(f"æ™ºèƒ½é«”é¸æ“‡å‹•ä½œå‡ºéŒ¯: {e}")
                action = int(np.random.choice(valid_actions))
                return action, 1.0/len(valid_actions), 0.0, False
        
        return agent_func

    def random_agent_func(self, state, valid_actions, training=True):
        """éš¨æ©Ÿæ™ºèƒ½é«”"""
        action = int(np.random.choice(valid_actions))
        return action, 1.0/len(valid_actions), 0.0, False

    def supervised_train(self, epochs=100, batch_size=128, max_lines=10000):
        """ä½¿ç”¨ç›£ç£å­¸ç¿’é€²è¡Œè¨“ç·´"""
        logger.info("ğŸš€ é–‹å§‹ç›£ç£å­¸ç¿’è¨“ç·´")
        
        # è¼‰å…¥æ•¸æ“šé›†
        states, action_values = self.load_state_action_dataset(max_lines=max_lines)
        if states is None or action_values is None:
            logger.error("âŒ æ•¸æ“šé›†è¼‰å…¥å¤±æ•—")
            return None
        
        logger.info(f"ğŸ“Š æ•¸æ“šé›†è¼‰å…¥æˆåŠŸ: {len(states)} å€‹æ¨£æœ¬")
        
        # æº–å‚™è¨“ç·´
        self.agent.policy_net.train()
        total_samples = len(states)
        
        # è¨“ç·´å¾ªç’°
        for epoch in range(epochs):
            epoch_start_time = time.time()
            total_loss = 0.0
            total_policy_loss = 0.0
            total_value_loss = 0.0
            num_batches = 0
            
            # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
            indices = np.random.permutation(total_samples)
            
            # æ‰¹æ¬¡è¨“ç·´
            for batch_start in range(0, total_samples, batch_size):
                batch_end = min(batch_start + batch_size, total_samples)
                batch_indices = indices[batch_start:batch_end]
                
                # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
                batch_states = torch.FloatTensor(states[batch_indices]).to(self.agent.device)
                batch_action_values = torch.FloatTensor(action_values[batch_indices]).to(self.agent.device)
                
                # å‰å‘å‚³æ’­
                predicted_probs, predicted_values = self.agent.policy_net(batch_states)
                
                # è¨ˆç®—ç›®æ¨™
                # å°æ–¼å‹•ä½œæ¦‚ç‡ï¼Œä½¿ç”¨softmaxå°‡å‹•ä½œåƒ¹å€¼è½‰æ›ç‚ºæ¦‚ç‡åˆ†ä½ˆ
                # é¦–å…ˆè™•ç†ç„¡æ•ˆå‹•ä½œï¼ˆå€¼ç‚º-999çš„å‹•ä½œï¼‰
                valid_mask = (batch_action_values > -900).float()
                masked_action_values = batch_action_values * valid_mask + (-1000) * (1 - valid_mask)
                
                # Softmaxè½‰æ›ç‚ºç›®æ¨™æ¦‚ç‡
                target_probs = F.softmax(masked_action_values / 0.1, dim=1)  # æº«åº¦åƒæ•¸0.1
                
                # å°æ–¼åƒ¹å€¼ï¼Œä½¿ç”¨æœ€å¤§å‹•ä½œåƒ¹å€¼ä½œç‚ºç›®æ¨™
                target_values = torch.max(batch_action_values * valid_mask + (-1000) * (1 - valid_mask), dim=1)[0].unsqueeze(1)
                # æ­£è¦åŒ–åƒ¹å€¼åˆ°[-1, 1]ç¯„åœ
                target_values = torch.tanh(target_values / 10.0)
                
                # è¨ˆç®—æå¤±
                policy_loss = F.kl_div(torch.log(predicted_probs + 1e-8), target_probs, reduction='batchmean')
                value_loss = F.mse_loss(predicted_values, target_values)
                
                total_loss_batch = policy_loss + 0.5 * value_loss
                
                # åå‘å‚³æ’­
                self.agent.optimizer.zero_grad()
                total_loss_batch.backward()
                torch.nn.utils.clip_grad_norm_(self.agent.policy_net.parameters(), 0.5)
                self.agent.optimizer.step()
                
                # ç´¯ç©æå¤±
                total_loss += total_loss_batch.item()
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                num_batches += 1
            
            # è¨ˆç®—å¹³å‡æå¤±
            avg_loss = total_loss / num_batches if num_batches > 0 else 0
            avg_policy_loss = total_policy_loss / num_batches if num_batches > 0 else 0
            avg_value_loss = total_value_loss / num_batches if num_batches > 0 else 0
            
            epoch_time = time.time() - epoch_start_time
            
            # æ¯10å€‹epochå ±å‘Šä¸€æ¬¡
            if (epoch + 1) % 10 == 0 or epoch == 0:
                logger.info(f"Epoch [{epoch+1}/{epochs}] "
                          f"Loss: {avg_loss:.6f} "
                          f"Policy: {avg_policy_loss:.6f} "
                          f"Value: {avg_value_loss:.6f} "
                          f"Time: {epoch_time:.2f}s")
            
            # å­¸ç¿’ç‡èª¿åº¦
            self.agent.scheduler.step(avg_loss)
            
            # ä¿å­˜æª¢æŸ¥é»
            if (epoch + 1) % 1000 == 0 and epoch > 2000:
                checkpoint_name = f"supervised_epoch_{epoch+1}.pt"
                self.save_checkpoint(checkpoint_name)
        
        logger.info("âœ… ç›£ç£å­¸ç¿’è¨“ç·´å®Œæˆ")
        return self.agent

    def evaluate_agent(self, num_games=100):
        """è©•ä¼°æ™ºèƒ½é«”æ€§èƒ½"""
        logger.info(f"ğŸ¯ é–‹å§‹è©•ä¼°æ™ºèƒ½é«”æ€§èƒ½ ({num_games} å±€éŠæˆ²)")
        
        wins = 0
        draws = 0
        losses = 0
        
        agent_func = self.create_agent_function()
        
        for i in range(num_games):
            try:
                # èˆ‡éš¨æ©Ÿå°æ‰‹å°å¼ˆ
                reward, steps = self.play_game(agent_func, self.random_agent_func, training=False)
                
                if reward > 0:
                    wins += 1
                elif reward == 0:
                    draws += 1
                else:
                    losses += 1
                
                if (i + 1) % 20 == 0:
                    current_wr = wins / (i + 1) * 100
                    logger.info(f"è©•ä¼°é€²åº¦: {i+1}/{num_games}, ç•¶å‰å‹ç‡: {current_wr:.1f}%")
                    
            except Exception as e:
                logger.error(f"è©•ä¼°ç¬¬ {i+1} å±€æ™‚å‡ºéŒ¯: {e}")
                losses += 1
        
        win_rate = wins / num_games * 100
        logger.info(f"ğŸ“Š è©•ä¼°çµæœ:")
        logger.info(f"   å‹åˆ©: {wins} ({win_rate:.1f}%)")
        logger.info(f"   å¹³å±€: {draws} ({draws/num_games*100:.1f}%)")
        logger.info(f"   å¤±æ•—: {losses} ({losses/num_games*100:.1f}%)")
        
        return win_rate
    def self_play_episode(self):
        """è‡ªå°å¼ˆå›åˆï¼ˆå¸¶éš¨æ©Ÿæ€§å¢å¼·å¤šæ¨£æ€§ï¼‰"""
        # éš¨æ©Ÿé¸æ“‡è‡ªå°å¼ˆç­–ç•¥ä»¥å¢åŠ å¤šæ¨£æ€§
        strategy = np.random.choice(['standard', 'noisy', 'exploration', 'temperature'], p=[0.4, 0.2, 0.2, 0.2])

        def agent_func(state, valid_actions, training):
            # ç²å–åŸå§‹å‹•ä½œ
            action, prob, value = self.agent.select_action(state, valid_actions, training)

            if training:
                # æ ¹æ“šç­–ç•¥æ·»åŠ ä¸åŒé¡å‹çš„éš¨æ©Ÿæ€§
                if strategy == 'noisy':
                    # å™ªéŸ³ç­–ç•¥ï¼šæœ‰ä¸€å®šæ¦‚ç‡é¸æ“‡éš¨æ©Ÿå‹•ä½œ
                    if np.random.random() < 0.15:  # 15%æ¦‚ç‡é¸æ“‡éš¨æ©Ÿå‹•ä½œ
                        action = int(np.random.choice(valid_actions))  # ç¢ºä¿è¿”å› Python int
                        logger.debug(f"è‡ªå°å¼ˆä½¿ç”¨å™ªéŸ³ç­–ç•¥ï¼Œéš¨æ©Ÿé¸æ“‡å‹•ä½œ: {action}")

                elif strategy == 'exploration':
                    # æ¢ç´¢ç­–ç•¥ï¼šåŸºæ–¼å‹•ä½œæ¦‚ç‡çš„æ¢ç´¢æ€§æ¡æ¨£
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.agent.device)
                    self.agent.policy_net.eval()  # é¿å… BatchNorm å•é¡Œ
                    with torch.no_grad():
                        action_probs, _ = self.agent.policy_net(state_tensor)

                    # é®ç½©ç„¡æ•ˆå‹•ä½œ
                    action_probs = action_probs.cpu().numpy()[0]
                    masked_probs = np.zeros_like(action_probs)
                    masked_probs[valid_actions] = action_probs[valid_actions]

                    if masked_probs.sum() > 0:
                        masked_probs /= masked_probs.sum()
                        # ä½¿ç”¨æº«åº¦æ¡æ¨£å¢åŠ æ¢ç´¢
                        temperature = 1.5
                        temp_probs = np.power(masked_probs, 1/temperature)
                        temp_probs[valid_actions] /= temp_probs[valid_actions].sum()
                        action = int(np.random.choice(7, p=temp_probs))  # ç¢ºä¿è¿”å› Python int
                        logger.debug(f"è‡ªå°å¼ˆä½¿ç”¨æ¢ç´¢ç­–ç•¥ï¼Œæº«åº¦æ¡æ¨£å‹•ä½œ: {action}")

                elif strategy == 'temperature':
                    # æº«åº¦ç­–ç•¥ï¼šéš¨æ©Ÿèª¿æ•´æ±ºç­–æº«åº¦
                    temperature = np.random.uniform(0.8, 2.0)
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.agent.device)
                    self.agent.policy_net.eval()  # é¿å… BatchNorm å•é¡Œ
                    with torch.no_grad():
                        action_probs, _ = self.agent.policy_net(state_tensor)

                    action_probs = action_probs.cpu().numpy()[0]
                    masked_probs = np.zeros_like(action_probs)
                    masked_probs[valid_actions] = action_probs[valid_actions]

                    if masked_probs.sum() > 0:
                        # æ‡‰ç”¨æº«åº¦
                        temp_probs = np.power(masked_probs, 1/temperature)
                        temp_probs /= temp_probs.sum()
                        action = int(valid_actions[np.argmax(temp_probs[valid_actions])])  # ç¢ºä¿è¿”å› Python int
                        logger.debug(f"è‡ªå°å¼ˆä½¿ç”¨æº«åº¦ç­–ç•¥ (T={temperature:.2f})ï¼Œå‹•ä½œ: {action}")

                # æª¢æŸ¥å±éšªå‹•ä½œ
                board = state[:42].reshape(6, 7).astype(int)
                mark = 1  # åœ¨è‡ªå°å¼ˆä¸­ï¼Œç•¶å‰ç©å®¶ç¸½æ˜¯1

                if self.is_dangerous_move(board, mark, action, look_ahead_steps=3):
                    logger.debug(f"è‡ªå°å¼ˆä¸­agenté¸æ“‡äº†å±éšªå‹•ä½œ {action} (ç­–ç•¥: {strategy})")
                    return int(action), prob, value, True  # ç¢ºä¿è¿”å› Python int

            return int(action), prob, value, False  # ç¢ºä¿è¿”å› Python int

        reward, episode_length = self.play_game(agent_func, agent_func, training=True)
        logger.debug(f"è‡ªå°å¼ˆå®Œæˆï¼Œç­–ç•¥: {strategy}, å›åˆé•·åº¦: {episode_length}")
        return reward, episode_length

    def adaptive_self_play_episode(self, episode_num):
        """è‡ªé©æ‡‰è‡ªå°å¼ˆå›åˆï¼ˆæ ¹æ“šè¨“ç·´é€²åº¦èª¿æ•´å¤šæ¨£æ€§ï¼‰"""
        # æ ¹æ“šè¨“ç·´é€²åº¦å‹•æ…‹èª¿æ•´åƒæ•¸
        progress = min(episode_num / 5000, 1.0)  # 5000å›åˆé”åˆ°ç©©å®š

        # å¤šæ¨£æ€§ç­–ç•¥é¸æ“‡æ¦‚ç‡ï¼ˆéš¨è¨“ç·´é€²åº¦è®ŠåŒ–ï¼‰
        if progress < 0.3:  # æ—©æœŸéšæ®µï¼šé«˜æ¢ç´¢
            strategy_probs = [0.2, 0.4, 0.3, 0.1]  # [standard, noisy, exploration, temperature]
        elif progress < 0.7:  # ä¸­æœŸéšæ®µï¼šå¹³è¡¡æ¢ç´¢
            strategy_probs = [0.4, 0.3, 0.2, 0.1]
        else:  # å¾ŒæœŸéšæ®µï¼šæ›´å¤šæ¨™æº–ç©æ³•
            strategy_probs = [0.6, 0.2, 0.15, 0.05]

        strategy = np.random.choice(['standard', 'noisy', 'exploration', 'temperature'], p=strategy_probs)

        def adaptive_agent_func(state, valid_actions, training):
            action, prob, value = self.agent.select_action(state, valid_actions, training)

            if training:
                if strategy == 'noisy':
                    noise_level = 0.2 * (1 - progress) + 0.05 * progress  # éš¨é€²åº¦é™ä½å™ªéŸ³
                    if np.random.random() < noise_level:
                        action = int(np.random.choice(valid_actions))  # ç¢ºä¿è¿”å› Python int

                elif strategy == 'exploration':
                    temperature = 1.8 - 0.6 * progress  # æº«åº¦éš¨é€²åº¦é™ä½
                    exploration_bonus = 0.15 * (1 - progress)  # æ¢ç´¢çå‹µéš¨é€²åº¦é™ä½
                    action, prob, value = self.agent.select_action(
                        state, valid_actions, training, temperature=temperature, exploration_bonus=exploration_bonus)

                elif strategy == 'temperature':
                    # æº«åº¦ç¯„åœéš¨é€²åº¦æ”¶çª„
                    temp_range = (0.8 + 0.4 * progress, 2.0 - 0.5 * progress)
                    temperature = np.random.uniform(*temp_range)
                    action, prob, value = self.agent.select_action(
                        state, valid_actions, training, temperature=temperature)

                # æª¢æŸ¥å±éšªå‹•ä½œ
                board = state[:42].reshape(6, 7).astype(int)
                mark = 1

                if self.is_dangerous_move(board, mark, action, look_ahead_steps=3):
                    return int(action), prob, value, True  # ç¢ºä¿è¿”å› Python int

            return int(action), prob, value, False  # ç¢ºä¿è¿”å› Python int

        reward, episode_length = self.play_game(adaptive_agent_func, adaptive_agent_func, training=True)
        return reward, episode_length

    def diverse_training_episode(self, episode_num):
        """å¤šæ¨£æ€§è¨“ç·´å›åˆ - å°æŠ—ä¸åŒå¼·åº¦çš„å°æ‰‹"""
        # æ ¹æ“šè¨“ç·´é€²åº¦é¸æ“‡å°æ‰‹
        if episode_num < 2000:
            # æ—©æœŸï¼šä¸»è¦è‡ªå°å¼ˆï¼ˆé«˜éš¨æ©Ÿæ€§ï¼‰
            if episode_num % 30 == 0:
                # æ¯10å›åˆå°æŠ—éš¨æ©Ÿå°æ‰‹ï¼ˆä¿æŒå°å¼±å°æ‰‹çš„çµ±æ²»åŠ›ï¼‰
                return self.play_against_random_agent()
            else:
                return self.play_against_minimax_agent()
            # æ¯8å›åˆå°æŠ—éš¨æ©Ÿå°æ‰‹ï¼ˆä¿æŒå°å¼±å°æ‰‹çš„çµ±æ²»åŠ›ï¼‰
        else:
            # å…¶ä»–æ™‚å€™æ¨™æº–è‡ªå°å¼ˆ
            if episode_num % 3 == 0:
                return self.self_play_episode()
            if episode_num % 3 == 1:
                return self.adaptive_self_play_episode(episode_num)
            if episode_num % 3 == 2:
                return self.diverse_self_play_episode(episode_num)

    def diverse_self_play_episode(self, episode_num):
        """é«˜å¤šæ¨£æ€§è‡ªå°å¼ˆå›åˆ"""
        # æ ¹æ“šè¨“ç·´é€²åº¦èª¿æ•´éš¨æ©Ÿæ€§å¼·åº¦
        progress = min(episode_num / 10000, 1.0)  # 10000å›åˆå¾Œé”åˆ°æœ€ä½éš¨æ©Ÿæ€§
        base_randomness = 0.3 * (1 - progress) + 0.1 * progress  # å¾30%é™åˆ°10%

        def diverse_agent_func(state, valid_actions, training):
            # ç²å–åŸå§‹å‹•ä½œ
            action, prob, value = self.agent.select_action(state, valid_actions, training)

            if training:
                # å‹•æ…‹èª¿æ•´çš„å¤šæ¨£æ€§ç­–ç•¥
                randomness_level = base_randomness + np.random.uniform(-0.05, 0.05)

                if np.random.random() < randomness_level:
                    # å¤šç¨®éš¨æ©ŸåŒ–æ–¹å¼
                    rand_type = np.random.choice(['pure_random', 'weighted_random', 'anti_pattern'])

                    if rand_type == 'pure_random':
                        # ç´”éš¨æ©Ÿé¸æ“‡
                        action = int(np.random.choice(valid_actions))  # ç¢ºä¿è¿”å› Python int
                        logger.debug(f"å¤šæ¨£æ€§è‡ªå°å¼ˆï¼šç´”éš¨æ©Ÿå‹•ä½œ {action}")

                    elif rand_type == 'weighted_random':
                        # å¸¶æ¬Šé‡çš„éš¨æ©Ÿé¸æ“‡ï¼ˆåå¥½ä¸­å¤®åˆ—ï¼‰
                        weights = np.array([1, 2, 3, 4, 3, 2, 1])  # ä¸­å¤®æ¬Šé‡æ›´é«˜
                        valid_weights = weights[valid_actions]
                        valid_weights = valid_weights / valid_weights.sum()
                        action = int(np.random.choice(valid_actions, p=valid_weights))  # ç¢ºä¿è¿”å› Python int
                        logger.debug(f"å¤šæ¨£æ€§è‡ªå°å¼ˆï¼šåŠ æ¬Šéš¨æ©Ÿå‹•ä½œ {action}")

                    elif rand_type == 'anti_pattern':
                        # åæ¨¡å¼é¸æ“‡ï¼šé¸æ“‡æ¨¡å‹èªç‚ºä¸å¤ªå¥½çš„å‹•ä½œ
                        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.agent.device)
                        self.agent.policy_net.eval()  # é¿å… BatchNorm å•é¡Œ
                        with torch.no_grad():
                            action_probs, _ = self.agent.policy_net(state_tensor)

                        action_probs = action_probs.cpu().numpy()[0]
                        masked_probs = np.zeros_like(action_probs)
                        masked_probs[valid_actions] = action_probs[valid_actions]

                        if masked_probs.sum() > 0:
                            # åè½‰æ¦‚ç‡ï¼ˆé¸æ“‡æ¨¡å‹ä¸å–œæ­¡çš„å‹•ä½œï¼‰
                            inv_probs = 1.0 - masked_probs
                            inv_probs[valid_actions] /= inv_probs[valid_actions].sum()
                            action = int(np.random.choice(valid_actions, p=inv_probs[valid_actions]))  # ç¢ºä¿è¿”å› Python int
                            logger.debug(f"å¤šæ¨£æ€§è‡ªå°å¼ˆï¼šåæ¨¡å¼å‹•ä½œ {action}")

                # æª¢æŸ¥å±éšªå‹•ä½œ
                board = state[:42].reshape(6, 7).astype(int)
                mark = 1

                if self.is_dangerous_move(board, mark, action, look_ahead_steps=3):
                    logger.debug(f"å¤šæ¨£æ€§è‡ªå°å¼ˆä¸­é¸æ“‡äº†å±éšªå‹•ä½œ {action}")
                    return int(action), prob, value, True  # ç¢ºä¿è¿”å› Python int

            return int(action), prob, value, False  # ç¢ºä¿è¿”å› Python int

        reward, episode_length = self.play_game(diverse_agent_func, diverse_agent_func, training=True)
        logger.debug(f"é«˜å¤šæ¨£æ€§è‡ªå°å¼ˆå®Œæˆï¼Œéš¨æ©Ÿæ€§: {base_randomness:.2f}, å›åˆé•·åº¦: {episode_length}")
        return reward, episode_length

    def check_win_move(self, board, mark, col):
        """æª¢æŸ¥åœ¨æŒ‡å®šåˆ—æ”¾ç½®æ£‹å­å¾Œæ˜¯å¦èƒ½ç²å‹"""
        # æ¨¡æ“¬æ”¾ç½®æ£‹å­
        test_board = board.copy()
        row = -1
        for r in range(5, -1, -1):  # å¾ä¸‹å¾€ä¸Šæ‰¾ç©ºä½
            if test_board[r][col] == 0:
                test_board[r][col] = mark
                row = r
                break

        if row == -1:  # è©²åˆ—å·²æ»¿
            return False

        # æª¢æŸ¥å››å€‹æ–¹å‘æ˜¯å¦é€£æˆå››å­
        directions = [
            (0, 1),   # æ°´å¹³
            (1, 0),   # å‚ç›´
            (1, 1),   # ä¸»å°è§’ç·š
            (1, -1)   # åå°è§’ç·š
        ]

        for dr, dc in directions:
            count = 1

            # æ­£æ–¹å‘æª¢æŸ¥
            r, c = row + dr, col + dc
            while 0 <= r < 6 and 0 <= c < 7 and test_board[r][c] == mark:
                count += 1
                r, c = r + dr, c + dc

            # åæ–¹å‘æª¢æŸ¥
            r, c = row - dr, col - dc
            while 0 <= r < 6 and 0 <= c < 7 and test_board[r][c] == mark:
                count += 1
                r, c = r - dr, c - dc

            if count >= 4:
                return True

        return False

    def if_i_can_finish(self, board, mark, valid_actions):
        """æª¢æŸ¥æ˜¯å¦æœ‰ç›´æ¥ç²å‹çš„å‹•ä½œ"""
        for col in valid_actions:
            if self.check_win_move(board, mark, col):
                return col
        return -1

    def if_i_will_lose(self, board, mark, valid_actions):
        """æª¢æŸ¥å°æ‰‹æ˜¯å¦èƒ½åœ¨ä¸‹ä¸€æ­¥ç²å‹ï¼Œå¦‚æœæ˜¯å‰‡è¿”å›é˜»æ“‹çš„å‹•ä½œ"""
        opponent_mark = 3 - mark  # å°æ‰‹æ¨™è¨˜
        for col in valid_actions:
            if self.check_win_move(board, opponent_mark, col):
                return col  # è¿”å›éœ€è¦é˜»æ“‹çš„åˆ—
        return -1

    def _check_win(self, board, last_col):
        """æª¢æŸ¥æ˜¯å¦ç²å‹"""
        if last_col is None:
            return False

        last_row = self._get_drop_row(board, last_col)
        if last_row is None:
            last_row = 0
        else:
            last_row += 1

        if last_row >= 6:
            return False

        player = board[last_row][last_col]
        if player == 0:
            return False

        # æª¢æŸ¥å››å€‹æ–¹å‘
        directions = [(0,1), (1,0), (1,1), (1,-1)]

        for dr, dc in directions:
            count = 1
            # æ­£æ–¹å‘
            r, c = last_row + dr, last_col + dc
            while 0 <= r < 6 and 0 <= c < 7 and board[r][c] == player:
                count += 1
                r, c = r + dr, c + dc
            # è² æ–¹å‘
            r, c = last_row - dr, last_col - dc
            while 0 <= r < 6 and 0 <= c < 7 and board[r][c] == player:
                count += 1
                r, c = r - dr, c - dc

            if count >= 4:
                return True

        return False
    def save_checkpoint(self, filename):
        """ä¿å­˜æª¢æŸ¥é»"""
        try:
            # ç¢ºä¿checkpointsç›®éŒ„å­˜åœ¨
            os.makedirs("checkpoints", exist_ok=True)

            checkpoint = {
                'model_state_dict': self.agent.policy_net.state_dict(),
                'optimizer_state_dict': self.agent.optimizer.state_dict(),
                'episode_rewards': self.episode_rewards,
                'win_rates': self.win_rates,
                'config': self.config,
                # æ·»åŠ æ¨¡å‹çµæ§‹ä¿¡æ¯ä»¥ä¾¿è¼‰å…¥æ™‚é©—è­‰
                'model_architecture': {
                    'input_size': self.config['agent']['input_size'],
                    'hidden_size': self.config['agent']['hidden_size'],
                    'num_layers': self.config['agent']['num_layers']
                },
                # æ·»åŠ ä¿å­˜æ™‚é–“æˆ³
                'save_timestamp': datetime.now().isoformat(),
                # ä½¿ç”¨å­—ç¬¦ä¸²è€Œä¸æ˜¯TorchVersionå°è±¡ä»¥é¿å…åºåˆ—åŒ–å•é¡Œ
                'pytorch_version': str(torch.__version__)
            }

            checkpoint_path = f"checkpoints/{filename}"
            torch.save(checkpoint, checkpoint_path)

            # è¨ˆç®—æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB

            logger.info(f"âœ… å·²ä¿å­˜æª¢æŸ¥é»: {filename}")
            logger.info(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
            logger.info(f"   æ¨¡å‹çµæ§‹: {self.config['agent']['num_layers']} å±¤, {self.config['agent']['hidden_size']} éš±è—å–®å…ƒ")

        except Exception as e:
            logger.error(f"ä¿å­˜æª¢æŸ¥é»æ™‚å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()

    def load_checkpoint(self, checkpoint_path):
        """è¼‰å…¥æª¢æŸ¥é»ä¸¦æ¢å¾©è¨“ç·´ç‹€æ…‹"""
        try:
            if not os.path.exists(checkpoint_path):
                logger.error(f"æª¢æŸ¥é»æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
                return False

            logger.info(f"è¼‰å…¥æª¢æŸ¥é»: {checkpoint_path}")

            # ç‚ºäº†å…¼å®¹æ€§ï¼Œå…ˆå˜—è©¦ä½¿ç”¨ weights_only=False
            try:
                checkpoint = torch.load(checkpoint_path, map_location=self.agent.device, weights_only=False)
            except Exception as e1:
                logger.warning(f"ä½¿ç”¨ weights_only=False è¼‰å…¥å¤±æ•—: {e1}")
                # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨å®‰å…¨çš„å…¨å±€å°è±¡
                try:
                    with torch.serialization.safe_globals([torch.torch_version.TorchVersion]):
                        checkpoint = torch.load(checkpoint_path, map_location=self.agent.device, weights_only=True)
                except Exception as e2:
                    logger.error(f"ä½¿ç”¨å®‰å…¨å…¨å±€å°è±¡è¼‰å…¥ä¹Ÿå¤±æ•—: {e2}")
                    # æœ€å¾Œå˜—è©¦åªè¼‰å…¥æ¨¡å‹æ¬Šé‡
                    try:
                        checkpoint = torch.load(checkpoint_path, map_location=self.agent.device, weights_only=True)
                        # å¦‚æœæˆåŠŸä½†æ˜¯æ ¼å¼ä¸å°ï¼Œå˜—è©¦åŒ…è£æˆæ­£ç¢ºæ ¼å¼
                        if not isinstance(checkpoint, dict) or 'model_state_dict' not in checkpoint:
                            # å‡è¨­ç›´æ¥è¼‰å…¥çš„æ˜¯state_dict
                            checkpoint = {'model_state_dict': checkpoint}
                            logger.warning("æª¢æŸ¥é»æ ¼å¼ä¸æ¨™æº–ï¼Œå˜—è©¦åŒ…è£ç‚ºæ¨™æº–æ ¼å¼")
                    except Exception as e3:
                        logger.error(f"æ‰€æœ‰è¼‰å…¥æ–¹æ³•éƒ½å¤±æ•—: {e3}")
                        return False

            # æª¢æŸ¥æ¨¡å‹çµæ§‹å…¼å®¹æ€§
            saved_state_dict = checkpoint['model_state_dict']
            current_state_dict = self.agent.policy_net.state_dict()

            # æª¢æŸ¥é—œéµåƒæ•¸åŒ¹é…
            if 'config' in checkpoint:
                saved_config = checkpoint['config']
                current_agent_config = self.config['agent']
                saved_agent_config = saved_config.get('agent', {})

                # æª¢æŸ¥é—œéµåƒæ•¸æ˜¯å¦åŒ¹é…
                key_params = ['input_size', 'hidden_size', 'num_layers']
                mismatch_found = False
                for param in key_params:
                    if (param in current_agent_config and param in saved_agent_config):
                        if current_agent_config[param] != saved_agent_config[param]:
                            logger.error(f"æ¨¡å‹çµæ§‹ä¸åŒ¹é… {param}: ç•¶å‰={current_agent_config[param]}, ä¿å­˜çš„={saved_agent_config[param]}")
                            mismatch_found = True

                if mismatch_found:
                    logger.error("æ¨¡å‹çµæ§‹ä¸åŒ¹é…ï¼Œç„¡æ³•è¼‰å…¥æª¢æŸ¥é»")
                    return False

            # æª¢æŸ¥state_dictéµæ˜¯å¦åŒ¹é…
            saved_keys = set(saved_state_dict.keys())
            current_keys = set(current_state_dict.keys())

            missing_keys = current_keys - saved_keys
            unexpected_keys = saved_keys - current_keys

            if missing_keys:
                logger.warning(f"ç•¶å‰æ¨¡å‹ç¼ºå°‘çš„æ¬Šé‡éµ: {list(missing_keys)[:10]}...")  # åªé¡¯ç¤ºå‰10å€‹
            if unexpected_keys:
                logger.warning(f"ä¿å­˜æ¨¡å‹ä¸­å¤šé¤˜çš„æ¬Šé‡éµ: {list(unexpected_keys)[:10]}...")  # åªé¡¯ç¤ºå‰10å€‹

            # å¦‚æœéµä¸åŒ¹é…å¤ªå¤šï¼Œæ‹’çµ•è¼‰å…¥
            if len(missing_keys) > 0 or len(unexpected_keys) > 10:  # å…è¨±å°‘é‡ä¸åŒ¹é…
                logger.error("æ¨¡å‹æ¬Šé‡éµåš´é‡ä¸åŒ¹é…ï¼Œæ‹’çµ•è¼‰å…¥")
                logger.info("æç¤ºï¼šå¯èƒ½éœ€è¦ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é…ç½®ï¼Œæˆ–å¾é ­é–‹å§‹è¨“ç·´")
                return False

            # è¼‰å…¥æ¨¡å‹ç‹€æ…‹ï¼ˆstrict=Falseå…è¨±éƒ¨åˆ†åŒ¹é…ï¼‰
            missing, unexpected = self.agent.policy_net.load_state_dict(saved_state_dict, strict=False)
            if missing:
                logger.warning(f"è¼‰å…¥æ™‚ç¼ºå°‘çš„éµ: {missing[:5]}...")
            if unexpected:
                logger.warning(f"è¼‰å…¥æ™‚æœªä½¿ç”¨çš„éµ: {unexpected[:5]}...")

            logger.info("âœ… æ¨¡å‹æ¬Šé‡è¼‰å…¥æˆåŠŸ")

            # è¼‰å…¥å„ªåŒ–å™¨ç‹€æ…‹ï¼ˆå¯é¸ï¼Œå› ç‚ºçµæ§‹å¯èƒ½ä¸åŒ¹é…ï¼‰
            if 'optimizer_state_dict' in checkpoint:
                try:
                    self.agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    logger.info("âœ… å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥å¤±æ•—ï¼ˆå°‡ä½¿ç”¨æ–°çš„å„ªåŒ–å™¨ç‹€æ…‹ï¼‰: {e}")

            # è¼‰å…¥è¨“ç·´æ­·å²ï¼ˆå¯é¸ï¼‰
            if 'episode_rewards' in checkpoint:
                self.episode_rewards = checkpoint['episode_rewards']
                logger.info(f"âœ… è¼‰å…¥è¨“ç·´æ­·å²: {len(self.episode_rewards)} å€‹å›åˆ")

            if 'win_rates' in checkpoint:
                self.win_rates = checkpoint['win_rates']
                logger.info(f"âœ… è¼‰å…¥å‹ç‡æ­·å²: {len(self.win_rates)} å€‹è©•ä¼°é»")

            logger.info("ğŸ‰ æª¢æŸ¥é»è¼‰å…¥å®Œæˆï¼")
            return True

        except Exception as e:
            logger.error(f"è¼‰å…¥æª¢æŸ¥é»æ™‚å‡ºéŒ¯: {e}")
            logger.error("å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
            logger.error("1. æª¢æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦èˆ‡ä¿å­˜æ™‚ä¸€è‡´")
            logger.error("2. ç¢ºèªæª¢æŸ¥é»æ–‡ä»¶æœªæå£")
            logger.error("3. è€ƒæ…®å¾é ­é–‹å§‹è¨“ç·´")
            import traceback
            traceback.print_exc()
            return False

# é¦–å…ˆä¿®å¤é…ç½®æ–‡ä»¶çš„åŠ è½½é—®é¢˜
def create_training_config():
    """åˆ›å»ºè®­ç»ƒé…ç½®"""
    config = {
        'agent': {
            'input_size': 126,      # 3ä¸ªé€šé“ Ã— 42ä¸ªä½ç½® = 126
            'hidden_size': 150,     # éšè—å±‚å¤§å°
            'num_layers': 300,        # éšè—å±‚æ•°é‡ (æ”¹ä¸ºåˆç†æ•°é‡)
            'learning_rate': 0.001, # å­¦ä¹ ç‡
            'weight_decay': 0.0001, # æƒé‡è¡°å‡
            'gamma': 0.99,          # æŠ˜æ‰£å› å­
            'eps_clip': 0.2,        # PPOå‰ªè£å‚æ•°
            'k_epochs': 4,          # PPOæ›´æ–°æ¬¡æ•°
            'entropy_coef': 0.01,   # ç†µç³»æ•°
            'value_coef': 0.5,      # ä»·å€¼ç³»æ•°
            'gae_lambda': 0.99,     # GAEå‚æ•°
            'buffer_size': 10000,   # ç»éªŒç¼“å†²åŒºå¤§å°
            'min_batch_size': 64    # æœ€å°æ‰¹æ¬¡å¤§å°
        },
        'training': {
            'supervised_epochs': 200,     # ç›‘ç£å­¦ä¹ epochs
            'batch_size': 128,           # æ‰¹æ¬¡å¤§å°
            'max_dataset_lines': -1,  # æœ€å¤§æ•°æ®é›†è¡Œæ•°
            'evaluation_games': 100,     # è¯„ä¼°æ¸¸æˆæ•°é‡
            'checkpoint_frequency': 50,  # æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡
            'log_frequency': 100         # æ—¥å¿—è®°å½•é¢‘ç‡
        }
    }
    return config
def save_config(config, filename='config_supervised.yaml'):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    logger.info(f"âœ… é…ç½®å·²ä¿å­˜åˆ° {filename}")
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸ® ConnectX ç›‘ç£å­¦ä¹ è®­ç»ƒ")
    print("=" * 50)
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    # åˆ›å»ºå¹¶ä½¿ç”¨é…ç½®å­—å…¸è€Œä¸æ˜¯æ–‡ä»¶
    config = create_training_config()
    
    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
    dataset_file = "connectx-state-action-value.txt"
    if not os.path.exists(dataset_file):
        logger.error(f"âŒ æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶: {dataset_file}")
        logger.error("è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•ä¸­")
        return
    
    logger.info(f"âœ… æ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶: {dataset_file}")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨ï¼Œç›´æ¥ä¼ å…¥é…ç½®å­—å…¸
        trainer = ConnectXTrainer(config)
        logger.info("âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
        
        # æ˜¾ç¤ºè®­ç»ƒé…ç½®
        print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
        print(f"   ç½‘ç»œç»“æ„: {config['agent']['hidden_size']} éšè—å•å…ƒ, {config['agent']['num_layers']} å±‚")
        print(f"   å­¦ä¹ ç‡: {config['agent']['learning_rate']}")
        print(f"   ç›‘ç£å­¦ä¹ epochs: {config['training']['supervised_epochs']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {config['training']['batch_size']}")
        print(f"   æœ€å¤§æ•°æ®é›†è¡Œæ•°: {config['training']['max_dataset_lines']}")
        
        # å¼€å§‹ç›‘ç£å­¦ä¹ è®­ç»ƒ
        print("\nğŸš€ å¼€å§‹ç›‘ç£å­¦ä¹ è®­ç»ƒ...")
        start_time = time.time()
        
        trained_agent = trainer.supervised_train(
            epochs=config['training']['supervised_epochs'],
            batch_size=config['training']['batch_size'],
            max_lines=config['training']['max_dataset_lines']
        )
        
        training_time = time.time() - start_time
        
        if trained_agent is not None:
            logger.info(f"âœ… ç›‘ç£å­¦ä¹ è®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {training_time:.1f}ç§’")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_checkpoint = f"supervised_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt"
            trainer.save_checkpoint(final_checkpoint)
            logger.info(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: checkpoints/{final_checkpoint}")
            
            # è¯„ä¼°æ¨¡å‹æ€§èƒ½
            print("\nğŸ¯ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            win_rate = trainer.evaluate_agent(num_games=config['training']['evaluation_games'])
            
            print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
            print(f"   æ€»ç”¨æ—¶: {training_time:.1f}ç§’ ({training_time/60:.1f}åˆ†é’Ÿ)")
            print(f"   æœ€ç»ˆèƒœç‡: {win_rate:.1f}%")
            print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: checkpoints/{final_checkpoint}")
            
            # æä¾›ä½¿ç”¨å»ºè®®
            print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
            if win_rate >= 80:
                print("   ğŸŒŸ æ¨¡å‹æ€§èƒ½ä¼˜å¼‚ï¼å¯ä»¥ç›´æ¥ç”¨äºæ¯”èµ›")
            elif win_rate >= 60:
                print("   ğŸ‘ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå»ºè®®è¿›è¡Œæ›´å¤šè®­ç»ƒæˆ–å¾®è°ƒ")
            else:
                print("   âš ï¸ æ¨¡å‹æ€§èƒ½éœ€è¦æ”¹è¿›ï¼Œå»ºè®®:")
                print("      - å¢åŠ è®­ç»ƒepochs")
                print("      - è°ƒæ•´å­¦ä¹ ç‡")
                print("      - å¢åŠ æ•°æ®é›†å¤§å°")
                print("      - æ£€æŸ¥ç½‘ç»œç»“æ„")
        
        else:
            logger.error("âŒ ç›‘ç£å­¦ä¹ è®­ç»ƒå¤±è´¥")
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
if __name__ == "__main__":
    main()
