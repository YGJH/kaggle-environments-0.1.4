#!/usr/bin/env python3

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import math
import re
import glob
import shutil
from collections import deque
import logging
from datetime import datetime
import argparse
import traceback
import multiprocessing as mp  # NEW: multiprocessing for parallel rollouts

try:
    import yaml
except ImportError:
    print("PyYAML not found. Installing...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "PyYAML"])
    import yaml

try:
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    from matplotlib.animation import FuncAnimation
    import matplotlib.colors as mcolors
    
    # é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans', 'Bitstream Vera Sans']
    plt.rcParams['axes.unicode_minus'] = False
    # è®¾ç½®å­—ä½“å¤§å°ï¼Œé¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
    plt.rcParams['font.size'] = 10
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['xtick.labelsize'] = 9
    plt.rcParams['ytick.labelsize'] = 9
    
    VISUALIZATION_AVAILABLE = True
except ImportError:
    print("Matplotlib not found. Installing for visualization...")
    try:
        import subprocess
        subprocess.check_call(["uv", "add","matplotlib"])
        import matplotlib.pyplot as plt
        import matplotlib.patches as patches
        from matplotlib.animation import FuncAnimation
        import matplotlib.colors as mcolors
        VISUALIZATION_AVAILABLE = True
    except Exception:
        print("âš ï¸ ç„¡æ³•å®‰è£matplotlibï¼Œå°‡è·³éå¯è¦–åŒ–åŠŸèƒ½")
        VISUALIZATION_AVAILABLE = False

from kaggle_environments import make, evaluate

# Set up logging with proper format
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # Console output
        logging.FileHandler('training.log')  # File output
    ]
)
logger = logging.getLogger(__name__)

# ç¢ºä¿loggerè¨Šæ¯ä¸è¢«æŠ‘åˆ¶
logger.setLevel(logging.INFO)

# ===== Shared Tactical Functions =====
# These functions are used by both worker processes and the main trainer

def flat_to_2d(board_flat):
    """Convert flat board to 2D grid format."""
    try:
        if isinstance(board_flat[0], list):
            return board_flat
    except Exception:
        pass
    return [list(board_flat[r*7:(r+1)*7]) for r in range(6)]

def find_drop_row(grid, col):
    """Find the lowest empty row in a column."""
    for r in range(5, -1, -1):
        if grid[r][col] == 0:
            return r
    return None

def is_win_from(grid, r, c, mark):
    """Check if placing a mark at (r,c) creates a winning line."""
    dirs = [(1,0), (0,1), (1,1), (-1,1)]
    for dr, dc in dirs:
        cnt = 1
        for s in (1, -1):
            rr, cc = r + dr*s, c + dc*s
            while 0 <= rr < 6 and 0 <= cc < 7 and grid[rr][cc] == mark:
                cnt += 1
                rr += dr*s
                cc += dc*s
        if cnt >= 4:
            return True
    return False

def is_winning_move(board_flat, col, mark):
    """Check if a move in col is a winning move for mark."""
    grid = flat_to_2d(board_flat)
    if col < 0 or col > 6:
        return False
    r = find_drop_row(grid, col)
    if r is None:
        return False
    grid[r][col] = mark
    return is_win_from(grid, r, col, mark)

def apply_move(board_flat, col, mark):
    """Apply a move and return the new board state."""
    grid = flat_to_2d(board_flat)
    if col < 0 or col > 6:
        return None
    r = find_drop_row(grid, col)
    if r is None:
        return None
    grid[r][col] = mark
    return [grid[i][j] for i in range(6) for j in range(7)]

def if_i_can_win(board_flat, mark, agent):
    """Find a winning move for mark, if any."""
    for c in agent.get_valid_actions(board_flat):
        if is_winning_move(board_flat, c, mark):
            return c
    return None

def if_i_will_lose(board_flat, mark, agent):
    """Find a move to block opponent's immediate win."""
    opp = 3 - mark
    for c in agent.get_valid_actions(board_flat):
        if is_winning_move(board_flat, c, opp):
            return c
    return None

def if_i_will_lose_at_next(board_flat, move_col, mark, agent):
    """Check if making move_col gives opponent an immediate winning reply."""
    grid = flat_to_2d(board_flat)
    r = find_drop_row(grid, move_col) if 0 <= move_col <= 6 else None
    if r is None:
        return True
    grid[r][move_col] = mark
    opp = 3 - mark
    # Check if opponent has immediate winning reply
    new_board = [grid[i][j] for i in range(6) for j in range(7)]
    for c in agent.get_valid_actions(new_board):
        if is_winning_move(new_board, c, opp):
            return True
    return False

def safe_moves(board_flat, mark, valid_actions, agent):
    """Return moves that don't give opponent immediate win."""
    return [a for a in valid_actions if not if_i_will_lose_at_next(board_flat, a, mark, agent)]

# ===== Shared Opponent Strategies =====

def random_opponent_strategy(board_flat, mark, valid_actions, agent):
    """Random opponent with basic tactics."""
    c = if_i_can_win(board_flat, mark, agent)
    if c is not None:
        return c
    c = if_i_will_lose(board_flat, mark, agent)
    if c is not None:
        return c
    safe = safe_moves(board_flat, mark, valid_actions, agent)
    if safe:
        return random.choice(safe)
    return random.choice(valid_actions)

def minimax_opponent_strategy(board_flat, mark, valid_actions, agent, depth=3):
    """Minimax opponent implementation."""
    def score_window(window, mark):
        opp = 3 - mark
        cnt_self = window.count(mark)
        cnt_opp = window.count(opp)
        cnt_empty = window.count(0)
        if cnt_self > 0 and cnt_opp > 0:
            return 0
        score = 0
        if cnt_self == 4:
            score += 10000
        elif cnt_self == 3 and cnt_empty == 1:
            score += 100
        elif cnt_self == 2 and cnt_empty == 2:
            score += 10
        if cnt_opp == 3 and cnt_empty == 1:
            score -= 120
        return score

    def evaluate_board(board_flat, mark):
        grid = flat_to_2d(board_flat)
        score = 0
        # center preference
        center_col = [grid[r][3] for r in range(6)]
        score += center_col.count(mark) * 3
        # Horizontal
        for r in range(6):
            row = grid[r]
            for c in range(4):
                window = row[c:c+4]
                score += score_window(window, mark)
        # Vertical
        for c in range(7):
            col = [grid[r][c] for r in range(6)]
            for r in range(3):
                window = col[r:r+4]
                score += score_window(window, mark)
        # Diagonals
        for r in range(3):
            for c in range(4):
                window = [grid[r+i][c+i] for i in range(4)]
                score += score_window(window, mark)
        for r in range(3, 6):
            for c in range(4):
                window = [grid[r-i][c+i] for i in range(4)]
                score += score_window(window, mark)
        return score

    def has_winner(board_flat, mark):
        grid = flat_to_2d(board_flat)
        for r in range(6):
            for c in range(7):
                if grid[r][c] != mark:
                    continue
                if is_win_from(grid, r, c, mark):
                    return True
        return False

    def minimax(board_flat, depth, alpha, beta, current_mark, maximizing_mark):
        valid_moves = agent.get_valid_actions(board_flat)
        if depth == 0 or not valid_moves:
            return evaluate_board(board_flat, maximizing_mark), None
        
        best_move = None
        if current_mark == maximizing_mark:
            value = -float('inf')
            for c in valid_moves:
                nb = apply_move(board_flat, c, current_mark)
                if nb is None:
                    continue
                if has_winner(nb, current_mark):
                    return 1e6 - (5 - depth), c
                child_val, _ = minimax(nb, depth-1, alpha, beta, 3-current_mark, maximizing_mark)
                if child_val > value:
                    value, best_move = child_val, c
                alpha = max(alpha, value)
                if alpha >= beta:
                    break
            return value, best_move
        else:
            value = float('inf')
            for c in valid_moves:
                nb = apply_move(board_flat, c, current_mark)
                if nb is None:
                    continue
                if has_winner(nb, current_mark):
                    return -1e6 + (5 - depth), c
                child_val, _ = minimax(nb, depth-1, alpha, beta, 3-current_mark, maximizing_mark)
                if child_val < value:
                    value, best_move = child_val, c
                beta = min(beta, value)
                if alpha >= beta:
                    break
            return value, best_move

    # Check tactical moves first
    c = if_i_can_win(board_flat, mark, agent)
    if c is not None:
        return c
    c = if_i_will_lose(board_flat, mark, agent)
    if c is not None:
        return c
    
    # Use minimax
    safe = safe_moves(board_flat, mark, valid_actions, agent)
    moves = safe if safe else valid_actions
    best_score = -float('inf')
    best_move = random.choice(moves)
    
    for a in moves:
        nb = apply_move(board_flat, a, mark)
        if nb is None:
            continue
        score, _ = minimax(nb, depth-1, -float('inf'), float('inf'), 3-mark, mark)
        if score > best_score:
            best_score, best_move = score, a
    return best_move

def self_play_opponent_strategy(board_flat, mark, valid_actions, agent):
    """Self-play opponent using current policy network."""
    state = agent.encode_state(board_flat, mark)
    action, _, _ = agent.select_action(state, valid_actions, training=False)
    return int(action)

_WORKER = {
    "agent": None,
    "env": None,
    "policy_version": -1,  # å°šæœªè¼‰å…¥ä»»ä½•ç‰ˆæœ¬
}

def _worker_init_persistent(agent_cfg):
    # é—œ GPUã€é™åŸ·è¡Œç·’ï¼Œé¿å… CPU oversubscription
    os.environ.setdefault('CUDA_VISIBLE_DEVICES', '')
    os.environ.setdefault('OMP_NUM_THREADS', '1')
    os.environ.setdefault('MKL_NUM_THREADS', '1')
    try:
        torch.set_num_threads(1)
    except Exception:
        pass

    # åˆå§‹åŒ–ä¸€æ¬¡ Agentï¼ˆCPUï¼‰èˆ‡ Env
    agent = PPOAgent(agent_cfg)            # ä½ çš„é¡åˆ¥
    agent.device = torch.device('cpu')
    agent.policy_net.to(agent.device)
    agent.policy_net.eval()

    from kaggle_environments import make    # æ”¾é€™è£¡é¿å…ä¸»é€²ç¨‹ import å½±éŸ¿
    env = make('connectx', debug=False)

    _WORKER["agent"] = agent
    _WORKER["env"] = env
    _WORKER["policy_version"] = -1

def _worker_play_one(args):
    """
    å–®å±€å°æˆ°ï¼›æŒä¹…åŒ– agent/envï¼Œä¸é‡å»ºã€‚
    åƒ…åœ¨æ”¶åˆ°æ›´é«˜ policy_version ä¸”å¤¾å¸¶æ¬Šé‡æ™‚æ‰ load_state_dictã€‚
    """
    try:
        agent = _WORKER["agent"]
        env = _WORKER["env"]

        # æ¬Šé‡æ›´æ–°ï¼ˆå¿…è¦æ™‚ï¼‰
        pv = int(args.get("policy_version", -1))
        weights_np = args.get("policy_state", None)  # åªæœ‰ç‰ˆæœ¬å‰›å‡æ™‚æ‰æœƒå¸¶
        if pv > _WORKER["policy_version"] and weights_np is not None:
            state_dict = {k: torch.from_numpy(v.copy()) if isinstance(v, np.ndarray) else v
                          for k, v in weights_np.items()}
            agent.policy_net.load_state_dict(state_dict, strict=True)
            agent.policy_net.eval()
            _WORKER["policy_version"] = pv

        # æ¯å±€çš„éš¨æ©Ÿæ€§
        seed = args.get('seed', None)
        if seed is not None:
            random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)

        # é–‹å±€
        env.reset()
        opponent_type = random.choice(['random', 'minimax', 'self'])
        player2_prob = float(args.get('player2_training_prob', 0.5))
        training_player = int(np.random.choice([1, 2], p=[1.0 - player2_prob, player2_prob]))

        transitions = []
        move_count, max_moves = 0, 50

        with torch.no_grad():
            while not env.done and move_count < max_moves:
                actions = []
                for player_idx in range(2):
                    if env.state[player_idx]['status'] == 'ACTIVE':
                        board, current_player = agent.extract_board_and_mark(env.state, player_idx)
                        valid_actions = agent.get_valid_actions(board)
                        if current_player == training_player:
                            state = agent.encode_state(board, current_player)
                            action, prob, value = agent.select_action(state, valid_actions, training=True)
                            transitions.append({
                                'state': state,
                                'action': int(action),
                                'prob': float(prob),
                                'value': float(value),
                                'training_player': training_player,
                                'opponent_type': opponent_type,
                                'is_dangerous': bool(if_i_will_lose_at_next(board, int(action), current_player, agent)),
                            })
                        else:
                            if opponent_type == 'random':
                                action = random_opponent_strategy(board, current_player, valid_actions, agent)
                            elif opponent_type == 'minimax':
                                action = minimax_opponent_strategy(board, current_player, valid_actions, agent, depth=3)
                            else:
                                action = self_play_opponent_strategy(board, current_player, valid_actions, agent)
                        actions.append(int(action))
                    else:
                        actions.append(0)
                try:
                    env.step(actions)
                except Exception:
                    break
                move_count += 1

        try:
            player_result = env.state[0]['reward'] if training_player == 1 else env.state[1]['reward']
        except Exception:
            player_result = 0

        return {
            'transitions': transitions,
            'training_player': training_player,
            'player_result': int(player_result),
            'game_length': len(transitions),
            'opponent_type': opponent_type,
            'policy_version_used': _WORKER["policy_version"],
        }
    except Exception as e:
        return {
            'transitions': [],
            'training_player': 1,
            'player_result': 0,
            'game_length': 0,
            'opponent_type': 'unknown',
            'error': str(e),
            'policy_version_used': _WORKER["policy_version"],
        }

class DropPath(nn.Module):
    """Stochastic Depth per sample (when training only)."""
    def __init__(self, drop_prob: float = 0.0):
        super().__init__()
        self.drop_prob = float(max(0.0, drop_prob))
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.drop_prob == 0.0 or not self.training:
            return x
        keep_prob = 1.0 - self.drop_prob
        # shape: (N, 1, 1, 1) for broadcasting
        mask = torch.empty((x.shape[0], 1, 1, 1), dtype=x.dtype, device=x.device).bernoulli_(keep_prob)
        return x.div(keep_prob) * mask


class ConnectXNet(nn.Module):
    """Advanced ConnectX policy-value network.
    Enhancements:
      - Coordinate embedding (row/col planes)
      - Wider channel width (128)
      - Bottleneck residual blocks with Squeeze-and-Excitation (SE)
      - Stochastic Depth (DropPath) per block
      - Periodic lightweight spatial self-attention (every attn_every blocks)
      - Shared trunk then dual heads (policy softmax, value tanh)
    Input remains flat 126 (3x6x7); coord planes added internally (->5 channels) for compatibility.
    """

    def __init__(self, input_size=126, hidden_size=192, num_layers=256, drop_path_rate: float = 0.08, attn_every: int = 4):
        super().__init__()

        print(f'input_size: {input_size}, hidden_size: {hidden_size}')
        # Clamp depth
        max_blocks = 32
        blocks = int(num_layers) if isinstance(num_layers, int) else max_blocks
        if blocks > max_blocks:
            try:
                logger.warning(f"num_layers={blocks} éæ·±ï¼Œç¸®æ¸›ç‚º {max_blocks}")
            except Exception:
                pass
            blocks = max_blocks
        self.blocks = blocks
        self.channels = 128
        self.drop_path_rate = float(max(0.0, drop_path_rate))
        self.attn_every = max(0, int(attn_every))

        # Coordinate embedding (2,6,7)
        row = torch.linspace(-1, 1, steps=6).unsqueeze(1).repeat(1, 7)
        col = torch.linspace(-1, 1, steps=7).unsqueeze(0).repeat(6, 1)
        coord = torch.stack([row, col], dim=0)  # (2,6,7)
        self.register_buffer('coord_embed', coord)

        in_ch = 3 + 2  # original planes + coord planes
        self.stem = nn.Sequential(
            nn.Conv2d(in_ch, self.channels, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(8, self.channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(p=0.05),
        )

        # --- Building blocks ---
        class BottleneckSE(nn.Module):
            def __init__(self, c, drop_path=0.0, reduction=4):
                super().__init__()
                mid = max(32, c // 2)
                self.conv1 = nn.Conv2d(c, mid, 1, bias=False)
                self.gn1 = nn.GroupNorm(8, mid)
                self.conv2 = nn.Conv2d(mid, mid, 3, padding=1, bias=False)
                self.gn2 = nn.GroupNorm(8, mid)
                self.conv3 = nn.Conv2d(mid, c, 1, bias=False)
                self.gn3 = nn.GroupNorm(8, c)
                # SE
                se_hidden = max(8, c // reduction)
                self.se_fc1 = nn.Linear(c, se_hidden)
                self.se_fc2 = nn.Linear(se_hidden, c)
                self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()
                self.act = nn.ReLU(inplace=True)
            def forward(self, x):
                identity = x
                out = self.act(self.gn1(self.conv1(x)))
                out = self.act(self.gn2(self.conv2(out)))
                out = self.gn3(self.conv3(out))
                # Squeeze Excitation
                w = out.mean(dim=[2,3])          # (B,C)
                w = self.act(self.se_fc1(w))
                w = torch.sigmoid(self.se_fc2(w)).unsqueeze(-1).unsqueeze(-1)  # (B,C,1,1)
                out = out * w
                out = self.drop_path(out)
                out = self.act(out + identity)
                return out

        class SpatialSelfAttention(nn.Module):
            def __init__(self, c, heads=4):
                super().__init__()
                self.heads = heads
                self.scale = (c // heads) ** -0.5
                self.qkv = nn.Conv2d(c, c * 3, 1, bias=False)
                self.proj = nn.Conv2d(c, c, 1, bias=False)
            def forward(self, x):  # x: (B,C,H,W)
                B, C, H, W = x.shape
                qkv = self.qkv(x).reshape(B, 3, self.heads, C // self.heads, H * W)
                q, k, v = qkv[:,0], qkv[:,1], qkv[:,2]  # (B,heads,dim,HW)
                attn = (q.transpose(-2,-1) @ k) * self.scale  # (B,heads,HW,HW)
                attn = torch.softmax(attn, dim=-1)
                out = (attn @ v.transpose(-2,-1)).transpose(-2,-1)  # (B,heads,dim,HW)
                out = out.reshape(B, C, H, W)
                return self.proj(out)

        # Assemble trunk
        dprates = torch.linspace(0, self.drop_path_rate, steps=self.blocks).tolist() if self.drop_path_rate > 0 else [0.0]*self.blocks
        self.trunk = nn.ModuleList()
        for i in range(self.blocks):
            self.trunk.append(BottleneckSE(self.channels, drop_path=dprates[i]))
            if self.attn_every > 0 and (i+1) % self.attn_every == 0:
                self.trunk.append(SpatialSelfAttention(self.channels))

        # Head
        self.head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.channels * 6 * 7, hidden_size),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
        )
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_size // 2, 7),
            nn.Softmax(dim=-1),
        )
        self.value_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_size // 2, 1),
            nn.Tanh(),
        )

        # Init
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.GroupNorm):
                if m.weight is not None:
                    nn.init.ones_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        B = x.size(0)
        x = x.view(B, 3, 6, 7)
        # concat coord embeddings
        coord = self.coord_embed.unsqueeze(0).expand(B, -1, -1, -1)
        x = torch.cat([x, coord], dim=1)  # (B,5,6,7)
        y = self.stem(x)
        for block in self.trunk:
            y = block(y)
        feat = self.head(y)
        policy = self.policy_head(feat)
        value = self.value_head(feat)
        return policy, value

class PPOAgent:
    """PPO å¼·åŒ–å­¸ç¿’æ™ºèƒ½é«”"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ç¥ç¶“ç¶²è·¯
        self.policy_net = ConnectXNet(
            input_size=config['input_size'],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers']
        ).to(self.device)

        # å„ªåŒ–å™¨
        self.optimizer = optim.Adam(
            self.policy_net.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )

        # å­¸ç¿’ç‡èª¿åº¦å™¨ - æ ¹æ“šæ€§èƒ½å‹•æ…‹èª¿æ•´
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',  # ç›£æ§å‹ç‡æœ€å¤§åŒ–
            factor=0.7,  # å­¸ç¿’ç‡é™ä½å› å­
            patience=1000,  # ç­‰å¾…å›åˆæ•¸
            min_lr=1e-8
        )

        # è¨“ç·´åƒæ•¸
        self.gamma = config['gamma']
        self.eps_clip = config['eps_clip']
        self.k_epochs = config['k_epochs']
        self.entropy_coef = config['entropy_coef']
        self.value_coef = config['value_coef']

        # ç¶“é©—ç·©è¡å€
        self.memory = deque(maxlen=config['buffer_size'])

        # --- Stuck detection / partial reset settings ---
        reset_cfg = config.get('reset', {}) if isinstance(config, dict) else {}
        from collections import deque as _dq
        self.entropy_window = _dq(maxlen=int(reset_cfg.get('entropy_window', 30)))
        self.entropy_threshold = float(reset_cfg.get('entropy_threshold', 0.9))  # ln(7)â‰ˆ1.95, 0.9 is fairly sharp
        self.low_entropy_patience = int(reset_cfg.get('low_entropy_patience', 30))
        self.reset_cooldown_updates = int(reset_cfg.get('cooldown_updates', 200))
        self.partial_reset_fraction = float(reset_cfg.get('fraction', 0.25))  # reset fraction of residual blocks
        self.update_step = 0
        self.last_partial_reset_update = -10**9
        # Ensure numpy alias
        self._np = np

    # === Kaggle env helpers used by trainer ===
    def extract_board_and_mark(self, env_state, player_idx):
        """Return (board_flat_list, mark) from kaggle_environments state list.
        board: list[int] length 42, row-major 6x7, 0 empty, 1/2 marks.
        mark: 1 or 2 for the given player index.
        """
        try:
            obs = env_state[player_idx]['observation']
            board = obs.get('board') or obs.get('board_state')
            mark = obs.get('mark') or (1 if player_idx == 0 else 2)
            return list(board), int(mark)
        except Exception:
            # Fallback: try top-level
            try:
                board = env_state['board']
                mark = env_state.get('mark', 1)
                return list(board), int(mark)
            except Exception:
                return [0] * 42, 1

    def encode_state(self, board, mark):
        """Encode board (len=42) and mark (1/2) into 126-d float state (3x6x7).
        Planes: [my pieces, opp pieces, ones].
        """
        try:
            grid = [board[r * 7:(r + 1) * 7] for r in range(6)]
            my = np.zeros((6, 7), dtype=np.float32)
            opp = np.zeros((6, 7), dtype=np.float32)
            for r in range(6):
                for c in range(7):
                    v = grid[r][c]
                    if v == mark:
                        my[r, c] = 1.0
                    elif v != 0:
                        opp[r, c] = 1.0
            ones = np.ones((6, 7), dtype=np.float32)
            stacked = np.stack([my, opp, ones], axis=0)
            return stacked.reshape(-1)
        except Exception:
            return np.zeros(126, dtype=np.float32)

    def get_valid_actions(self, board):
        """Valid columns where top cell is empty."""
        try:
            grid_top = [board[c] for c in range(7)]  # row 0
            return [c for c in range(7) if grid_top[c] == 0]
        except Exception:
            # Generic fallback
            valid = []
            try:
                for c in range(7):
                    col_full = True
                    for r in range(6):
                        if board[r * 7 + c] == 0:
                            col_full = False
                            break
                    if not col_full:
                        valid.append(c)
            except Exception:
                valid = list(range(7))
            return valid

    def select_action(self, state, valid_actions, training=True, temperature=1.0, exploration_bonus=0.0):
        """Return (action, prob, value). Masks invalid actions and samples during training.
        temperature <= 0 or not training -> greedy.
        """
        self.policy_net.eval()  # eval OK for GroupNorm with small batch; sampling unaffected
        with torch.no_grad():
            s = torch.as_tensor(state, dtype=torch.float32, device=self.device).view(1, -1)
            probs, value = self.policy_net(s)
            probs = probs.squeeze(0).clamp_min(1e-8)
            mask = torch.zeros_like(probs)
            if not valid_actions:
                valid_actions = list(range(7))
            mask[valid_actions] = 1.0
            masked = probs * mask
            if masked.sum() <= 0:
                # fallback to uniform over valid
                masked = mask / mask.sum()
            else:
                masked = masked / masked.sum()
            if training and temperature is not None and temperature > 1e-6:
                logits = torch.log(masked + 1e-8) / float(temperature)
                dist = torch.distributions.Categorical(logits=logits)
            else:
                dist = torch.distributions.Categorical(probs=masked)
            action = int(dist.sample().item())
            prob = float(masked[action].item())
            return action, prob, float(value.item())

    def store_transition(self, state, action, prob, reward, done):
        self.memory.append((state, action, prob, reward, done))

    def compute_gae(self, rewards, values, dones, next_value, gamma=0.99, lam=0.95):
        T = len(rewards)
        adv = np.zeros(T, dtype=np.float32)
        last_gae = 0.0
        for t in reversed(range(T)):
            v_next = values[t + 1] if t + 1 < T else next_value
            mask = 1.0 - float(dones[t])
            delta = rewards[t] + gamma * v_next * mask - values[t]
            last_gae = delta + gamma * lam * mask * last_gae
            adv[t] = last_gae
        returns = adv + np.array(values[:T], dtype=np.float32)
        return adv.tolist(), returns.tolist()

    def partial_reset(self, which: str = 'res_blocks_and_head', fraction: float | None = None):
        """Reinitialize a subset of the network to escape local minima.
        which: 'res_blocks', 'head', 'res_blocks_and_head'
        fraction: portion of residual blocks to reset (default from config)
        """
        fraction = self.partial_reset_fraction if fraction is None else float(fraction)
        net: ConnectXNet = self.policy_net  # type: ignore
        reset_count = 0
        
        # Reset a random subset of trunk blocks
        if which in ('res_blocks', 'res_blocks_and_head'):
            trunk_blocks = list(net.trunk)
            import random as _rnd
            k = max(1, int(len(trunk_blocks) * max(0.0, min(1.0, fraction))))
            idxs = _rnd.sample(range(len(trunk_blocks)), k)
            for i in idxs:
                block = trunk_blocks[i]
                # Reinitialize parameters for this block
                for m in block.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
                    elif isinstance(m, nn.Linear):
                        nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
                    elif isinstance(m, nn.GroupNorm):
                        if m.weight is not None:
                            nn.init.ones_(m.weight)
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
                reset_count += 1
        
        # Optionally reset head and policy head
        if which in ('head', 'res_blocks_and_head'):
            for m in net.head.modules():
                if isinstance(m, nn.Linear):
                    nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
            for m in net.policy_head.modules():
                if isinstance(m, nn.Linear):
                    nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
            # Value head left intact to preserve value estimates
        
        try:
            logger.info(f"ğŸ”„ Partial reset applied: which={which}, fraction={fraction:.2f}, reset_blocks={reset_count}")
        except Exception:
            pass
        self.last_partial_reset_update = self.update_step

    def update_policy(self):
        """ä½¿ç”¨ PPO æ›´æ–°ç­–ç•¥"""
        if len(self.memory) < self.config['min_batch_size']:
            return None

        # æº–å‚™è¨“ç·´æ•¸æ“š
        states = []
        actions = []
        old_probs = []
        rewards = []
        dones = []

        for transition in self.memory:
            state, action, prob, reward, done = transition
            states.append(state)
            actions.append(action)
            old_probs.append(prob)
            rewards.append(reward)
            dones.append(done)

        # è¨ˆç®—æ‰€æœ‰ç‹€æ…‹çš„åƒ¹å€¼
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        with torch.no_grad():
            _, values_tensor = self.policy_net(states_tensor)
            values = values_tensor.cpu().numpy().flatten()

        # è¨ˆç®—å„ªå‹¢å’Œå›å ±
        advantages, returns = self.compute_gae(
            rewards, values, dones, 0,
            self.gamma, self.config['gae_lambda']
        )

        # æ­£è¦åŒ–å„ªå‹¢
        advantages = np.array(advantages)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # è½‰æ›ç‚ºå¼µé‡
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        actions_tensor = torch.LongTensor(actions).to(self.device)
        old_probs_tensor = torch.FloatTensor(old_probs).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        returns_tensor = torch.FloatTensor(returns).to(self.device)

        # PPO æ›´æ–°
        total_loss = 0
        entropy_sum = 0.0
        for _ in range(self.k_epochs):
            # å‰å‘å‚³æ’­
            new_probs, values = self.policy_net(states_tensor)

            # è¨ˆç®—æ¯”ç‡
            new_action_probs = new_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze()
            ratio = new_action_probs / (old_probs_tensor + 1e-8)

            # PPO æå¤±
            surr1 = ratio * advantages_tensor
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages_tensor
            policy_loss = -torch.min(surr1, surr2).mean()

            # åƒ¹å€¼æå¤±
            value_loss = nn.MSELoss()(values.squeeze(), returns_tensor)

            # ç†µæå¤±ï¼ˆæ¢ç´¢ï¼‰
            entropy = -(new_probs * torch.log(new_probs + 1e-8)).sum(dim=1).mean()
            entropy_sum += float(entropy.item())

            # ç¸½æå¤±
            loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
            total_loss += loss.item()

            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
            self.optimizer.step()

        # æ¸…ç©ºè¨˜æ†¶é«”
        self.memory.clear()

        # --- Stuck detection on entropy ---
        self.update_step += 1
        avg_entropy = entropy_sum / max(1, self.k_epochs)
        self.entropy_window.append(avg_entropy)
        if len(self.entropy_window) >= max(5, self.entropy_window.maxlen or 5):
            try:
                mean_ent = float(np.mean(self.entropy_window))
                if mean_ent < self.entropy_threshold and (self.update_step - self.last_partial_reset_update) >= self.reset_cooldown_updates:
                    logger.info(f"ğŸ” ä½ç†µè§¸ç™¼éƒ¨åˆ†é‡ç½®: mean_entropy={mean_ent:.3f} < thr={self.entropy_threshold:.3f}")
                    self.partial_reset('res_blocks_and_head')
            except Exception as e:
                try:
                    logger.debug(f"entropy reset check failed: {e}")
                except Exception:
                    pass
        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': avg_entropy,
            'total_loss': total_loss / self.k_epochs
        }

class ConnectXTrainer:
    """ConnectX è¨“ç·´å™¨"""

    def __init__(self, config_path_or_dict="config.yaml"):
        # è¼‰å…¥é…ç½®
        if isinstance(config_path_or_dict, dict):
            self.config = config_path_or_dict
        else:
            with open(config_path_or_dict, 'r') as f:
                self.config = yaml.safe_load(f)

        # åˆå§‹åŒ–æ™ºèƒ½é«”
        self.agent = PPOAgent(self.config['agent'])

        # è¨“ç·´çµ±è¨ˆ
        self.episode_rewards = []
        self.win_rates = []
        self.training_losses = []
        
        # æ–°å¢ï¼šæ­·å²æ¨¡å‹ä¿å­˜ï¼ˆç”¨æ–¼èª²ç¨‹åŒ–å­¸ç¿’ï¼‰
        self.historical_models = []
        self.model_save_frequency = 1000  # æ¯1000å›åˆä¿å­˜ä¸€å€‹æ­·å²æ¨¡å‹
        
        # æ¢ç´¢å¢å¼·åƒæ•¸
        self.exploration_decay = 0.995
        self.min_exploration = 0.1
        self.current_exploration = 1.0

        # è¨“ç·´ç©å®¶é¸æ“‡æ©Ÿç‡è¨­å®š
        training_config = self.config.get('training', {})
        self.player2_training_prob = training_config.get('player2_training_probability', 0.7)  # é»˜èª70%æ©Ÿç‡é¸æ“‡player2
        # å¼·åŒ–å¾Œæ‰‹è¨“ç·´æ¯”ä¾‹ï¼Œæå‡å¾Œæ‰‹å‹ç‡
        # boosted_p2 = max(self.player2_training_prob, 0.85)
        # if boosted_p2 != self.player2_training_prob:
        #     logger.info(f"æé«˜å¾Œæ‰‹è¨“ç·´æ¯”ä¾‹: {self.player2_training_prob:.2f} -> {boosted_p2:.2f}")
        #     self.player2_training_prob = boosted_p2
        
        # æŒçºŒå­¸ç¿’æ•¸æ“š
        self.continuous_learning_data = None
        self.continuous_learning_targets = None

        # å‰µå»ºç›®éŒ„
        os.makedirs('checkpoints', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        # --- æ–°å¢ï¼šåœæ»¯åµæ¸¬ç‹€æ…‹ ---
        from collections import deque as _dq
        self._stagnation_scores = _dq(maxlen=50)  # æœ€è¿‘è©•ä¼°åˆ†æ•¸ç·©è¡
        self._best_score_seen = -1.0
        self._best_score_episode = 0
        self._last_stagnation_handle_ep = -10**9

    # --- Board helpers (6x7) - using shared functions ---
    def _flat_to_2d(self, board_flat):
        return flat_to_2d(board_flat)

    def _find_drop_row(self, grid, col):
        return find_drop_row(grid, col)

    def _apply_move(self, board_flat, col, mark):
        return apply_move(board_flat, col, mark)

    def _is_win_from(self, grid, r, c, mark):
        return is_win_from(grid, r, c, mark)

    def _is_winning_move(self, board_flat, col, mark):
        return is_winning_move(board_flat, col, mark)

    def _any_winning_moves(self, board_flat, mark):
        wins = []
        for c in self.agent.get_valid_actions(board_flat):
            if self._is_winning_move(board_flat, c, mark):
                wins.append(c)
        return wins

    # --- Tactical utilities - using shared functions ---
    def if_i_can_win(self, board_flat, mark):
        return if_i_can_win(board_flat, mark, self.agent)

    def if_i_will_lose(self, board_flat, mark):
        return if_i_will_lose(board_flat, mark, self.agent)

    def if_i_will_lose_at_next(self, board_flat, move_col, mark):
        return if_i_will_lose_at_next(board_flat, move_col, mark, self.agent)

    # NEW: list all moves that do NOT give opponent an immediate winning reply
    def _safe_moves(self, board_flat, mark, valid_actions):
        return safe_moves(board_flat, mark, valid_actions, self.agent)

    # Tactical-aware random opponent move - using shared function
    def _random_with_tactics(self, board_flat, mark, valid_actions):
        return random_opponent_strategy(board_flat, mark, valid_actions, self.agent)

    # --- Random opening tactic for first player (mark==1) ---
    def _random_opening_move(self, board_flat, mark, valid_actions):
        """If this random player is the starter (mark==1), follow opening: 3 -> 2 -> 4 -> then 5 or 1 depending on top row empty.
        Returns a move or None if not applicable."""
        try:
            if int(mark) != 1:
                return None
            grid = self._flat_to_2d(board_flat)
            # Count own tokens on board to determine which turn for this player
            my_tokens = 0
            for r in range(6):
                for c in range(7):
                    if grid[r][c] == 1:
                        my_tokens += 1
            # Opening sequence by own move index (before placing this move)
            if my_tokens == 0 and 3 in valid_actions:
                return 3
            if my_tokens == 1 and 2 in valid_actions:
                return 2
            if my_tokens == 2 and 4 in valid_actions:
                return 4
            if my_tokens == 3:
                top5_empty = (grid[0][5] == 0)
                top1_empty = (grid[0][1] == 0)
                if top5_empty and 5 in valid_actions:
                    return 5
                if top1_empty and 1 in valid_actions:
                    return 1
            return None
        except Exception:
            return None

    def _random_with_opening(self, board_flat, mark, valid_actions):
        """Opening when starting; after choosing, ensure it is safe, else pick a safe move, else random."""
        move = self._random_opening_move(board_flat, mark, valid_actions)
        if move is not None:
            # If opening move would immediately allow opponent to win, try to pick a safe alternative
            if self.if_i_will_lose_at_next(board_flat, move, mark):
                safe = self._safe_moves(board_flat, mark, valid_actions)
                if safe:
                    return random.choice(safe)
                # no safe moves, keep original (or random)
                return random.choice(valid_actions)
            return move
        # No opening move; prefer safe moves
        safe = self._safe_moves(board_flat, mark, valid_actions)
        if safe:
            return random.choice(safe)
        return random.choice(valid_actions)

    # --- Unified tactical + opening random opponent - using shared functions ---
    def _tactical_random_opening_agent(self, board_flat, mark, valid_actions):
        """Priority: win -> block -> (if starter opening) -> safe move -> random.
        Adds safe-move layer to avoid handing immediate wins to opponent."""
        # Win if possible
        c = if_i_can_win(board_flat, mark, self.agent)
        if c is not None:
            return c
        # Block if necessary
        c = if_i_will_lose(board_flat, mark, self.agent)
        if c is not None:
            return c
        # If starter, try opening move (but reject if unsafe)
        move = self._random_opening_move(board_flat, mark, valid_actions)
        if move is not None and not if_i_will_lose_at_next(board_flat, move, mark, self.agent):
            return move
        # Safe moves filter
        safe = safe_moves(board_flat, mark, valid_actions, self.agent)
        if safe:
            return random.choice(safe)
        # Fallback to random
        return random.choice(valid_actions)

    def _play_one_game_vs_random(self):
        env = make('connectx', debug=False)
        env.reset()
        move_count = 0
        max_moves = 50
        with torch.no_grad():
            while not env.done and move_count < max_moves:
                actions = []
                for p in range(2):
                    if env.state[p]['status'] == 'ACTIVE':
                        board, mark = self.agent.extract_board_and_mark(env.state, p)
                        state = self.agent.encode_state(board, mark)
                        valid = self.agent.get_valid_actions(board)
                        if p == 0:
                            a, _, _ = self.agent.select_action(state, valid, training=False)
                        else:
                            a = self._tactical_random_opening_agent(board, mark, valid)
                        actions.append(int(a))
                    else:
                        actions.append(0)
                try:
                    env.step(actions)
                except Exception:
                    break
                move_count += 1
        # å›å‚³ç©å®¶1çµæœï¼ˆæˆ‘æ–¹ï¼‰
        try:
            return 1 if env.state[0]['reward'] == 1 else (0 if env.state[0]['reward'] == 0 else -1)
        except Exception:
            return 0

    # --- Minimax opponent for evaluation ---
    def _choose_minimax_move(self, board_flat, mark, max_depth):
        # Use shared minimax strategy
        valid = self.agent.get_valid_actions(board_flat)
        return minimax_opponent_strategy(board_flat, mark, valid, self.agent, depth=max_depth)

    def evaluate_against_random(self, games: int = 50):
        wins = 0
        draws = 0
        total_games = max(1, int(games))
        
        for game_idx in range(total_games):
            try:
                r = self._play_one_game_vs_random()
                if r == 1:
                    wins += 1
                elif r == 0:
                    draws += 1
            except Exception as e:
                logger.warning(f"Game {game_idx} vs random failed: {e}")
                continue
                
        win_rate = wins / total_games
        logger.info(f"evaluate_against_random: {wins}/{total_games} wins, win_rate={win_rate:.3f}")
        return win_rate

    def evaluate_against_minimax(self, games: int = 20):
        wins = 0
        draws = 0
        total_games = max(1, int(games))
        
        for game_idx in range(total_games):
            try:
                env = make('connectx', debug=False)
                env.reset()
                moves = 0
                with torch.no_grad():
                    while not env.done and moves < 50:
                        actions = []
                        for p in range(2):
                            if env.state[p]['status'] == 'ACTIVE':
                                board, mark = self.agent.extract_board_and_mark(env.state, p)
                                state = self.agent.encode_state(board, mark)
                                valid = self.agent.get_valid_actions(board)
                                if p == 0:
                                    a, _, _ = self.agent.select_action(state, valid, training=False)
                                else:
                                    a = self._choose_minimax_move(board, mark, max_depth=int(self.config.get('evaluation', {}).get('minimax_depth', 3)))
                                actions.append(int(a))
                            else:
                                actions.append(0)
                        try:
                            env.step(actions)
                        except Exception:
                            break
                        moves += 1
                try:
                    res = env.state[0]['reward']
                    if res == 1:
                        wins += 1
                    elif res == 0:
                        draws += 1
                except Exception:
                    draws += 1
            except Exception as e:
                logger.warning(f"Game {game_idx} vs minimax failed: {e}")
                continue
                
        win_rate = wins / total_games
        logger.info(f"evaluate_against_minimax: {wins}/{total_games} wins, win_rate={win_rate:.3f}")
        return win_rate

    def evaluate_with_metrics(self, games: int = 50):
        wins = 0
        losses = 0
        draws = 0
        lengths = []
        for _ in range(max(1, int(games))):
            env = make('connectx', debug=False)
            env.reset()
            moves = 0
            with torch.no_grad():
                while not env.done and moves < 50:
                    actions = []
                    for p in range(2):
                        if env.state[p]['status'] == 'ACTIVE':
                            board, mark = self.agent.extract_board_and_mark(env.state, p)
                            state = self.agent.encode_state(board, mark)
                            valid = self.agent.get_valid_actions(board)
                            if p == 0:
                                a, _, _ = self.agent.select_action(state, valid, training=False)
                            else:
                                a = self._tactical_random_opening_agent(board, mark, valid)
                            actions.append(int(a))
                        else:
                            actions.append(0)
                    try:
                        env.step(actions)
                    except Exception:
                        break
                    moves += 1
            try:
                res = env.state[0]['reward']
                if res == 1:
                    wins += 1
                elif res == -1:
                    losses += 1
                else:
                    draws += 1
            except Exception:
                draws += 1
            lengths.append(moves)
        total = max(1, int(games))
        win_rate = wins / total
        avg_len = float(np.mean(lengths)) if lengths else 0.0
        return {
            'win_rate': win_rate,
            'avg_game_length': avg_len,
            'wins': wins,
            'losses': losses,
            'draws': draws,
            'quick_wins': 0,
            'comeback_wins': 0,
        }

    def _choose_policy_with_tactics(self, board_flat, mark, valid_actions, training=False):
        # å…ˆçœ‹æ˜¯å¦èƒ½ç›´æ¥è´
        c = if_i_can_win(board_flat, mark, self.agent)
        if c is not None:
            return c
        # å†çœ‹æ˜¯å¦å¿…é ˆæ“‹ä¸‹å°æ–¹
        c = if_i_will_lose(board_flat, mark, self.agent)
        if c is not None:
            return c
        # ç”¨ç­–ç•¥ç¶²è·¯é¸æ“‡
        state = self.agent.encode_state(board_flat, mark)
        a, _, _ = self.agent.select_action(state, valid_actions, training=training)
        a = int(a)
        # é¿å…ç«‹å³é€çµ¦å°æ‰‹è‡´å‹
        if if_i_will_lose_at_next(board_flat, a, mark, self.agent):
            safe = safe_moves(board_flat, mark, valid_actions, self.agent)
            if safe:
                return random.choice(safe)
        return a

    def _play_one_game_self_tactical(self, main_as_player1=True):
        """è‡ªæˆ‘å°æˆ°ï¼šå°æ‰‹ä½¿ç”¨æˆ°è¡“è¦å‰‡(if_i_can_win/if_i_will_lose)ï¼Œä¸»æ§æ–¹ç”¨ç­–ç•¥ç¶²è·¯ã€‚
        main_as_player1 æ±ºå®šæˆ‘æ–¹æ˜¯å…ˆæ‰‹é‚„æ˜¯å¾Œæ‰‹ï¼Œå›å‚³æˆ‘æ–¹å‹/å’Œ/è²  (1/0/-1)ã€‚"""
        env = make('connectx', debug=False)
        env.reset()
        moves = 0
        with torch.no_grad():
            while not env.done and moves < 50:
                actions = []
                for p in range(2):
                    if env.state[p]['status'] != 'ACTIVE':
                        actions.append(0)
                        continue
                    board, mark = self.agent.extract_board_and_mark(env.state, p)
                    valid = self.agent.get_valid_actions(board)
                    # æ±ºå®šå“ªä¸€æ–¹æ˜¯æˆ‘æ–¹
                    is_main = (p == 0 and main_as_player1) or (p == 1 and not main_as_player1)
                    if is_main:
                        # æˆ‘æ–¹ç”¨ç­–ç•¥ + å®‰å…¨æª¢æŸ¥
                        a = self._choose_policy_with_tactics(board, mark, valid, training=False)
                    else:
                        # å°æ‰‹ä½¿ç”¨åˆä½µç‰ˆï¼šæˆ°è¡“å„ªå…ˆ + å…ˆæ‰‹é–‹å±€ + éš¨æ©Ÿ
                        a = self._tactical_random_opening_agent(board, mark, valid)
                    actions.append(int(a))
                try:
                    env.step(actions)
                except Exception:
                    break
                moves += 1
        try:
            if main_as_player1:
                res = env.state[0]['reward']
            else:
                res = env.state[1]['reward']
            return 1 if res == 1 else (0 if res == 0 else -1)
        except Exception:
            return 0

    def evaluate_self_play_tactical(self, games: int = 50):
        wins = 0
        draws = 0
        for _ in range(max(1, int(games))):
            r = self._play_one_game_self_tactical(main_as_player1=True)
            if r == 1:
                wins += 1
            elif r == 0:
                draws += 1
        return wins / max(1, int(games))

    def evaluate_self_play_player2_focus(self, games: int = 50):
        """è©•ä¼°æˆ‘æ–¹ä½œç‚ºå¾Œæ‰‹(ç©å®¶2)æ™‚çš„å‹ç‡ï¼Œå°æ‰‹æ¡æˆ°è¡“è¦å‰‡ã€‚"""
        wins = 0
        draws = 0
        total_games = max(1, int(games))
        
        for game_idx in range(total_games):
            try:
                r = self._play_one_game_self_tactical(main_as_player1=False)
                if r == 1:
                    wins += 1
                elif r == 0:
                    draws += 1
            except Exception as e:
                logger.warning(f"Game {game_idx} self_play_player2 failed: {e}")
                continue
                
        win_rate = wins / total_games
        logger.info(f"evaluate_self_play_player2_focus: {wins}/{total_games} wins, win_rate={win_rate:.3f}")
        return win_rate

    def evaluate_comprehensive(self, games: int = 50):
        try:
            vs_random = self.evaluate_against_random(games)
            logger.info(f"vs_random result: {vs_random}")
        except Exception as e:
            logger.warning(f"evaluate_against_random failed: {e}")
            vs_random = 0.0
            
        try:
            vs_minimax = self.evaluate_against_minimax(max(1, games // 2))
            logger.info(f"vs_minimax result: {vs_minimax}")
        except Exception as e:
            logger.warning(f"evaluate_against_minimax failed: {e}")
            vs_minimax = 0.0
            
        try:
            # è‡ªæˆ‘å°æˆ°åˆ†å…©ç¨®ï¼šå…ˆæ‰‹èˆ‡å¾Œæ‰‹ç„¦é»ï¼Œé€™è£¡ä½¿ç”¨å¾Œæ‰‹ç„¦é»ä»¥é¼“å‹µå¾Œæ‰‹èƒ½åŠ›
            self_play = self.evaluate_self_play_player2_focus(max(1, games // 2))
            logger.info(f"self_play result: {self_play}")
        except Exception as e:
            logger.warning(f"evaluate_self_play_player2_focus failed: {e}")
            self_play = 0.0
            
        # æ¬Šé‡ï¼ˆè‹¥é…ç½®æœ‰æä¾›ï¼‰
        weights = self.config.get('evaluation', {}).get('weights', {})
        w_self = float(weights.get('self_play', 0.4))
        w_minimax = float(weights.get('vs_minimax', 0.4))
        w_random = float(weights.get('vs_random', 0.2))
        total_w = max(1e-6, w_self + w_minimax + w_random)
        comprehensive = (w_self * self_play + w_minimax * vs_minimax + w_random * vs_random) / total_w
        
        return {
            'vs_random': vs_random,
            'vs_minimax': vs_minimax,
            'self_play': self_play,
            'comprehensive_score': comprehensive,
        }

    # --- Stagnation detection / handling ---
    def _detect_convergence_stagnation(self, current_episode: int, current_score: float) -> bool:
        """åµæ¸¬æ˜¯å¦é€²å…¥æ”¶æ–‚åœæ»¯ç‹€æ…‹ã€‚
        æ¢ä»¶(æ»¿è¶³ä»»ä¸€æ ¸å¿ƒæ¢ä»¶ + åŸºæœ¬æ¢ä»¶)ï¼š
          1. æœ€è¿‘çª—å£(score_window)çš„åˆ†æ•¸æ³¢å‹•ç¯„åœéå° (range < range_threshold)
          2. å·²ç¶“è¶…é patience_episodes ä»æ²’æœ‰åˆ·æ–°æœ€ä½³åˆ†æ•¸
          3. åˆ†æ•¸é•·æœŸä½æ–¼ low_score_threshold (å¹³å‡å€¼ < è©²å€¼ ä¸” episode >= min_episode_start)
        è§¸ç™¼å¾Œç”± _handle_convergence_stagnation åŸ·è¡Œéƒ¨åˆ†æ¬Šé‡é‡ç½®èˆ‡æ¢ç´¢å¢å¼·ã€‚
        """
        cfg = self.config.get('stagnation', {}) if isinstance(self.config, dict) else {}
        window_size = int(cfg.get('window', 10))
        range_threshold = float(cfg.get('range_threshold', 0.02))
        patience_eps = int(cfg.get('patience_episodes', 800))
        low_score_threshold = float(cfg.get('low_score_threshold', 0.35))
        min_episode_start = int(cfg.get('min_episode_start', 500))
        cooldown_eps = int(cfg.get('cooldown_episodes', 800))

        # æ›´æ–°ç‹€æ…‹
        self._stagnation_scores.append(float(current_score))
        if current_score > self._best_score_seen + 1e-6:
            self._best_score_seen = float(current_score)
            self._best_score_episode = int(current_episode)

        # å†·å»æœªéä¸å†è§¸ç™¼
        if current_episode - self._last_stagnation_handle_ep < cooldown_eps:
            return False

        if len(self._stagnation_scores) < max(5, window_size):
            return False

        recent = list(self._stagnation_scores)[-window_size:]
        r_min, r_max = min(recent), max(recent)
        r_range = r_max - r_min
        avg_recent = sum(recent) / len(recent)

        no_improve_eps = current_episode - self._best_score_episode

        cond_small_range = (r_range < range_threshold)
        cond_patience = (no_improve_eps >= patience_eps and current_episode >= min_episode_start)
        cond_low = (avg_recent < low_score_threshold and current_episode >= min_episode_start)

        triggered = (cond_small_range or cond_patience or cond_low)
        if triggered:
            try:
                logger.info(
                    f"âš ï¸ åµæ¸¬åˆ°å¯èƒ½åœæ»¯: eps={current_episode} score={current_score:.3f} "
                    f"range={r_range:.4f} avg={avg_recent:.3f} no_improve={no_improve_eps} "
                    f"(range<{range_threshold}? {cond_small_range}, patience? {cond_patience}, low? {cond_low})"
                )
            except Exception:
                pass
        return triggered

    def _handle_convergence_stagnation(self, current_episode: int):
        """è™•ç†æ”¶æ–‚åœæ»¯ï¼šéƒ¨åˆ†é‡ç½®æ¨¡å‹æ¬Šé‡ã€æå‡æ¢ç´¢ã€é©åº¦èª¿æ•´å­¸ç¿’ç‡/æ¸…ç·©è¡ã€‚
        ç­–ç•¥ï¼š
          1. ä½¿ç”¨ agent.partial_reset é‡ç½®éƒ¨åˆ†æ®˜å·®å€å¡Šèˆ‡ head (ä¾ reset_fraction)
          2. æå‡ entropy_coefï¼ˆä¸Šé™ä¿è­·ï¼‰é¼“å‹µæ¢ç´¢
          3. å¯é¸ï¼šè¼•å¾®èª¿æ•´å­¸ç¿’ç‡(é™ä½æˆ–å›å½ˆ)ï¼›é€™è£¡æ¡ç”¨ *0.9 è®“é‡æ–°æœç´¢æ›´ç©©å®š
          4. æ¸…ç©º PPO è¨˜æ†¶é¿å…èˆŠç­–ç•¥åˆ†ä½ˆå¹²æ“¾
          5. é‡ç½®å°æ‡‰ optimizer state ä»¥é˜²éºç•™å‹•é‡
        """
        cfg = self.config.get('stagnation', {}) if isinstance(self.config, dict) else {}
        cooldown_eps = int(cfg.get('cooldown_episodes', 800))
        reset_fraction = float(cfg.get('reset_fraction', 0.30))
        entropy_boost = float(cfg.get('entropy_boost', 1.35))
        entropy_max = float(cfg.get('entropy_max', 0.02))  # entropy_coef é€šå¸¸å¾ˆå°ï¼Œè¨­å®šä¸Šé™
        lr_decay = float(cfg.get('lr_decay_factor', 0.9))
        
        min_gap_ok = (current_episode - self._last_stagnation_handle_ep) >= cooldown_eps
        if not min_gap_ok:
            return
        
        try:
            logger.info(
                f"ğŸ› ï¸ åŸ·è¡Œåœæ»¯è™•ç†: episode={current_episode} reset_fraction={reset_fraction} entropy_boost={entropy_boost}"
            )
        except Exception:
            pass
        
        # 1) éƒ¨åˆ†é‡ç½®ï¼ˆè‹¥ agent æä¾› partial_resetï¼‰
        try:
            if hasattr(self.agent, 'partial_reset'):
                self.agent.partial_reset('res_blocks_and_head', fraction=reset_fraction)
            else:
                # å¾Œå‚™ï¼šæ‰‹å‹•æŒ‘é¸éƒ¨åˆ†å±¤é‡æ–°åˆå§‹åŒ–
                import random
                import math
                modules = []
                for m in self.agent.policy_net.modules():  # type: ignore
                    if isinstance(m, (torch.nn.Conv2d, torch.nn.Linear)) and m.weight.requires_grad:
                        modules.append(m)
                random.shuffle(modules)
                k = max(1, int(len(modules) * reset_fraction))
                for m in modules[:k]:
                    if isinstance(m, torch.nn.Conv2d):
                        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                    else:
                        torch.nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                    if m.bias is not None:
                        torch.nn.init.zeros_(m.bias)
        except Exception as e:
            try:
                logger.warning(f"éƒ¨åˆ†é‡ç½®éç¨‹å‡ºéŒ¯: {e}")
            except Exception:
                pass
        
        # 2) å¢åŠ æ¢ç´¢: entropy_coef ä¹˜ä»¥ boost (æœ‰ä¸Šé™)
        try:
            if hasattr(self.agent, 'entropy_coef'):
                new_entropy = min(entropy_max, getattr(self.agent, 'entropy_coef', 0.0) * entropy_boost + 1e-12)
                self.agent.entropy_coef = new_entropy
                logger.info(f"ğŸ”„ entropy_coef -> {new_entropy:.6f}")
        except Exception:
            pass
        
        # 3) èª¿æ•´å­¸ç¿’ç‡ï¼ˆå…¨éƒ¨ param groupï¼‰
        try:
            for pg in self.agent.optimizer.param_groups:  # type: ignore
                old_lr = pg.get('lr', 0.0)
                pg['lr'] = max(1e-6, old_lr * lr_decay)
            logger.info("ğŸ“‰ å·²èª¿æ•´å­¸ç¿’ç‡ (ä¹˜ä»¥ lr_decay_factor)")
        except Exception:
            pass
        
        # 4) æ¸…ç©ºè¨˜æ†¶ / 5) æ¸… optimizer state (åªä¿ç•™å¿…è¦çµæ§‹)
        try:
            if hasattr(self.agent, 'memory'):
                self.agent.memory.clear()
            # é‡å»º optimizer ä¾†æ¸…å‹•é‡
            opt_cls = type(self.agent.optimizer)
            self.agent.optimizer = opt_cls(self.agent.policy_net.parameters(), **self.agent.optimizer.defaults)  # type: ignore
        except Exception:
            pass
        
        # 6) æ›´æ–°å†·å»æ¨™è¨˜
        self._last_stagnation_handle_ep = int(current_episode)
        try:
            logger.info("âœ… åœæ»¯è™•ç†å®Œæˆï¼Œé€²å…¥å†·å»éšæ®µã€‚")
        except Exception:
            pass
        return

    # ------------------------------
    # Checkpoint I/O utilities
    # ------------------------------
    def save_checkpoint(self, filename: str):
        """ä¿å­˜æª¢æŸ¥é»åˆ° checkpoints/ ç›®éŒ„"""
        try:
            os.makedirs("checkpoints", exist_ok=True)
            checkpoint = {
                'model_state_dict': self.agent.policy_net.state_dict(),
                'optimizer_state_dict': self.agent.optimizer.state_dict(),
                'episode_rewards': self.episode_rewards,
                'win_rates': self.win_rates,
                'config': self.config,
                'model_architecture': {
                    'input_size': self.config['agent']['input_size'],
                    'hidden_size': self.config['agent']['hidden_size'],
                    'num_layers': self.config['agent']['num_layers'],
                },
                'save_timestamp': datetime.now().isoformat(),
                'pytorch_version': str(torch.__version__),
            }
            path = f"checkpoints/{filename}"
            tmp_path = path + ".tmp"
            # å…ˆå¯«åˆ°è‡¨æ™‚æª”å†åŸå­æ›¿æ›ï¼Œé¿å…ç”¢ç”ŸåŠå¯«å…¥çš„å£æª”
            torch.save(checkpoint, tmp_path)
            os.replace(tmp_path, path)
            size_mb = os.path.getsize(path) / (1024 * 1024)
            logger.info(f"âœ… å·²ä¿å­˜æª¢æŸ¥é»: {path} ({size_mb:.2f} MB)")
        except Exception as e:
            logger.error(f"ä¿å­˜æª¢æŸ¥é»å¤±æ•—: {e}")

    def load_checkpoint(self, checkpoint_path: str) -> bool:
        """è¼‰å…¥æª¢æŸ¥é»ä¸¦æ¢å¾©è¨“ç·´ç‹€æ…‹"""
        try:
            if not os.path.exists(checkpoint_path):
                logger.error(f"æª¢æŸ¥é»ä¸å­˜åœ¨: {checkpoint_path}")
                return False

            logger.info(f"è¼‰å…¥æª¢æŸ¥é»: {checkpoint_path}")
            # å˜—è©¦æ¨™æº–è¼‰å…¥
            try:
                checkpoint = torch.load(checkpoint_path, map_location=self.agent.device)
            except Exception as e:
                logger.warning(f"æ¨™æº–è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦åªè¼‰å…¥æ¬Šé‡: {e}")
                checkpoint = torch.load(checkpoint_path, map_location='cpu')

            # è§£ææ¬Šé‡
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                # å…¼å®¹ç›´æ¥ä¿å­˜state_dictçš„æƒ…æ³
                state_dict = checkpoint if isinstance(checkpoint, dict) else {}

            missing, unexpected = self.agent.policy_net.load_state_dict(state_dict, strict=False)
            if missing:
                logger.warning(f"è¼‰å…¥æ™‚ç¼ºå°‘éµ(å‰è‹¥å¹²): {missing[:5]}")
            if unexpected:
                logger.warning(f"è¼‰å…¥æ™‚æœªä½¿ç”¨éµ(å‰è‹¥å¹²): {unexpected[:5]}")
            logger.info("âœ… æ¨¡å‹æ¬Šé‡è¼‰å…¥æˆåŠŸ")

            # å„ªåŒ–å™¨
            if isinstance(checkpoint, dict) and 'optimizer_state_dict' in checkpoint:
                try:
                    self.agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    logger.info("âœ… å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥å¤±æ•—: {e}")

            # è¨“ç·´æ­·å²
            if isinstance(checkpoint, dict):
                self.episode_rewards = checkpoint.get('episode_rewards', self.episode_rewards)
                self.win_rates = checkpoint.get('win_rates', self.win_rates)

            logger.info("ğŸ‰ æª¢æŸ¥é»è¼‰å…¥å®Œæˆï¼")
            return True
        except Exception as e:
            logger.error(f"è¼‰å…¥æª¢æŸ¥é»æ™‚å‡ºéŒ¯: {e}")
            return False

    # =========================
    # [CHANGED] ä¸»è¨“ç·´æµç¨‹
    # =========================

    def train_parallel(self):
        cfg_t = self.config.get('training', {})
        num_workers = int(cfg_t.get('num_workers', max(1, (mp.cpu_count() or 2) - 1)))
        episodes_per_update = int(cfg_t.get('episodes_per_update', 16))  # æ¯æ¬¡æƒ³è™•ç†å¤šå°‘å±€ï¼ˆåƒ…åšç¯€å¥æ§åˆ¶ï¼‰
        max_episodes = int(cfg_t.get('max_episodes', 100000))
        eval_frequency = int(cfg_t.get('eval_frequency', 200))
        eval_games = int(cfg_t.get('eval_games', 30))
        win_scale = float(cfg_t.get('win_reward_scaling', 1.0))
        loss_scale = float(cfg_t.get('loss_penalty_scaling', 1.0))
        danger_scale = float(self.config.get('agent', {}).get('tactical_bonus', 0.1))
        min_batch_size = int(self.agent.config.get('min_batch_size', 512))
        visualize_every = int(cfg_t.get('visualize_every', 100))

        inflight_multiplier = int(cfg_t.get('inflight_multiplier', 2))  # å»ºè­° 1~3
        target_inflight = max(1, num_workers * inflight_multiplier)

        try:
            mp_ctx = mp.get_context('spawn')
        except ValueError:
            mp_ctx = mp

        pool = mp_ctx.Pool(
            processes=num_workers,
            initializer=_worker_init_persistent,
            initargs=(self.config['agent'],)
        )

        rng = random.Random()
        best_score = -1.0
        episodes_done_total = len(self.episode_rewards)

        # ç‰ˆæœ¬æ§åˆ¶
        policy_version = 0
        pending_weight_tasks = 0  # æœ¬ç‰ˆæœ¬é‚„éœ€è¦é€å‡ºå¹¾å€‹å¸¶æ¬Šé‡çš„ä»»å‹™ï¼ˆåˆå§‹ 0ï¼Œç”±é¦–æ¬¡æ›´æ–°å¾Œè§¸ç™¼ï¼‰

        def _make_args(send_weights: bool):
            weights_np = None
            if send_weights:
                # åƒ…ç•¶éœ€è¦å»£æ’­æ–°ç‰ˆæœ¬æ™‚æ‰åºåˆ—åŒ–ä¸€æ¬¡
                weights_np = {k: v.detach().cpu().numpy()
                            for k, v in self.agent.policy_net.state_dict().items()}
            return {
                'policy_version': policy_version,
                'policy_state': weights_np if send_weights else None,
                'player2_training_prob': self.player2_training_prob,
                'seed': rng.randrange(2**31 - 1),
            }

        # å»ºç«‹åˆå§‹ in-flight ä»»å‹™
        in_flight = []
        # ç¬¬ä¸€æ¬¡ä¹Ÿéœ€è¦æŠŠç‰ˆæœ¬ 0 çš„æ¬Šé‡é€çµ¦æ‰€æœ‰ worker
        pending_weight_tasks = num_workers
        for _ in range(target_inflight):
            send_w = pending_weight_tasks > 0
            if send_w:
                pending_weight_tasks -= 1
            ar = pool.apply_async(_worker_play_one, (_make_args(send_w),))
            in_flight.append(ar)

        logger.info(f"ğŸš€ å®Œå…¨æµæ°´ç·šè¨“ç·´é–‹å§‹ï¼šworkers={num_workers}, target_inflight={target_inflight}")

        steps_since_update = 0
        try:
            while episodes_done_total < max_episodes:
                # è¼•é‡è¼ªè©¢å®Œæˆçš„ä»»å‹™ï¼ˆé¿å… busy-waitï¼‰
                i = 0
                while i < len(in_flight):
                    ar = in_flight[i]
                    if ar.ready():
                        # å–çµæœä¸¦è™•ç†
                        res = ar.get()
                        # å¾ in_flight ç§»é™¤ï¼ˆäº¤æ›åˆªé™¤é¿å… O(n)ï¼‰
                        in_flight[i] = in_flight[-1]
                        in_flight.pop()
                        # ç«‹åˆ»è£œä¸Šä¸€å€‹æ–°ä»»å‹™ï¼ˆä¿æŒç®¡ç·šæ»¿ï¼‰
                        send_w = pending_weight_tasks > 0
                        if send_w:
                            pending_weight_tasks -= 1
                        in_flight.append(pool.apply_async(_worker_play_one, (_make_args(send_w),)))

                        # === consume result ===
                        err_msg = res.get('error') if isinstance(res, dict) else None
                        transitions = res.get('transitions', []) if isinstance(res, dict) else []
                        if not transitions:
                            self.episode_rewards.append(0.0)
                            if err_msg:
                                logger.debug(f"worker episode returned empty transitions: {err_msg}")
                        else:
                            player_result = int(res.get('player_result', 0))
                            ep_reward_sum = 0.0
                            last_idx = len(transitions) - 1
                            for idx, tr in enumerate(transitions):
                                state = tr['state']
                                action = int(tr['action'])
                                prob = float(tr['prob'])
                                reward = 0.0
                                if idx == last_idx:
                                    reward += (1.0 * win_scale) if player_result == 1 else (
                                            -1.0 * loss_scale if player_result == -1 else 0.0)
                                if tr.get('is_dangerous', False):
                                    reward += -10.0 * danger_scale
                                ep_reward_sum += reward
                                done = (idx == last_idx)
                                self.agent.store_transition(state, action, prob, reward, done)
                                steps_since_update += 1
                            self.episode_rewards.append(ep_reward_sum)

                            # ä¾ä½ å–œæ­¡çš„ç¯€å¥ï¼šæ­¥æ•¸é”åˆ°å°±å³åˆ»æ›´æ–°
                            if steps_since_update >= min_batch_size:
                                info = self.agent.update_policy()
                                if info is not None:
                                    self.training_losses.append(info.get('total_loss', 0.0))
                                    policy_version += 1
                                    # æ–°ç‰ˆæœ¬å‡ºçˆ â†’ è‡³å°‘å»£æ’­ num_workers ä»½å¸¶æ¬Šé‡ä»»å‹™
                                    pending_weight_tasks += num_workers
                                steps_since_update = 0

                        episodes_done_total += 1

                        # é€±æœŸæ€§è©•ä¼° / è¦–è¦ºåŒ–ï¼ˆä¸è¦å¡å¤ªä¹…â€”ä¿æŒå¿«é€Ÿï¼‰
                        if eval_frequency > 0 and episodes_done_total % eval_frequency == 0:
                            metrics = self.evaluate_comprehensive(games=eval_games)
                            score = float(metrics.get('comprehensive_score', 0.0))
                            self.win_rates.append(score)
                            try:
                                self.agent.scheduler.step(score)
                            except Exception:
                                pass
                            logger.info(
                                f"ğŸ“ˆ Eps={episodes_done_total} | Score={score:.3f} | "
                                f"self={metrics.get('self_play', 0):.3f} minimax={metrics.get('vs_minimax', 0):.3f} rand={metrics.get('vs_random', 0):.3f}"
                            )
                            try:
                                if self._detect_convergence_stagnation(episodes_done_total, score):
                                    self._handle_convergence_stagnation(episodes_done_total)
                            except Exception:
                                pass
                            if score > best_score:
                                best_score = score
                                self.save_checkpoint(f"best_model_wr_{best_score:.3f}.pt")

                        if visualize_every > 0 and episodes_done_total % visualize_every == 0:
                            try:
                                quick_games = max(5, int(eval_games // 2))
                                metrics_v = self.evaluate_comprehensive(games=quick_games)
                                score_v = float(metrics_v.get('comprehensive_score', 0.0))
                                logger.info(
                                    f"ğŸ¯ è¦–è¦ºåŒ–è©•ä¼° Eps={episodes_done_total} | Score={score_v:.3f} | "
                                    f"self={metrics_v.get('self_play', 0):.3f} minimax={metrics_v.get('vs_minimax', 0):.3f} rand={metrics_v.get('vs_random', 0):.3f}"
                                )
                            except Exception as ee:
                                logger.warning(f"è¦–è¦ºåŒ–å‰è©•ä¼°å¤±æ•—ï¼š{ee}")
                            try:
                                self.visualize_training_game(episodes_done_total, save_dir='videos', opponent='tactical', fps=2)
                            except Exception as ve:
                                logger.warning(f"å¯è¦–è¦ºåŒ–å¤±æ•—ï¼š{ve}")

                        # ä¸éå¢ iï¼Œå› ç‚ºæˆ‘å€‘æŠŠæœ«å°¾å…ƒç´ æ¬åˆ° i äº†ï¼›ç¹¼çºŒæª¢æŸ¥æ–°çš„ in_flight[i]
                        continue

                    else:
                        i += 1

                # è‹¥ in_flight å› æ•…å°‘æ–¼ç›®æ¨™ï¼Œè£œè¶³ï¼ˆç†è«–ä¸Šä¸æœƒç™¼ç”Ÿï¼Œä½†ä¿éšªï¼‰
                while len(in_flight) < target_inflight:
                    send_w = pending_weight_tasks > 0
                    if send_w:
                        pending_weight_tasks -= 1
                    in_flight.append(pool.apply_async(_worker_play_one, (_make_args(send_w),)))
                import time
                # å°ç¡ä¸€ä¸‹ï¼Œé™ä½ busy-waitï¼ˆä¸å½±éŸ¿ååï¼‰
                time.sleep(0.002)

        finally:
            try:
                pool.close(); pool.join()
            except Exception:
                pass

        logger.info("âœ… å®Œå…¨æµæ°´ç·šè¨“ç·´å®Œæˆ")
        return self.agent

    def _record_game_frames(self, opponent: str = 'tactical', max_moves: int = 50):
        """éŠç©ä¸€å±€ä¸¦è¨˜éŒ„æ¯æ­¥çš„æ£‹ç›¤å½±æ ¼ï¼ˆ6x7æ•¸çµ„ï¼‰ã€‚"""
        frames = []
        try:
            env = make('connectx', debug=False)
            env.reset()
            moves = 0
            with torch.no_grad():
                # è¨˜éŒ„åˆå§‹æ£‹ç›¤ï¼ˆè‹¥å¯ï¼‰
                try:
                    board, _ = self.agent.extract_board_and_mark(env.state, 0)
                    frames.append(self._flat_to_2d(board))
                except Exception:
                    pass
                while not env.done and moves < max_moves:
                    actions = []
                    for p in range(2):
                        if env.state[p]['status'] != 'ACTIVE':
                            actions.append(0)
                            continue
                        board, mark = self.agent.extract_board_and_mark(env.state, p)
                        valid = self.agent.get_valid_actions(board)
                        if p == 0:
                            # æˆ‘æ–¹ç”¨ç­–ç•¥ + å®‰å…¨æª¢æŸ¥
                            a = self._choose_policy_with_tactics(board, mark, valid, training=False)
                        else:
                            if opponent == 'tactical':
                                a = self._tactical_random_opening_agent(board, mark, valid)
                            elif opponent == 'minimax':
                                a = self._choose_minimax_move(board, mark, max_depth=int(self.config.get('evaluation', {}).get('minimax_depth', 3)))
                            else:
                                # random family uses the same unified logic
                                a = self._tactical_random_opening_agent(board, mark, valid)
                        actions.append(int(a))
                    try:
                        env.step(actions)
                    except Exception:
                        break
                    moves += 1
                    try:
                        board, _ = self.agent.extract_board_and_mark(env.state, 0)
                        frames.append(self._flat_to_2d(board))
                    except Exception:
                        pass
        except Exception:
            pass
        return frames

    def visualize_training_game(self, episode_idx: int, save_dir: str = 'videos', opponent: str = 'tactical', fps: int = 2):
        """å°‡ä¸€å±€å¯è¦–åŒ–ä¸¦è¼¸å‡ºç‚ºå½±ç‰‡ï¼ˆå¼·åˆ¶ä½¿ç”¨FFmpeg MP4ï¼‰ã€‚è¿”å›è¼¸å‡ºè·¯å¾‘æˆ–Noneã€‚"""
        if not globals().get('VISUALIZATION_AVAILABLE', False):
            logger.info("æœªå®‰è£å¯è¦–åŒ–ä¾è³´ï¼Œç•¥éå½±ç‰‡è¼¸å‡ºã€‚")
            return None
        try:
            import shutil
            import matplotlib
            import matplotlib.pyplot as plt
            from matplotlib import animation
            from matplotlib.patches import Patch
            # Configure ffmpeg path from system PATH. Force MP4 via FFmpeg.
            ffmpeg_path = shutil.which('ffmpeg')
            if not ffmpeg_path:
                raise RuntimeError("ffmpeg not found in PATH. Please install ffmpeg to export MP4.")
            matplotlib.rcParams['animation.ffmpeg_path'] = ffmpeg_path
        except Exception as e:
            logger.warning(f"è¼‰å…¥matplotlib/ffmpegå¤±æ•—ï¼Œç•¥éå½±ç‰‡è¼¸å‡º: {e}")
            return None

        frames = self._record_game_frames(opponent=opponent)
        if not frames:
            logger.info("ç„¡å¯è¦–åŒ–å½±æ ¼ï¼Œç•¥éå½±ç‰‡è¼¸å‡ºã€‚")
            return None

        # è®“æœ€å¾Œä¸€å¹€å¤šåœç•™ä¸€æœƒï¼ˆç´„2ç§’ï¼‰
        try:
            hold_frames = max(1, int(fps * 2))  # åœç•™ 2 ç§’
            frames = frames + [frames[-1]] * hold_frames
        except Exception:
            pass

        # æº–å‚™è¼¸å‡ºç›®éŒ„èˆ‡æª”å
        os.makedirs(save_dir, exist_ok=True)
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        mp4_path = os.path.join(save_dir, f"episode_{episode_idx}_{ts}.mp4")
        # gif_path ä¿ç•™ä½†ä¸ä½¿ç”¨ï¼Œè‹¥éœ€å›é€€å¯å•Ÿç”¨
        gif_path = os.path.join(save_dir, f"episode_{episode_idx}_{ts}.gif")

        # å»ºç«‹ç•«å¸ƒèˆ‡åº§æ¨™è»¸
        fig, ax = plt.subplots(figsize=(5.6, 4.8))
        ax.set_xlim(-0.5, 6.5)
        ax.set_ylim(-0.5, 5.5)
        ax.set_xticks(range(7))
        ax.set_yticks(range(6))
        ax.grid(True)
        ax.set_title(f"Episode {episode_idx} vs {opponent}")

        # æ–°å¢åœ–ä¾‹èªªæ˜é¡è‰²: ç´…=Agent(P1), é‡‘=Opponent(P2)
        handles = [
            Patch(color='red', label='Agent (P1)'),
            Patch(color='gold', label='Opponent (P2)')
        ]
        ax.legend(handles=handles, loc='upper right', framealpha=0.9)

        # åˆå§‹åŒ–åœ“ç‰‡é›†åˆ
        discs = []
        def draw_board(grid):
            # æ¸…é™¤å…ˆå‰åœ“ç‰‡
            for d in discs:
                d.remove()
            discs.clear()
            # ç¹ªè£½æ£‹å­ï¼šgrid[r][c] 1->ç´…, 2->é»ƒ
            for r in range(6):
                for c in range(7):
                    v = grid[r][c]
                    if v == 0:
                        continue
                    color = 'red' if v == 1 else 'gold'
                    circle = plt.Circle((c, 5 - r), 0.4, color=color)
                    ax.add_patch(circle)
                    discs.append(circle)

        def init():
            draw_board(frames[0])
            return discs

        def update(i):
            draw_board(frames[i])
            return discs

        anim = animation.FuncAnimation(fig, update, init_func=init, frames=len(frames), interval=int(1000 / max(1, fps)), blit=False)

        out_path = None
        # å¼·åˆ¶ä½¿ç”¨ FFmpeg è¼¸å‡º MP4ï¼ˆå¦‚æœå¤±æ•—å‰‡ä¸å›é€€ GIFï¼‰
        try:
            writer = animation.FFMpegWriter(fps=fps, codec='mpeg4', bitrate=1800, extra_args=['-pix_fmt', 'yuv420p'])
            anim.save(mp4_path, writer=writer)
            out_path = mp4_path
        except Exception as e:
            logger.warning(f"ä½¿ç”¨FFmpegè¼¸å‡ºMP4å¤±æ•—: {e}")
            out_path = None
        finally:
            plt.close(fig)

        if out_path:
            logger.info(f"ğŸ¬ å·²è¼¸å‡ºè¨“ç·´å°å±€å½±ç‰‡: {out_path}")
        return out_path
def main():
    """Main entry: load config, resume from latest checkpoint, train, notify."""
    # Local imports to keep global scope clean
    from datetime import datetime

    # --- helpers ---
    def parse_ts_from_name(fname: str):
        FINAL_RE = re.compile(r"^final_(\d{8})_(\d{6})\.pt$")
        TS_TAIL_RE = re.compile(r"_(\d{8})_(\d{6})\.pt$")
        m = FINAL_RE.match(fname)
        if m:
            try:
                return datetime.strptime(m.group(1) + m.group(2), "%Y%m%d%H%M%S")
            except Exception:
                return None
        m = TS_TAIL_RE.search(fname)
        if m:
            try:
                return datetime.strptime(m.group(1) + m.group(2), "%Y%m%d%H%M%S")
            except Exception:
                return None
        return None

    def choose_latest_checkpoint_by_name(files):
        latest = None
        latest_time = None
        for p in files:
            fname = os.path.basename(p)
            ts = parse_ts_from_name(fname)
            try:
                t = ts.timestamp() if ts is not None else os.path.getmtime(p)

            except OSError:
                t = 0
            if latest is None or t > latest_time:
                latest = p
                latest_time = t
        return latest

    def find_working_checkpoint(files):
        """å›å‚³ç¬¬ä¸€å€‹å¯æˆåŠŸ torch.load çš„æª¢æŸ¥é»ï¼ˆä¾æ™‚é–“æ–°â†’èˆŠï¼‰ã€‚è‹¥æ²’æœ‰å‰‡å›å‚³ Noneã€‚"""
        def _mtime(p):
            try:
                ts = parse_ts_from_name(os.path.basename(p))
                return ts.timestamp() if ts else os.path.getmtime(p)
            except Exception:
                return 0
        for p in sorted(files, key=_mtime, reverse=True):
            try:
                # å…ˆå¿«é€Ÿå˜—è©¦è®€å–ï¼Œåƒ…ç‚ºé©—è­‰æª”æ¡ˆçµæ§‹ï¼Œä¸éœ€çœŸæ­£å¥—ç”¨
                _ = torch.load(p, map_location='cpu')
                logger.info(f"æª¢æŸ¥é»å¯ç”¨ï¼š{p}")
                return p
            except Exception as e:
                logger.warning(f"ç•¥éä¸å¯ç”¨æª¢æŸ¥é» {p}: {e}")
                continue
       
        return None

    def send_telegram(msg: str):
        token = os.getenv("TELEGRAM_BOT_TOKEN") or os.getenv("BOT_TOKEN")
        chat_id = os.getenv("TELEGRAM_CHAT_ID") or "6166024220"
        if not token or not chat_id:
            logger.info("æœªè¨­ç½® TELEGRAM_BOT_TOKEN/TELEGRAM_CHAT_IDï¼Œç•¥éè¨Šæ¯é€šçŸ¥ã€‚")
            return
        try:
            base = f"https://api.telegram.org/bot{token}/sendMessage"

            logger.info("å·²ç™¼é€ Telegram é€šçŸ¥ã€‚")
        except Exception as e:
            logger.warning(f"Telegram ç™¼é€å¤±æ•—: {e}")

    # --- choose config ---
    cfg_path = "config.yaml"
    if not os.path.exists(cfg_path):
        alt_path = "connectx_config.yaml"
        if os.path.exists(alt_path):
            cfg_path = alt_path
        else:
            logger.error("æ‰¾ä¸åˆ° config.yaml æˆ– connectx_config.yamlï¼Œç„¡æ³•å•Ÿå‹•è¨“ç·´ã€‚")
            send_telegram("âš ï¸ æ‰¾ä¸åˆ°è¨­å®šæª”ï¼Œè¨“ç·´æœªå•Ÿå‹•ã€‚")
            return

    trainer = ConnectXTrainer(cfg_path)

    # --- resume from checkpoint ---
    import glob
    resume_from_env = os.getenv("CHECKPOINT_PATH") or os.getenv("RESUME_FROM")
    ckpt_to_load = None
    if resume_from_env and os.path.exists(resume_from_env):
        ckpt_to_load = resume_from_env
    else:
        files = glob.glob(os.path.join('checkpoints', '*.pt'))
       
        # å„ªå…ˆé¸æ“‡å¯æˆåŠŸè¢« torch.load çš„æœ€æ–°æª”ï¼Œé¿å…è®€åˆ°åŠå¯«å…¥å£æª”
        ckpt_to_load = find_working_checkpoint(files)
        if ckpt_to_load is None:
            # é€€è€Œæ±‚å…¶æ¬¡é¸åç¨±/mtime æœ€æ–°è€…
            ckpt_to_load = choose_latest_checkpoint_by_name(files)

    logger.info(f"ckpt_to_load: {ckpt_to_load}")

    if ckpt_to_load:
        loaded = trainer.load_checkpoint(ckpt_to_load)
        if not loaded:
            logger.warning(f"ç„¡æ³•è¼‰å…¥æª¢æŸ¥é»: {ckpt_to_load}ï¼Œå°‡ä»¥éš¨æ©Ÿåˆå§‹åŒ–é–‹å§‹ã€‚")
    else:
        logger.info("æœªæ‰¾åˆ°å¯ç”¨æª¢æŸ¥é»ï¼Œå°‡ä»¥éš¨æ©Ÿåˆå§‹åŒ–é–‹å§‹è¨“ç·´ã€‚")

    # --- train ---
    start_ts = datetime.now()
    err = None
    try:
        training_cfg = trainer.config.get('training', {})
        use_parallel = bool(training_cfg.get('parallel_rollout', True))
        if use_parallel:
            logger.info("ä½¿ç”¨å¹³è¡Œè’é›†é€²è¡Œè¨“ç·´ train_parallel()")
            trainer.train_parallel()
        else:
            logger.info("ä½¿ç”¨å–®åŸ·è¡Œç·’è¨“ç·´ train()")
           
            trainer.train()
    except Exception as e:
        err = e
        logger.error(f"è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}\n{traceback.format_exc()}")
    finally:
        # å„²å­˜æœ€å¾Œæª¢æŸ¥é»
        ts_tag = datetime.now().strftime("%Y%m%d_%H%M%S")
        try:
            trainer.save_checkpoint(f"final_{ts_tag}.pt")
        except Exception as se:
            logger.warning(f"ä¿å­˜æœ€çµ‚æª¢æŸ¥é»å¤±æ•—: {se}")

    # --- notify ---

    elapsed = datetime.now() - start_ts
    episodes_done = len(trainer.episode_rewards)
    last_wr = trainer.win_rates[-1] if trainer.win_rates else None
    if err is None:
        msg = f"ğŸ‰ è¨“ç·´å®Œç•¢ï¼\nå›åˆæ•¸: {episodes_done}\næœ€å¾Œå‹ç‡: {last_wr if last_wr is not None else 'N/A'}\nè€—æ™‚: {str(elapsed).split('.')[0]}"
    else:
        msg = f"âš ï¸ è¨“ç·´å¤±æ•—: {err}\nå›åˆæ•¸: {episodes_done}\nè€—æ™‚: {str(elapsed).split('.')[0]}"
    send_telegram(msg)

if __name__ == "__main__":
    main()
