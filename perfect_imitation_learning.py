#!/usr/bin/env python3
"""
Connect4 å¼·åŒ–æ¨¡ä»¿å­¸ç¿’ç³»çµ± - å®Œå…¨é‡å¯«ç‰ˆæœ¬
ç¢ºä¿æ¨¡å‹100%å­¸æœƒC4Solverçš„å®Œæ•´ç­–ç•¥
"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import yaml
import logging
from collections import deque, defaultdict
from datetime import datetime
from typing import List, Dict, Tuple, Set
import time
import json

# å°å…¥å¿…è¦çš„çµ„ä»¶
from train_connectx_rl_robust import ConnectXNet, PPOAgent, flat_to_2d, is_winning_move, find_drop_row, is_win_from
from c4solver_wrapper import get_c4solver, C4SolverWrapper
from kaggle_environments import make

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('enhanced_imitation_learning.log')
    ]
)
logger = logging.getLogger(__name__)

class SystematicPositionGenerator:
    """ç³»çµ±åŒ–å±€é¢ç”Ÿæˆå™¨ - ç¢ºä¿è¦†è“‹æ‰€æœ‰é¡å‹çš„å±€é¢"""
    
    def __init__(self, solver: C4SolverWrapper):
        self.solver = solver
        self.generated_positions = set()  # é¿å…é‡è¤‡å±€é¢
        self.position_stats = defaultdict(int)  # çµ±è¨ˆä¸åŒæ·±åº¦çš„å±€é¢æ•¸é‡
        
    def _position_to_key(self, board: List[int]) -> str:
        """å°‡å±€é¢è½‰æ›ç‚ºå”¯ä¸€éµå€¼"""
        return ''.join(map(str, board))
    
    def generate_systematic_positions(self, target_per_depth: int = 500) -> List[Tuple[List[int], int]]:
        """ç³»çµ±åŒ–ç”Ÿæˆä¸åŒæ·±åº¦çš„å±€é¢"""
        positions = []
        
        logger.info("ğŸ¯ ç³»çµ±åŒ–ç”Ÿæˆè¨“ç·´å±€é¢...")
        
        # 0. ç©ºå±€é¢ (é‡è¦!)
        empty_board = [0] * 42
        positions.append((empty_board.copy(), 1))
        positions.append((empty_board.copy(), 2))
        logger.info("âœ… æ·»åŠ ç©ºå±€é¢")
        
        # 1. é–‹å±€å±€é¢ (1-6æ­¥)
        logger.info("ç”Ÿæˆé–‹å±€å±€é¢ (1-6æ­¥)...")
        opening_positions = self._generate_opening_positions(target_per_depth // 3)
        positions.extend(opening_positions)
        logger.info(f"âœ… ç”Ÿæˆ {len(opening_positions)} å€‹é–‹å±€å±€é¢")
        
        # 2. ä¸­å±€å±€é¢ (7-20æ­¥)
        logger.info("ç”Ÿæˆä¸­å±€å±€é¢ (7-20æ­¥)...")
        midgame_positions = self._generate_midgame_positions(target_per_depth // 2)
        positions.extend(midgame_positions)
        logger.info(f"âœ… ç”Ÿæˆ {len(midgame_positions)} å€‹ä¸­å±€å±€é¢")
        
        # 3. çµ‚å±€å±€é¢ (21+æ­¥)
        logger.info("ç”Ÿæˆçµ‚å±€å±€é¢ (21+æ­¥)...")
        endgame_positions = self._generate_endgame_positions(target_per_depth // 3)
        positions.extend(endgame_positions)
        logger.info(f"âœ… ç”Ÿæˆ {len(endgame_positions)} å€‹çµ‚å±€å±€é¢")
        
        # 4. æˆ°è¡“å±€é¢ (æœ‰å¨è„…çš„å±€é¢)
        logger.info("ç”Ÿæˆæˆ°è¡“å±€é¢...")
        tactical_positions = self._generate_tactical_positions(target_per_depth // 4)
        positions.extend(tactical_positions)
        logger.info(f"âœ… ç”Ÿæˆ {len(tactical_positions)} å€‹æˆ°è¡“å±€é¢")
        
        # å»é‡
        unique_positions = []
        seen_keys = set()
        for board, player in positions:
            key = self._position_to_key(board) + f"_p{player}"
            if key not in seen_keys:
                seen_keys.add(key)
                unique_positions.append((board, player))
        
        logger.info(f"ğŸ¯ ç¸½å…±ç”Ÿæˆ {len(unique_positions)} å€‹ç¨ç‰¹å±€é¢")
        return unique_positions
    
    def _generate_opening_positions(self, target_count: int) -> List[Tuple[List[int], int]]:
        """ç”Ÿæˆé–‹å±€å±€é¢ (1-6æ­¥)"""
        positions = []
        
        # é‡è¦çš„é–‹å±€æ¨¡å¼
        important_openings = [
            "4",      # ä¸­å¤®é–‹å±€
            "43",     # ä¸­å¤® + åç§»
            "44",     # ä¸­å¤®ç–ŠåŠ 
            "443",    # ç¶“å…¸é–‹å±€
            "4433",   # å°ç¨±é–‹å±€
            "4343",   # äº¤æ›¿ä¸­å¤®
            "3",      # å·¦ä¸­é–‹å±€
            "34",     # å·¦ä¸­è½‰ä¸­å¤®
            "343",    # æˆ°è¡“æ€§é–‹å±€
            "3423",   # è¤‡é›œé–‹å±€
        ]
        
        for opening in important_openings:
            try:
                board = self._apply_move_sequence(opening)
                if board:
                    # æ·»åŠ å…©å€‹ç©å®¶çš„è¦–è§’
                    current_player = (len(opening) % 2) + 1
                    positions.append((board, current_player))
                    positions.append((board, 3 - current_player))
            except Exception as e:
                logger.warning(f"é–‹å±€åºåˆ— {opening} å¤±æ•—: {e}")
                continue
        
        # éš¨æ©Ÿé–‹å±€è®ŠåŒ–
        for _ in range(target_count - len(positions)):
            try:
                depth = random.randint(1, 6)
                board = self._generate_random_game(max_moves=depth)
                if board and sum(1 for x in board if x != 0) == depth:
                    current_player = (depth % 2) + 1
                    positions.append((board, current_player))
            except Exception:
                continue
                
        return positions[:target_count]
    
    def _generate_midgame_positions(self, target_count: int) -> List[Tuple[List[int], int]]:
        """ç”Ÿæˆä¸­å±€å±€é¢ (7-20æ­¥)"""
        positions = []
        
        for _ in range(target_count):
            try:
                depth = random.randint(7, 20)
                board = self._generate_random_game(max_moves=depth)
                if board and 7 <= sum(1 for x in board if x != 0) <= 20:
                    current_player = (depth % 2) + 1
                    positions.append((board, current_player))
            except Exception:
                continue
                
        return positions
    
    def _generate_endgame_positions(self, target_count: int) -> List[Tuple[List[int], int]]:
        """ç”Ÿæˆçµ‚å±€å±€é¢ (21+æ­¥)"""
        positions = []
        
        for _ in range(target_count):
            try:
                depth = random.randint(21, 35)
                board = self._generate_random_game(max_moves=depth)
                if board and sum(1 for x in board if x != 0) >= 21:
                    current_player = (depth % 2) + 1
                    positions.append((board, current_player))
            except Exception:
                continue
                
        return positions
    
    def _generate_tactical_positions(self, target_count: int) -> List[Tuple[List[int], int]]:
        """ç”Ÿæˆæœ‰æˆ°è¡“æ„ç¾©çš„å±€é¢ (æœ‰å¨è„…ã€éœ€è¦é˜²å®ˆç­‰)"""
        positions = []
        attempts = 0
        max_attempts = target_count * 10
        
        while len(positions) < target_count and attempts < max_attempts:
            attempts += 1
            try:
                # ç”Ÿæˆéš¨æ©Ÿå±€é¢
                depth = random.randint(8, 25)
                board = self._generate_random_game(max_moves=depth)
                if not board:
                    continue
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æˆ°è¡“æ„ç¾©
                player1_can_win = self._has_immediate_threat(board, 1)
                player2_can_win = self._has_immediate_threat(board, 2)
                has_multiple_threats = self._count_threats(board) >= 2
                
                if player1_can_win or player2_can_win or has_multiple_threats:
                    current_player = (depth % 2) + 1
                    positions.append((board, current_player))
                    
            except Exception:
                continue
        
        return positions
    
    def _apply_move_sequence(self, sequence: str) -> List[int]:
        """æ‡‰ç”¨ç§»å‹•åºåˆ—ç”Ÿæˆå±€é¢"""
        board = [0] * 42
        current_player = 1
        
        for move_char in sequence:
            col = int(move_char) - 1  # C4Solverä½¿ç”¨1-7ï¼Œæˆ‘å€‘ä½¿ç”¨0-6
            if col < 0 or col > 6:
                continue
                
            # æ‰¾åˆ°è©²åˆ—çš„è½é»
            grid = flat_to_2d(board)
            row = find_drop_row(grid, col)
            if row is None:
                return None  # è©²åˆ—å·²æ»¿
            
            board[row * 7 + col] = current_player
            current_player = 3 - current_player
        
        return board
    
    def _generate_random_game(self, max_moves: int) -> List[int]:
        """ç”Ÿæˆéš¨æ©Ÿå°å±€åˆ°æŒ‡å®šæ­¥æ•¸"""
        board = [0] * 42
        current_player = 1
        moves_made = 0
        
        while moves_made < max_moves:
            valid_actions = [c for c in range(7) if board[c] == 0]
            if not valid_actions:
                break
            
            col = random.choice(valid_actions)
            grid = flat_to_2d(board)
            row = find_drop_row(grid, col)
            if row is None:
                break
            
            board[row * 7 + col] = current_player
            
            # æª¢æŸ¥æ˜¯å¦éŠæˆ²çµæŸ
            if is_win_from(grid, row, col, current_player):
                break
                
            current_player = 3 - current_player
            moves_made += 1
        
        return board
    
    def _has_immediate_threat(self, board: List[int], player: int) -> bool:
        """æª¢æŸ¥ç©å®¶æ˜¯å¦æœ‰ç«‹å³ç²å‹å¨è„…"""
        valid_actions = [c for c in range(7) if board[c] == 0]
        for col in valid_actions:
            if is_winning_move(board, col, player):
                return True
        return False
    
    def _count_threats(self, board: List[int]) -> int:
        """è¨ˆç®—å±€é¢ä¸­çš„å¨è„…æ•¸é‡"""
        threats = 0
        for player in [1, 2]:
            if self._has_immediate_threat(board, player):
                threats += 1
        return threats

class PerfectExpertPolicy:
    """å®Œç¾å°ˆå®¶ç­–ç•¥ - ç›´æ¥ä½¿ç”¨C4Solverçš„æœ€å„ªæ±ºç­–"""
    
    def __init__(self, solver: C4SolverWrapper):
        self.solver = solver
        self.cache = {}  # ç·©å­˜C4Solverçµæœ
        
    def get_expert_policy(self, board: List[int], valid_actions: List[int]) -> np.ndarray:
        """ç²å–C4Solverçš„å®Œç¾ç­–ç•¥åˆ†ä½ˆ
        
        é‡è¦ï¼šä¸ä½¿ç”¨softmaxï¼ç›´æ¥ä½¿ç”¨C4Solverçš„æœ€å„ªå‹•ä½œä½œç‚ºone-hotåˆ†ä½ˆ
        """
        # å¿«é€Ÿç·©å­˜æŸ¥æ‰¾
        board_key = ''.join(map(str, board))
        if board_key in self.cache:
            return self.cache[board_key]
        
        try:
            # ç²å–C4Solveråˆ†æ
            result = self.solver.evaluate_board(board, analyze=True)
            
            if not result['valid'] or len(result['scores']) != 7:
                # Fallback: å‡å‹»åˆ†ä½ˆ
                policy = np.zeros(7)
                for action in valid_actions:
                    policy[action] = 1.0 / len(valid_actions)
                self.cache[board_key] = policy
                return policy
            
            scores = np.array(result['scores'])
            
            # æ‰¾å‡ºæœ‰æ•ˆå‹•ä½œä¸­çš„æœ€é«˜åˆ†æ•¸
            best_score = float('-inf')
            best_actions = []
            
            for action in valid_actions:
                if action < len(scores):
                    score = scores[action]
                    if score > best_score:
                        best_score = score
                        best_actions = [action]
                    elif score == best_score:
                        best_actions.append(action)
            
            # å‰µå»ºç­–ç•¥åˆ†ä½ˆï¼šæœ€å„ªå‹•ä½œç²å¾—100%æ¦‚ç‡ï¼Œå…¶ä»–ç‚º0
            policy = np.zeros(7)
            for action in best_actions:
                policy[action] = 1.0 / len(best_actions)
            
            self.cache[board_key] = policy
            return policy
            
        except Exception as e:
            logger.warning(f"Expert policy failed: {e}")
            # Fallbackç­–ç•¥
            policy = np.zeros(7)
            if valid_actions:
                # åå¥½ä¸­å¤®
                center_preference = [3, 4, 2, 5, 1, 6, 0]
                for col in center_preference:
                    if col in valid_actions:
                        policy[col] = 1.0
                        break
            
            self.cache[board_key] = policy
            return policy

class EnhancedImitationDataset:
    """å¢å¼·çš„æ¨¡ä»¿å­¸ç¿’æ•¸æ“šé›†"""
    
    def __init__(self, solver: C4SolverWrapper):
        self.solver = solver
        self.position_generator = SystematicPositionGenerator(solver)
        self.expert_policy = PerfectExpertPolicy(solver)
        self.training_samples = []
        
    def generate_comprehensive_dataset(self, positions_per_depth: int = 1000) -> List[Dict]:
        """ç”Ÿæˆå®Œæ•´çš„è¨“ç·´æ•¸æ“šé›†"""
        logger.info("ğŸ¯ é–‹å§‹ç”Ÿæˆå®Œæ•´è¨“ç·´æ•¸æ“šé›†...")
        
        # ç¬¬ä¸€æ­¥ï¼šç³»çµ±åŒ–ç”Ÿæˆå±€é¢
        positions = self.position_generator.generate_systematic_positions(positions_per_depth)
        logger.info(f"âœ… ç”Ÿæˆäº† {len(positions)} å€‹è¨“ç·´å±€é¢")
        
        # ç¬¬äºŒæ­¥ï¼šç‚ºæ¯å€‹å±€é¢ç”Ÿæˆå°ˆå®¶ç­–ç•¥
        samples = []
        failed_count = 0
        
        for i, (board, current_player) in enumerate(positions):
            if i % 500 == 0:
                logger.info(f"è™•ç†é€²åº¦: {i}/{len(positions)} ({100*i/len(positions):.1f}%)")
            
            try:
                # æª¢æŸ¥éŠæˆ²æ˜¯å¦å·²çµæŸ
                if self._is_game_over(board):
                    continue
                
                # ç²å–æœ‰æ•ˆå‹•ä½œ
                valid_actions = [c for c in range(7) if board[c] == 0]
                if not valid_actions:
                    continue
                
                # ç·¨ç¢¼ç‹€æ…‹
                encoded_state = self._encode_state(board, current_player)
                
                # ç²å–å°ˆå®¶ç­–ç•¥
                expert_policy = self.expert_policy.get_expert_policy(board, valid_actions)
                
                # é©—è­‰ç­–ç•¥åˆç†æ€§
                if np.sum(expert_policy) == 0:
                    failed_count += 1
                    continue
                
                sample = {
                    'state': encoded_state,
                    'policy': expert_policy,
                    'board': board.copy(),
                    'player': current_player,
                    'valid_actions': valid_actions.copy(),
                    'move_count': sum(1 for x in board if x != 0)
                }
                
                samples.append(sample)
                
            except Exception as e:
                failed_count += 1
                if failed_count < 10:  # åªè¨˜éŒ„å‰å¹¾å€‹éŒ¯èª¤
                    logger.warning(f"æ¨£æœ¬ç”Ÿæˆå¤±æ•—: {e}")
                continue
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(samples)} å€‹è¨“ç·´æ¨£æœ¬")
        logger.info(f"âš ï¸ å¤±æ•— {failed_count} å€‹æ¨£æœ¬")
        
        # åˆ†ææ•¸æ“šé›†çµ±è¨ˆ
        self._analyze_dataset(samples)
        
        return samples
    
    def _is_game_over(self, board: List[int]) -> bool:
        """æª¢æŸ¥éŠæˆ²æ˜¯å¦çµæŸ"""
        grid = flat_to_2d(board)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰äººç²å‹
        for r in range(6):
            for c in range(7):
                if board[r*7 + c] != 0:
                    if is_win_from(grid, r, c, board[r*7 + c]):
                        return True
        
        # æª¢æŸ¥æ˜¯å¦æ£‹ç›¤å·²æ»¿
        return all(board[i] != 0 for i in range(7))
    
    def _encode_state(self, board: List[int], current_player: int) -> np.ndarray:
        """ç·¨ç¢¼ç‹€æ…‹ç‚ºæ¨¡å‹è¼¸å…¥"""
        try:
            # åŸºæœ¬æ£‹ç›¤ç·¨ç¢¼ (6*7*2 = 84ç¶­)
            encoded = np.zeros((6, 7, 2), dtype=np.float32)
            grid = flat_to_2d(board)
            
            for r in range(6):
                for c in range(7):
                    if grid[r][c] == 1:
                        encoded[r, c, 0] = 1
                    elif grid[r][c] == 2:
                        encoded[r, c, 1] = 1
            
            # ç•¶å‰ç©å®¶ç·¨ç¢¼ (1ç¶­)
            current_player_encoded = np.array([current_player - 1], dtype=np.float32)
            
            # æœ‰æ•ˆå‹•ä½œç·¨ç¢¼ (7ç¶­)
            valid_actions = [c for c in range(7) if board[c] == 0]
            valid_mask = np.zeros(7, dtype=np.float32)
            for action in valid_actions:
                valid_mask[action] = 1
            
            # ä½ç½®ç‰¹å¾µ (34ç¶­)
            position_features = np.zeros(34, dtype=np.float32)
            
            # çµ„åˆæ‰€æœ‰ç‰¹å¾µ (84 + 1 + 7 + 34 = 126ç¶­)
            combined = np.concatenate([
                encoded.flatten(),      # 84
                current_player_encoded, # 1  
                valid_mask,            # 7
                position_features      # 34
            ])
            
            return combined
            
        except Exception as e:
            logger.error(f"ç‹€æ…‹ç·¨ç¢¼å¤±æ•—: {e}")
            return np.zeros(126, dtype=np.float32)
    
    def _analyze_dataset(self, samples: List[Dict]):
        """åˆ†ææ•¸æ“šé›†çµ±è¨ˆä¿¡æ¯"""
        if not samples:
            return
        
        move_counts = [s['move_count'] for s in samples]
        player_counts = [s['player'] for s in samples]
        
        logger.info("ğŸ“Š æ•¸æ“šé›†çµ±è¨ˆ:")
        logger.info(f"  ç¸½æ¨£æœ¬æ•¸: {len(samples)}")
        logger.info(f"  ç§»å‹•æ­¥æ•¸åˆ†ä½ˆ: æœ€å°={min(move_counts)}, æœ€å¤§={max(move_counts)}, å¹³å‡={np.mean(move_counts):.1f}")
        logger.info(f"  ç©å®¶åˆ†ä½ˆ: P1={player_counts.count(1)}, P2={player_counts.count(2)}")
        
        # åˆ†æç­–ç•¥åˆ†ä½ˆ
        policy_entropies = []
        max_probs = []
        for sample in samples[:100]:  # åˆ†æå‰100å€‹æ¨£æœ¬
            policy = sample['policy']
            entropy = -np.sum(policy * np.log(policy + 1e-8))
            max_prob = np.max(policy)
            policy_entropies.append(entropy)
            max_probs.append(max_prob)
        
        logger.info(f"  ç­–ç•¥ç†µå€¼ç¯„åœ: {np.min(policy_entropies):.3f} - {np.max(policy_entropies):.3f}")
        logger.info(f"  æœ€å¤§æ¦‚ç‡ç¯„åœ: {np.min(max_probs):.3f} - {np.max(max_probs):.3f}")

class EnhancedImitationLearner:
    """å¢å¼·çš„æ¨¡ä»¿å­¸ç¿’è¨“ç·´å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # åˆå§‹åŒ–C4Solver
        self.solver = get_c4solver()
        if self.solver is None:
            raise RuntimeError("ç„¡æ³•åˆå§‹åŒ–C4Solver")
        
        # åˆå§‹åŒ–æ•¸æ“šé›†
        self.dataset = EnhancedImitationDataset(self.solver)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = ConnectXNet(
            input_size=config['model']['input_size'],
            hidden_size=config['model']['hidden_size'], 
            num_layers=config['model']['num_layers']
        ).to(self.device)
        
        # å„ªåŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay']
        )
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=8
        )
        
        # è¨“ç·´çµ±è¨ˆ
        self.training_stats = {
            'losses': [],
            'accuracies': [],
            'learning_rates': [],
            'policy_kl_divs': []
        }
        
    def train(self, positions_per_depth: int = 1000, num_epochs: int = 200, 
              batch_size: int = 64, val_split: float = 0.15, 
              save_path: str = "perfect_imitation_model.pt"):
        """ä¸»è¨“ç·´å¾ªç’°"""
        logger.info("ğŸš€ é–‹å§‹å®Œç¾æ¨¡ä»¿å­¸ç¿’è¨“ç·´...")
        
        # ç”Ÿæˆè¨“ç·´æ•¸æ“š
        all_samples = self.dataset.generate_comprehensive_dataset(positions_per_depth)
        
        if len(all_samples) < 100:
            raise RuntimeError(f"è¨“ç·´æ¨£æœ¬å¤ªå°‘: {len(all_samples)}")
        
        # åˆ†å‰²æ•¸æ“šé›†
        random.shuffle(all_samples)
        val_size = int(len(all_samples) * val_split)
        train_samples = all_samples[:-val_size] if val_size > 0 else all_samples
        val_samples = all_samples[-val_size:] if val_size > 0 else []
        
        logger.info(f"ğŸ“Š æ•¸æ“šé›†åˆ†å‰²: è¨“ç·´={len(train_samples)}, é©—è­‰={len(val_samples)}")
        
        best_val_loss = float('inf')
        patience_counter = 0
        max_patience = 15
        
        for epoch in range(num_epochs):
            start_time = time.time()
            
            # è¨“ç·´ä¸€å€‹epoch
            train_loss, train_acc, train_kl = self._train_epoch(train_samples, batch_size)
            
            # é©—è­‰
            val_loss, val_acc, val_kl = self._evaluate(val_samples, batch_size) if val_samples else (0, 0, 0)
            
            # æ›´æ–°å­¸ç¿’ç‡
            self.scheduler.step(val_loss if val_samples else train_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è¨˜éŒ„çµ±è¨ˆ
            self.training_stats['losses'].append(train_loss)
            self.training_stats['accuracies'].append(train_acc)
            self.training_stats['learning_rates'].append(current_lr)
            self.training_stats['policy_kl_divs'].append(train_kl)
            
            epoch_time = time.time() - start_time
            
            # æ—¥èªŒè¼¸å‡º
            log_msg = (f"Epoch {epoch+1:3d}/{num_epochs}: "
                      f"train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, "
                      f"train_kl={train_kl:.4f}")
            
            if val_samples:
                log_msg += f", val_loss={val_loss:.4f}, val_acc={val_acc:.3f}, val_kl={val_kl:.4f}"
            
            log_msg += f", lr={current_lr:.2e}, time={epoch_time:.1f}s"
            logger.info(log_msg)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            current_loss = val_loss if val_samples else train_loss
            if current_loss < best_val_loss:
                best_val_loss = current_loss
                patience_counter = 0
                self._save_model(save_path.replace('.pt', '_best.pt'), epoch, current_loss)
                logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹! æå¤±: {current_loss:.4f}")
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= max_patience:
                logger.info(f"â° æ—©åœè§¸ç™¼ (è€å¿ƒ={max_patience})")
                break
            
            # å®šæœŸä¿å­˜
            if (epoch + 1) % 20 == 0:
                self._save_model(save_path.replace('.pt', f'_epoch_{epoch+1}.pt'), epoch, current_loss)
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        self._save_model(save_path, epoch, train_loss)
        logger.info("âœ… å®Œç¾æ¨¡ä»¿å­¸ç¿’è¨“ç·´å®Œæˆ!")
        
        # è©•ä¼°æœ€çµ‚æ€§èƒ½
        self._final_evaluation()
    
    def _train_epoch(self, samples: List[Dict], batch_size: int) -> Tuple[float, float, float]:
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        random.shuffle(samples)
        
        total_loss = 0.0
        total_accuracy = 0.0
        total_kl_div = 0.0
        num_batches = 0
        
        for i in range(0, len(samples), batch_size):
            batch = samples[i:i + batch_size]
            
            # æº–å‚™æ‰¹æ¬¡æ•¸æ“š - å„ªåŒ–tensorå‰µå»º
            states_array = np.array([s['state'] for s in batch], dtype=np.float32)
            policies_array = np.array([s['policy'] for s in batch], dtype=np.float32)
            
            states = torch.from_numpy(states_array).to(self.device)
            target_policies = torch.from_numpy(policies_array).to(self.device)
            
            # å‰å‘å‚³æ’­
            self.optimizer.zero_grad()
            pred_policies, pred_values = self.model(states)
            
            # è¨ˆç®—æå¤± - ä½¿ç”¨äº¤å‰ç†µè€Œä¸æ˜¯MSE
            policy_loss = self._policy_loss(pred_policies, target_policies)
            
            # åå‘å‚³æ’­
            policy_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # çµ±è¨ˆ
            with torch.no_grad():
                accuracy = self._calculate_accuracy(pred_policies, target_policies)
                kl_div = self._calculate_kl_divergence(pred_policies, target_policies)
                
                total_loss += policy_loss.item()
                total_accuracy += accuracy
                total_kl_div += kl_div
                num_batches += 1
        
        return (total_loss / max(1, num_batches), 
                total_accuracy / max(1, num_batches),
                total_kl_div / max(1, num_batches))
    
    def _evaluate(self, samples: List[Dict], batch_size: int) -> Tuple[float, float, float]:
        """è©•ä¼°æ¨¡å‹"""
        if not samples:
            return 0.0, 0.0, 0.0
            
        self.model.eval()
        
        total_loss = 0.0
        total_accuracy = 0.0
        total_kl_div = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for i in range(0, len(samples), batch_size):
                batch = samples[i:i + batch_size]
                
                # å„ªåŒ–tensorå‰µå»º
                states_array = np.array([s['state'] for s in batch], dtype=np.float32)
                policies_array = np.array([s['policy'] for s in batch], dtype=np.float32)
                
                states = torch.from_numpy(states_array).to(self.device)
                target_policies = torch.from_numpy(policies_array).to(self.device)
                
                pred_policies, pred_values = self.model(states)
                
                loss = self._policy_loss(pred_policies, target_policies)
                accuracy = self._calculate_accuracy(pred_policies, target_policies)
                kl_div = self._calculate_kl_divergence(pred_policies, target_policies)
                
                total_loss += loss.item()
                total_accuracy += accuracy
                total_kl_div += kl_div
                num_batches += 1
        
        return (total_loss / max(1, num_batches),
                total_accuracy / max(1, num_batches),
                total_kl_div / max(1, num_batches))
    
    def _policy_loss(self, pred_policies: torch.Tensor, target_policies: torch.Tensor) -> torch.Tensor:
        """è¨ˆç®—ç­–ç•¥æå¤± - ä½¿ç”¨KLæ•£åº¦"""
        # æ·»åŠ å°å€¼é¿å…log(0)
        pred_policies = torch.softmax(pred_policies, dim=1) + 1e-8
        target_policies = target_policies + 1e-8
        
        # KLæ•£åº¦: sum(target * log(target / pred))
        kl_loss = torch.sum(target_policies * torch.log(target_policies / pred_policies), dim=1)
        return torch.mean(kl_loss)
    
    def _calculate_accuracy(self, pred_policies: torch.Tensor, target_policies: torch.Tensor) -> float:
        """è¨ˆç®—é æ¸¬æº–ç¢ºç‡"""
        pred_actions = torch.argmax(pred_policies, dim=1)
        target_actions = torch.argmax(target_policies, dim=1)
        correct = (pred_actions == target_actions).float()
        return torch.mean(correct).item()
    
    def _calculate_kl_divergence(self, pred_policies: torch.Tensor, target_policies: torch.Tensor) -> float:
        """è¨ˆç®—KLæ•£åº¦"""
        pred_policies = torch.softmax(pred_policies, dim=1) + 1e-8
        target_policies = target_policies + 1e-8
        
        kl_div = torch.sum(target_policies * torch.log(target_policies / pred_policies), dim=1)
        return torch.mean(kl_div).item()
    
    def _save_model(self, save_path: str, epoch: int, loss: float):
        """ä¿å­˜æ¨¡å‹"""
        try:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'epoch': epoch,
                'loss': loss,
                'config': self.config,
                'training_stats': self.training_stats
            }, save_path)
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±æ•—: {e}")
    
    def _final_evaluation(self):
        """æœ€çµ‚è©•ä¼°"""
        logger.info("ğŸ” é€²è¡Œæœ€çµ‚æ€§èƒ½è©•ä¼°...")
        
        # æ¸¬è©¦å¹¾å€‹é—œéµå±€é¢
        test_positions = [
            ([0] * 42, "ç©ºå±€é¢"),
            ([0]*35 + [1, 0, 0, 0, 0, 0, 0], "ä¸­å¤®é–‹å±€"),
        ]
        
        for board, description in test_positions:
            try:
                valid_actions = [c for c in range(7) if board[c] == 0]
                if not valid_actions:
                    continue
                
                # æ¨¡å‹é æ¸¬
                state = self.dataset._encode_state(board, 1)
                with torch.no_grad():
                    state_tensor = torch.from_numpy(np.array(state, dtype=np.float32)).unsqueeze(0).to(self.device)
                    pred_policy, _ = self.model(state_tensor)
                    pred_policy = torch.softmax(pred_policy, dim=1).cpu().numpy()[0]
                
                # C4Solveré æ¸¬
                expert_policy = self.dataset.expert_policy.get_expert_policy(board, valid_actions)
                
                # æ¯”è¼ƒ
                model_action = np.argmax(pred_policy)
                expert_action = np.argmax(expert_policy)
                
                logger.info(f"ğŸ“‹ {description}:")
                logger.info(f"  å°ˆå®¶å‹•ä½œ: {expert_action}, æ¨¡å‹å‹•ä½œ: {model_action}, åŒ¹é…: {'âœ…' if model_action == expert_action else 'âŒ'}")
                logger.info(f"  å°ˆå®¶ç­–ç•¥: {expert_policy}")
                logger.info(f"  æ¨¡å‹ç­–ç•¥: {pred_policy}")
                
            except Exception as e:
                logger.warning(f"è©•ä¼° {description} å¤±æ•—: {e}")

def load_enhanced_config(config_path: str = "enhanced_imitation_config.yaml") -> Dict:
    """è¼‰å…¥å¢å¼·é…ç½®"""
    default_config = {
        'model': {
            'input_size': 126,
            'hidden_size': 512,
            'num_layers': 32
        },
        'training': {
            'learning_rate': 0.0005,
            'weight_decay': 0.0001,
            'positions_per_depth': 200,  # æ¯å€‹æ·±åº¦çš„å±€é¢æ•¸
            'num_epochs': 200,
            'batch_size': 64,
            'val_split': 0.15
        },
        'paths': {
            'output_model': 'perfect_imitation_model.pt'
        }
    }
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.safe_load(f)
            
            # æ·±åº¦åˆä½µé…ç½®
            for section, values in user_config.items():
                if section in default_config:
                    default_config[section].update(values)
                else:
                    default_config[section] = values
                    
        except Exception as e:
            logger.warning(f"è¼‰å…¥é…ç½®æ–‡ä»¶å¤±æ•—: {e}ï¼Œä½¿ç”¨é»˜èªé…ç½®")
    
    return default_config

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸ¯ Connect4 å®Œç¾æ¨¡ä»¿å­¸ç¿’ç³»çµ±")
    logger.info("=" * 60)
    
    try:
        # è¼‰å…¥é…ç½®
        config = load_enhanced_config()
        logger.info("âœ… é…ç½®è¼‰å…¥å®Œæˆ")
        
        # å‰µå»ºè¨“ç·´å™¨
        trainer = EnhancedImitationLearner(config)
        
        # é–‹å§‹è¨“ç·´
        training_config = config['training']
        output_path = config['paths']['output_model']
        
        trainer.train(
            positions_per_depth=training_config['positions_per_depth'],
            num_epochs=training_config['num_epochs'],
            batch_size=training_config['batch_size'],
            val_split=training_config['val_split'],
            save_path=output_path
        )
        
        logger.info("ğŸ‰ å®Œç¾æ¨¡ä»¿å­¸ç¿’è¨“ç·´å®Œæˆ!")
        logger.info(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {output_path}")
        
    except Exception as e:
        logger.error(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
