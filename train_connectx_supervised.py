#!/usr/bin/env python3
"""
ConnectX ç›£ç£å­¸ç¿’è¨“ç·´è…³æœ¬ - ç°¡åŒ–ç‰ˆæœ¬
ä½¿ç”¨ connectx-state-action-value.txt è³‡æ–™é›†é€²è¡Œè¨“ç·´
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import logging
import time
from datetime import datetime
from collections import deque
from kaggle_environments import make
from tqdm import tqdm

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ConnectXNet(nn.Module):
    """å¼·åŒ–å­¸ç¿’ç”¨çš„ ConnectX æ·±åº¦ç¥ç¶“ç¶²è·¯"""

    def __init__(self, input_size=126, hidden_size=200, num_layers=256):
        super(ConnectXNet, self).__init__()

        # è¼¸å…¥å±¤
        self.input_layer = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # éš±è—å±¤ï¼ˆæ®˜å·®é€£æ¥ + å±¤æ­£è¦åŒ–ä»£æ›¿æ‰¹é‡æ­£è¦åŒ–ï¼‰
        self.hidden_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.LayerNorm(hidden_size),  # ä½¿ç”¨ LayerNorm ä»£æ›¿ BatchNorm1d
                nn.ReLU(),
                nn.Dropout(0.15),  # ç¨å¾®å¢åŠ dropout
                nn.Linear(hidden_size, hidden_size),
                nn.LayerNorm(hidden_size)   # ä½¿ç”¨ LayerNorm ä»£æ›¿ BatchNorm1d
            ) for _ in range(num_layers)
        ])

        # ç­–ç•¥é ­ï¼ˆå‹•ä½œæ¦‚ç‡ï¼‰
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),  # ä½¿ç”¨ LayerNorm ä»£æ›¿ BatchNorm1d
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 7),  # 7 åˆ—
            nn.Softmax(dim=-1)
        )

        # åƒ¹å€¼é ­ï¼ˆç‹€æ…‹åƒ¹å€¼ï¼‰
        self.value_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),  # ä½¿ç”¨ LayerNorm ä»£æ›¿ BatchNorm1d
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 1),
            nn.Tanh()
        )

    def forward(self, x):
        # è¼¸å…¥è™•ç†
        x = self.input_layer(x)

        # æ®˜å·®é€£æ¥
        for hidden_layer in self.hidden_layers:
            residual = x
            x = hidden_layer(x)
            x = torch.relu(x + residual)

        # è¼¸å‡ºé ­
        policy = self.policy_head(x)
        value = self.value_head(x)

        return policy, value

class PPOAgent:
    """PPO å¼·åŒ–å­¸ç¿’æ™ºèƒ½é«”"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ç¥ç¶“ç¶²è·¯
        self.policy_net = ConnectXNet(
            input_size=config['input_size'],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers']
        ).to(self.device)

        # å„ªåŒ–å™¨
        self.optimizer = optim.Adam(
            self.policy_net.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )

        # å­¸ç¿’ç‡èª¿åº¦å™¨ - æ ¹æ“šæ€§èƒ½å‹•æ…‹èª¿æ•´
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',  # ç›£æ§å‹ç‡æœ€å¤§åŒ–
            factor=0.7,  # å­¸ç¿’ç‡é™ä½å› å­
            patience=1000,  # ç­‰å¾…å›åˆæ•¸
            min_lr=1e-8
        )

        # è¨“ç·´åƒæ•¸
        self.gamma = config['gamma']
        self.eps_clip = config['eps_clip']
        self.k_epochs = config['k_epochs']
        self.entropy_coef = config['entropy_coef']
        self.value_coef = config['value_coef']

        # ç¶“é©—ç·©è¡å€
        self.memory = deque(maxlen=config['buffer_size'])

    def extract_board_and_mark(self, env_state, player_idx):
        """å¾ç’°å¢ƒç‹€æ…‹ä¸­æå–æ£‹ç›¤å’Œç©å®¶æ¨™è¨˜"""
        try:
            # æª¢æŸ¥ç’°å¢ƒç‹€æ…‹åŸºæœ¬çµæ§‹
            if not env_state or len(env_state) <= player_idx:
                logger.warning(f"ç’°å¢ƒç‹€æ…‹ç„¡æ•ˆæˆ–ç©å®¶ç´¢å¼•è¶…å‡ºç¯„åœ: len={len(env_state) if env_state else 0}, player_idx={player_idx}")
                return [0] * 42, player_idx + 1

            # ç²å–ç©å®¶ç‹€æ…‹
            player_state = env_state[player_idx]
            if 'observation' not in player_state:
                logger.warning(f"ç©å®¶ {player_idx} ç‹€æ…‹ä¸­æ²’æœ‰ observation")
                return [0] * 42, player_idx + 1

            obs = player_state['observation']

            # é¦–å…ˆå˜—è©¦ç²å–æ¨™è¨˜
            mark = None
            if 'mark' in obs:
                mark = obs['mark']
            elif hasattr(obs, 'mark'):
                mark = obs.mark
            else:
                mark = player_idx + 1  # é»˜èªæ¨™è¨˜
                logger.warning(f"ç„¡æ³•ç²å–ç©å®¶æ¨™è¨˜ï¼Œä½¿ç”¨é»˜èªå€¼: {mark}")

            # å˜—è©¦ç²å–æ£‹ç›¤
            board = None

            # æ–¹æ³•1: ç›´æ¥å¾ç•¶å‰ç©å®¶è§€å¯Ÿç²å–
            if 'board' in obs:
                board = obs['board']
                logger.debug(f"å¾ç©å®¶ {player_idx} å­—å…¸æ–¹å¼ç²å–æ£‹ç›¤")
            elif hasattr(obs, 'board'):
                board = obs.board
                logger.debug(f"å¾ç©å®¶ {player_idx} å±¬æ€§æ–¹å¼ç²å–æ£‹ç›¤")

            # æ–¹æ³•2: å¦‚æœç•¶å‰ç©å®¶æ²’æœ‰æ£‹ç›¤ï¼Œå¾å…¶ä»–ç©å®¶ç²å–
            if board is None:
                logger.warning(f"ç©å®¶ {player_idx} è§€å¯Ÿä¸­æ²’æœ‰æ£‹ç›¤æ•¸æ“šï¼Œå¯ç”¨éµ: {list(obs.keys()) if hasattr(obs, 'keys') else 'N/A'}")
                logger.warning(f"ç©å®¶ {player_idx} ç‹€æ…‹: {player_state.get('status', 'Unknown')}")

                # å˜—è©¦å¾å…¶ä»–ç©å®¶ç²å–
                for other_idx in range(len(env_state)):
                    if other_idx != player_idx:
                        try:
                            other_state = env_state[other_idx]
                            if 'observation' in other_state:
                                other_obs = other_state['observation']
                                if 'board' in other_obs:
                                    board = other_obs['board']
                                    logger.info(f"å¾ç©å®¶ {other_idx} ç²å–æ£‹ç›¤ (å­—å…¸æ–¹å¼)")
                                    break
                                elif hasattr(other_obs, 'board'):
                                    board = other_obs.board
                                    logger.info(f"å¾ç©å®¶ {other_idx} ç²å–æ£‹ç›¤ (å±¬æ€§æ–¹å¼)")
                                    break
                        except Exception as e:
                            logger.debug(f"å¾ç©å®¶ {other_idx} ç²å–æ£‹ç›¤å¤±æ•—: {e}")
                            continue

            # æ–¹æ³•3: æœ€å¾Œå‚™ç”¨æ–¹æ¡ˆ
            if board is None:
                logger.warning("æ‰€æœ‰æ–¹æ³•éƒ½ç„¡æ³•ç²å–æ£‹ç›¤ï¼Œä½¿ç”¨ç©ºæ£‹ç›¤")
                board = [0] * 42

            # é©—è­‰æ£‹ç›¤æ•¸æ“š
            if not isinstance(board, (list, tuple)) or len(board) != 42:
                logger.warning(f"æ£‹ç›¤æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º: type={type(board)}, len={len(board) if hasattr(board, '__len__') else 'N/A'}")
                board = [0] * 42

            logger.debug(f"æˆåŠŸæå–ç©å®¶ {player_idx} çš„ç‹€æ…‹: æ£‹ç›¤é•·åº¦={len(board)}, æ¨™è¨˜={mark}")
            return list(board), mark

        except Exception as e:
            logger.error(f"æå–æ£‹ç›¤ç‹€æ…‹æ™‚å‡ºéŒ¯: {e}")
            logger.error(f"ç’°å¢ƒç‹€æ…‹é¡å‹: {type(env_state)}")

            # è©³ç´°èª¿è©¦ä¿¡æ¯
            try:
                if env_state and len(env_state) > player_idx:
                    player_state = env_state[player_idx]
                    logger.error(f"ç©å®¶ {player_idx} ç‹€æ…‹éµ: {list(player_state.keys()) if hasattr(player_state, 'keys') else 'N/A'}")
                    logger.error(f"ç©å®¶ {player_idx} ç‹€æ…‹: {player_state.get('status', 'Unknown')}")

                    if 'observation' in player_state:
                        obs = player_state['observation']
                        obs_keys = list(obs.keys()) if hasattr(obs, 'keys') else [attr for attr in dir(obs) if not attr.startswith('_')]
                        logger.error(f"è§€å¯Ÿéµ: {obs_keys}")

                        # æª¢æŸ¥è§€å¯Ÿçš„å…§å®¹
                        if hasattr(obs, 'keys'):
                            for key in obs.keys():
                                try:
                                    value = obs[key]
                                    logger.error(f"  {key}: {type(value)} = {value}")
                                except:
                                    logger.error(f"  {key}: ç„¡æ³•è¨ªå•")
            except Exception as debug_e:
                logger.error(f"èª¿è©¦ä¿¡æ¯æ”¶é›†å¤±æ•—: {debug_e}")

            # è¿”å›é»˜èªå€¼
            return [0] * 42, player_idx + 1

    def encode_state(self, board, mark):
        """ç·¨ç¢¼æ£‹ç›¤ç‹€æ…‹"""
        # ç¢ºä¿ board æ˜¯æœ‰æ•ˆçš„
        if not board:
            board = [0] * 42
        elif len(board) != 42:
            # å¦‚æœé•·åº¦ä¸å°ï¼Œèª¿æ•´æˆ–å¡«å……
            if len(board) < 42:
                board = list(board) + [0] * (42 - len(board))
            else:
                board = list(board)[:42]

        # è½‰æ›ç‚º 6x7 çŸ©é™£
        state = np.array(board).reshape(6, 7)

        # å‰µå»ºä¸‰å€‹ç‰¹å¾µé€šé“
        # é€šé“ 1: ç•¶å‰ç©å®¶çš„æ£‹å­
        player_pieces = (state == mark).astype(np.float32)
        # é€šé“ 2: å°æ‰‹çš„æ£‹å­
        opponent_pieces = (state == (3 - mark)).astype(np.float32)
        # é€šé“ 3: ç©ºä½
        empty_spaces = (state == 0).astype(np.float32)

        # æ‹‰å¹³ä¸¦é€£æ¥
        encoded = np.concatenate([
            player_pieces.flatten(),
            opponent_pieces.flatten(),
            empty_spaces.flatten()
        ])

        return encoded

    def get_valid_actions(self, board):
        """ç²å–æœ‰æ•ˆå‹•ä½œ"""
        # ç¢ºä¿ board æ˜¯æœ‰æ•ˆçš„
        if not board or len(board) != 42:
            board = [0] * 42

        # æª¢æŸ¥æ¯ä¸€åˆ—çš„é ‚éƒ¨æ˜¯å¦ç‚ºç©º
        valid_actions = []
        for col in range(7):
            if board[col] == 0:  # æª¢æŸ¥æ¯åˆ—çš„é ‚éƒ¨
                valid_actions.append(col)

        # å¦‚æœæ²’æœ‰æœ‰æ•ˆå‹•ä½œï¼Œè¿”å›æ‰€æœ‰åˆ—ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
        if not valid_actions:
            valid_actions = list(range(7))

        return valid_actions

    def select_action(self, state, valid_actions, training=True, temperature=1.0, exploration_bonus=0.0):
        """é¸æ“‡å‹•ä½œï¼ˆæ”¯æŒæº«åº¦æ¡æ¨£å’Œæ¢ç´¢çå‹µï¼‰"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        # æ ¹æ“š batch size æ±ºå®šæ˜¯å¦ä½¿ç”¨ BatchNorm
        use_eval_mode = state_tensor.size(0) == 1  # å–®å€‹æ¨£æœ¬æ™‚ä½¿ç”¨evalæ¨¡å¼

        if use_eval_mode:
            # å–®å€‹æ¨£æœ¬æ™‚ä½¿ç”¨è©•ä¼°æ¨¡å¼é¿å… BatchNorm å•é¡Œ
            self.policy_net.eval()

        with torch.no_grad():
            action_probs, state_value = self.policy_net(state_tensor)

        # æ¢å¾©è¨“ç·´æ¨¡å¼
        if training and use_eval_mode:
            self.policy_net.train()

        # é®ç½©ç„¡æ•ˆå‹•ä½œ
        action_probs = action_probs.cpu().numpy()[0]
        masked_probs = np.zeros_like(action_probs)
        masked_probs[valid_actions] = action_probs[valid_actions]

        # æ­£è¦åŒ–æ¦‚ç‡
        if masked_probs.sum() > 0:
            masked_probs /= masked_probs.sum()
        else:
            # å¾Œå‚™æ–¹æ¡ˆï¼šå‡å‹»åˆ†ä½ˆ
            masked_probs[valid_actions] = 1.0 / len(valid_actions)

        # æ‡‰ç”¨æº«åº¦å’Œæ¢ç´¢çå‹µ
        if training and (temperature != 1.0 or exploration_bonus > 0.0):
            # æº«åº¦æ¡æ¨£
            if temperature != 1.0:
                masked_probs = np.power(masked_probs + 1e-8, 1/temperature)
                masked_probs /= masked_probs.sum()

            # æ¢ç´¢çå‹µï¼ˆç‚ºä¸å¸¸é¸æ“‡çš„å‹•ä½œå¢åŠ æ¦‚ç‡ï¼‰
            if exploration_bonus > 0.0:
                uniform_dist = np.zeros_like(masked_probs)
                uniform_dist[valid_actions] = 1.0 / len(valid_actions)
                masked_probs = (1 - exploration_bonus) * masked_probs + exploration_bonus * uniform_dist

        if training:
            # è¨“ç·´æ™‚æ¡æ¨£å‹•ä½œ
            action = np.random.choice(7, p=masked_probs)
        else:
            # è©•ä¼°æ™‚é¸æ“‡æœ€ä½³å‹•ä½œ
            action = valid_actions[np.argmax(masked_probs[valid_actions])]

        # ç¢ºä¿è¿”å›çš„å‹•ä½œæ˜¯ Python int é¡å‹ï¼Œé¿å… numpy é¡å‹å•é¡Œ
        action = int(action)

        return action, action_probs[action], state_value.item()

    def store_transition(self, state, action, prob, reward, done):
        """å„²å­˜è½‰æ›"""
        self.memory.append((state, action, prob, reward, done))

    def compute_gae(self, rewards, values, dones, next_value, gamma=0.99, lam=0.95):
        """è¨ˆç®—å»£ç¾©å„ªå‹¢ä¼°è¨ˆï¼ˆGAEï¼‰"""
        advantages = []
        gae = 0

        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_val = next_value
            else:
                next_val = values[i + 1]

            delta = rewards[i] + gamma * next_val * (1 - dones[i]) - values[i]
            gae = delta + gamma * lam * (1 - dones[i]) * gae
            advantages.insert(0, gae)

        returns = [adv + val for adv, val in zip(advantages, values)]
        return advantages, returns

    def update_policy(self):
        """ä½¿ç”¨ PPO æ›´æ–°ç­–ç•¥"""
        if len(self.memory) < self.config['min_batch_size']:
            return None

        # æº–å‚™è¨“ç·´æ•¸æ“š
        states = []
        actions = []
        old_probs = []
        rewards = []
        dones = []

        for transition in self.memory:
            state, action, prob, reward, done = transition
            states.append(state)
            actions.append(action)
            old_probs.append(prob)
            rewards.append(reward)
            dones.append(done)

        # è¨ˆç®—æ‰€æœ‰ç‹€æ…‹çš„åƒ¹å€¼
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        with torch.no_grad():
            _, values_tensor = self.policy_net(states_tensor)
            values = values_tensor.cpu().numpy().flatten()

        # è¨ˆç®—å„ªå‹¢å’Œå›å ±
        advantages, returns = self.compute_gae(
            rewards, values, dones, 0,
            self.gamma, self.config['gae_lambda']
        )

        # æ­£è¦åŒ–å„ªå‹¢
        advantages = np.array(advantages)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # è½‰æ›ç‚ºå¼µé‡
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        actions_tensor = torch.LongTensor(actions).to(self.device)
        old_probs_tensor = torch.FloatTensor(old_probs).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        returns_tensor = torch.FloatTensor(returns).to(self.device)

        # PPO æ›´æ–°
        total_loss = 0
        for _ in range(self.k_epochs):
            # å‰å‘å‚³æ’­
            new_probs, values = self.policy_net(states_tensor)

            # è¨ˆç®—æ¯”ç‡
            new_action_probs = new_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze()
            ratio = new_action_probs / (old_probs_tensor + 1e-8)

            # PPO æå¤±
            surr1 = ratio * advantages_tensor
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages_tensor
            policy_loss = -torch.min(surr1, surr2).mean()

            # åƒ¹å€¼æå¤±
            value_loss = nn.MSELoss()(values.squeeze(), returns_tensor)

            # ç†µæå¤±ï¼ˆæ¢ç´¢ï¼‰
            entropy = -(new_probs * torch.log(new_probs + 1e-8)).sum(dim=1).mean()

            # ç¸½æå¤±
            loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
            total_loss += loss.item()

            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
            self.optimizer.step()

        # æ¸…ç©ºè¨˜æ†¶é«”
        self.memory.clear()

        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': entropy.item(),
            'total_loss': total_loss / self.k_epochs
        }

class ConnectXTrainer:
    """ConnectX è¨“ç·´å™¨"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ç¥ç¶“ç¶²è·¯
        self.policy_net = ConnectXNet(
            input_size=config['agent']['input_size'],
            hidden_size=config['agent']['hidden_size'],
            num_layers=config['agent']['num_layers']
        ).to(self.device)

        # å„ªåŒ–å™¨
        self.optimizer = optim.Adam(
            self.policy_net.parameters(),
            lr=config['agent']['learning_rate'],
            weight_decay=config['agent']['weight_decay']
        )

        # å­¸ç¿’ç‡èª¿åº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.7,
            patience=10,
            min_lr=1e-8
        )

        # å‰µå»ºç›®éŒ„
        os.makedirs('checkpoints', exist_ok=True)
        os.makedirs('logs', exist_ok=True)

    def encode_state(self, board, mark):
        """ç·¨ç¢¼æ£‹ç›¤ç‹€æ…‹"""
        # ç¢ºä¿ board æ˜¯æœ‰æ•ˆçš„
        if not board:
            board = [0] * 42
        elif len(board) != 42:
            if len(board) < 42:
                board = list(board) + [0] * (42 - len(board))
            else:
                board = list(board)[:42]

        # è½‰æ›ç‚º 6x7 çŸ©é™£
        state = np.array(board).reshape(6, 7)

        # å‰µå»ºä¸‰å€‹ç‰¹å¾µé€šé“
        # é€šé“ 1: ç•¶å‰ç©å®¶çš„æ£‹å­
        player_pieces = (state == mark).astype(np.float32)
        # é€šé“ 2: å°æ‰‹çš„æ£‹å­
        opponent_pieces = (state == (3 - mark)).astype(np.float32)
        # é€šé“ 3: ç©ºä½
        empty_spaces = (state == 0).astype(np.float32)

        # æ‹‰å¹³ä¸¦é€£æ¥
        encoded = np.concatenate([
            player_pieces.flatten(),
            opponent_pieces.flatten(),
            empty_spaces.flatten()
        ])

        return encoded

    def load_dataset(self, file_path="connectx-state-action-value.txt", max_lines=10000):
        """è¼‰å…¥è¨“ç·´æ•¸æ“šé›† - è¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬"""
        skipped_lines = 0
        valid_samples = 0

        try:
            logger.info(f"è¼‰å…¥è¨“ç·´æ•¸æ“šé›†: {file_path}")
            logger.info(f"é™åˆ¶è¼‰å…¥è¡Œæ•¸: {max_lines}")

            # ç¬¬ä¸€æ¬¡æƒæï¼šè¨ˆç®—æœ‰æ•ˆæ¨£æœ¬æ•¸é‡
            logger.info("ğŸ” ç¬¬ä¸€æ¬¡æƒæï¼šè¨ˆç®—æœ‰æ•ˆæ¨£æœ¬æ•¸é‡...")
            with open(file_path, 'r') as f:
                for i, line in enumerate(f):
                    if i >= max_lines:
                        break
                    
                    line = line.strip()
                    if not line:
                        continue
                        
                    try:
                        parts = line.split(',')
                        if len(parts) < 8:
                            continue
                            
                        board_str = parts[0]
                        if len(board_str) != 42 or not all(c in '012' for c in board_str):
                            continue
                            
                        # å¿«é€Ÿæª¢æŸ¥å‹•ä½œåƒ¹å€¼
                        action_vals = []
                        for j in range(1, 8):
                            val_str = parts[j].strip()
                            if val_str == '':
                                action_vals.append(-999.0)
                            else:
                                try:
                                    action_vals.append(float(val_str))
                                except ValueError:
                                    action_vals.append(0.0)
                        
                        # æª¢æŸ¥æœ‰æ•ˆå‹•ä½œ
                        if any(val > -900 for val in action_vals):
                            valid_samples += 1
                            
                    except Exception:
                        continue

            logger.info(f"ğŸ“Š æƒæçµæœï¼šé è¨ˆ {valid_samples} å€‹æœ‰æ•ˆæ¨£æœ¬")
            
            if valid_samples == 0:
                logger.error("æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ¨£æœ¬ï¼")
                return None, None

            # é åˆ†é…è¨˜æ†¶é«”
            states = np.zeros((valid_samples, 126), dtype=np.float32)
            action_values = np.zeros((valid_samples, 7), dtype=np.float32)
            
            # ç¬¬äºŒæ¬¡æƒæï¼šè¼‰å…¥æ•¸æ“š
            logger.info("ğŸ“¥ ç¬¬äºŒæ¬¡æƒæï¼šè¼‰å…¥æ•¸æ“š...")
            sample_idx = 0
            
            with open(file_path, 'r') as f:
                with tqdm(total=min(max_lines, valid_samples), desc="è¼‰å…¥æ•¸æ“š") as pbar:
                    for i, line in enumerate(f):
                        if i >= max_lines or sample_idx >= valid_samples:
                            break
                        
                        line = line.strip()
                        if not line:
                            continue

                        try:
                            # è§£æä¸€è¡Œæ•¸æ“š
                            parts = line.split(',')
                            if len(parts) < 8:
                                skipped_lines += 1
                                continue

                            # è§£ææ£‹ç›¤ç‹€æ…‹
                            board_str = parts[0]
                            if len(board_str) != 42:
                                skipped_lines += 1
                                continue

                            # ç›´æ¥è½‰æ›æ£‹ç›¤ç‹€æ…‹ï¼ˆé¿å…ä¸­é–“åˆ—è¡¨ï¼‰
                            board_state = [int(c) for c in board_str if c in '012']
                            if len(board_state) != 42:
                                skipped_lines += 1
                                continue

                            # è§£æå‹•ä½œåƒ¹å€¼ï¼ˆç›´æ¥å¯«å…¥æ•¸çµ„ï¼‰
                            action_vals = np.full(7, -999.0, dtype=np.float32)
                            for j in range(1, 8):
                                val_str = parts[j].strip()
                                if val_str != '':
                                    try:
                                        action_vals[j-1] = float(val_str)
                                    except ValueError:
                                        action_vals[j-1] = 0.0

                            # æª¢æŸ¥æœ‰æ•ˆå‹•ä½œ
                            if not np.any(action_vals > -900):
                                skipped_lines += 1
                                continue

                            # ç·¨ç¢¼ç‹€æ…‹ï¼ˆç›´æ¥å¯«å…¥é åˆ†é…çš„æ•¸çµ„ï¼‰
                            encoded_state = self.encode_state(board_state, 1)
                            states[sample_idx] = encoded_state
                            action_values[sample_idx] = action_vals
                            
                            sample_idx += 1
                            pbar.update(1)

                        except Exception as e:
                            logger.debug(f"ç¬¬ {i + 1} è¡Œè§£æéŒ¯èª¤: {e}")
                            skipped_lines += 1
                            continue

            # è£å‰ªåˆ°å¯¦éš›ä½¿ç”¨çš„å¤§å°
            if sample_idx < valid_samples:
                states = states[:sample_idx]
                action_values = action_values[:sample_idx]

            logger.info(f"æ•¸æ“šè¼‰å…¥å®Œæˆ:")
            logger.info(f"  æˆåŠŸè§£æ: {sample_idx} å€‹æ¨£æœ¬")
            logger.info(f"  è·³éè¡Œæ•¸: {skipped_lines}")
            logger.info(f"  è¨˜æ†¶é«”ä½¿ç”¨: {states.nbytes / 1024 / 1024:.1f} MB (ç‹€æ…‹) + {action_values.nbytes / 1024 / 1024:.1f} MB (å‹•ä½œå€¼)")

            if sample_idx == 0:
                logger.error("æ²’æœ‰æˆåŠŸè§£æä»»ä½•æ•¸æ“šï¼")
                return None, None

            return states, action_values

        except FileNotFoundError:
            logger.error(f"æ‰¾ä¸åˆ°æ•¸æ“šé›†æ–‡ä»¶: {file_path}")
            return None, None
        except Exception as e:
            logger.error(f"è¼‰å…¥æ•¸æ“šé›†æ™‚å‡ºéŒ¯: {e}")
            return None, None

    def train(self, epochs=100, batch_size=128, max_lines=10000, memory_efficient=True):
        """ç›£ç£å­¸ç¿’è¨“ç·´ - æ”¯æ´è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼"""
        logger.info("ğŸš€ é–‹å§‹ç›£ç£å­¸ç¿’è¨“ç·´")

        if memory_efficient and max_lines > 20000:
            # è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼ï¼šåˆ†æ‰¹è¼‰å…¥è¨“ç·´
            return self.train_memory_efficient(epochs, batch_size, max_lines)
        else:
            # æ¨™æº–æ¨¡å¼ï¼šä¸€æ¬¡è¼‰å…¥æ‰€æœ‰æ•¸æ“š
            return self.train_standard(epochs, batch_size, max_lines)

    def train_memory_efficient(self, epochs=100, batch_size=128, max_lines=10000):
        """è¨˜æ†¶é«”å„ªåŒ–çš„è¨“ç·´æ¨¡å¼"""
        logger.info("ğŸ’¾ ä½¿ç”¨è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼è¨“ç·´")
        
        # åˆ†æ‰¹è¼‰å…¥åƒæ•¸
        chunk_size = min(10000, max_lines // 4)  # æ¯æ¬¡è¼‰å…¥1/4æ•¸æ“š
        
        self.policy_net.train()
        best_loss = float('inf')
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            total_loss = 0.0
            total_policy_loss = 0.0
            total_value_loss = 0.0
            num_batches = 0
            
            # åˆ†å¡Šè™•ç†æ•¸æ“š
            for chunk_start in range(0, max_lines, chunk_size):
                chunk_end = min(chunk_start + chunk_size, max_lines)
                chunk_max_lines = chunk_end - chunk_start
                
                # è¼‰å…¥æ•¸æ“šå¡Š
                states, action_values = self.load_dataset_chunk(
                    file_path="connectx-state-action-value.txt",
                    start_line=chunk_start,
                    max_lines=chunk_max_lines
                )
                
                if states is None or len(states) == 0:
                    continue
                
                # éš¨æ©Ÿæ‰“äº‚ç•¶å‰å¡Šçš„æ•¸æ“š
                indices = np.random.permutation(len(states))
                
                # æ‰¹æ¬¡è¨“ç·´ç•¶å‰æ•¸æ“šå¡Š
                for batch_start in range(0, len(states), batch_size):
                    batch_end = min(batch_start + batch_size, len(states))
                    batch_indices = indices[batch_start:batch_end]
                    
                    # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
                    batch_states = torch.FloatTensor(states[batch_indices]).to(self.device)
                    batch_action_values = torch.FloatTensor(action_values[batch_indices]).to(self.device)
                    
                    # åŸ·è¡Œä¸€å€‹è¨“ç·´æ­¥é©Ÿ
                    loss_info = self.train_step(batch_states, batch_action_values)
                    
                    total_loss += loss_info['total_loss']
                    total_policy_loss += loss_info['policy_loss']
                    total_value_loss += loss_info['value_loss']
                    num_batches += 1
                
                # æ¸…ç†è¨˜æ†¶é«”
                del states, action_values
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # è¨ˆç®—å¹³å‡æå¤±ä¸¦è¨˜éŒ„
            self.log_epoch_results(epoch, epochs, total_loss, total_policy_loss, 
                                 total_value_loss, num_batches, epoch_start_time, best_loss)
            
            # æ›´æ–°æœ€ä½³æå¤±
            avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
            if avg_loss < best_loss:
                best_loss = avg_loss
                self.save_checkpoint("best_supervised_model.pt")
        
        logger.info("âœ… è¨˜æ†¶é«”å„ªåŒ–è¨“ç·´å®Œæˆ")
        return self.policy_net

    def train_standard(self, epochs=100, batch_size=128, max_lines=10000):
        """æ¨™æº–è¨“ç·´æ¨¡å¼"""
        logger.info("ğŸ”„ ä½¿ç”¨æ¨™æº–æ¨¡å¼è¨“ç·´")
        
        # è¼‰å…¥æ•¸æ“šé›†
        states, action_values = self.load_dataset(max_lines=max_lines)
        if states is None or action_values is None:
            logger.error("âŒ æ•¸æ“šé›†è¼‰å…¥å¤±æ•—")
            return None

        logger.info(f"ğŸ“Š æ•¸æ“šé›†è¼‰å…¥æˆåŠŸ: {len(states)} å€‹æ¨£æœ¬")

        # è¨“ç·´å¾ªç’°
        self.policy_net.train()
        best_loss = float('inf')

        for epoch in range(epochs):
            epoch_start_time = time.time()
            total_loss = 0.0
            total_policy_loss = 0.0
            total_value_loss = 0.0
            num_batches = 0

            # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
            indices = np.random.permutation(len(states))

            # æ‰¹æ¬¡è¨“ç·´
            for batch_start in range(0, len(states), batch_size):
                batch_end = min(batch_start + batch_size, len(states))
                batch_indices = indices[batch_start:batch_end]

                # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
                batch_states = torch.FloatTensor(states[batch_indices]).to(self.device)
                batch_action_values = torch.FloatTensor(action_values[batch_indices]).to(self.device)

                # åŸ·è¡Œä¸€å€‹è¨“ç·´æ­¥é©Ÿ
                loss_info = self.train_step(batch_states, batch_action_values)
                
                total_loss += loss_info['total_loss']
                total_policy_loss += loss_info['policy_loss']
                total_value_loss += loss_info['value_loss']
                num_batches += 1

            # è¨˜éŒ„çµæœ
            self.log_epoch_results(epoch, epochs, total_loss, total_policy_loss, 
                                 total_value_loss, num_batches, epoch_start_time, best_loss)
            
            # æ›´æ–°æœ€ä½³æå¤±
            avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
            if avg_loss < best_loss:
                best_loss = avg_loss
                self.save_checkpoint("best_supervised_model.pt")

        logger.info("âœ… æ¨™æº–è¨“ç·´å®Œæˆ")
        return self.policy_net

    def train_step(self, batch_states, batch_action_values):
        """åŸ·è¡Œå–®å€‹è¨“ç·´æ­¥é©Ÿ"""
        # å‰å‘å‚³æ’­
        predicted_probs, predicted_values = self.policy_net(batch_states)

        # è¨ˆç®—ç›®æ¨™
        # è™•ç†ç„¡æ•ˆå‹•ä½œ
        valid_mask = (batch_action_values > -900).float()
        masked_action_values = batch_action_values * valid_mask + (-1000) * (1 - valid_mask)

        # è½‰æ›ç‚ºç›®æ¨™æ¦‚ç‡åˆ†ä½ˆ
        target_probs = F.softmax(masked_action_values / 0.5, dim=1)  # æº«åº¦åƒæ•¸

        # åƒ¹å€¼ç›®æ¨™ï¼šæœ€å¤§å‹•ä½œåƒ¹å€¼
        target_values = torch.max(batch_action_values * valid_mask + (-1000) * (1 - valid_mask), dim=1)[0].unsqueeze(1)
        target_values = torch.tanh(target_values / 10.0)  # æ­£è¦åŒ–åˆ°[-1,1]

        # è¨ˆç®—æå¤±
        policy_loss = F.kl_div(torch.log(predicted_probs + 1e-8), target_probs, reduction='batchmean')
        value_loss = F.mse_loss(predicted_values, target_values)
        total_loss_batch = policy_loss + 0.5 * value_loss

        # åå‘å‚³æ’­
        self.optimizer.zero_grad()
        total_loss_batch.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
        self.optimizer.step()

        return {
            'total_loss': total_loss_batch.item(),
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item()
        }

    def log_epoch_results(self, epoch, epochs, total_loss, total_policy_loss, 
                         total_value_loss, num_batches, epoch_start_time, best_loss):
        """è¨˜éŒ„epochçµæœ"""
        # è¨ˆç®—å¹³å‡æå¤±
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        avg_policy_loss = total_policy_loss / num_batches if num_batches > 0 else 0
        avg_value_loss = total_value_loss / num_batches if num_batches > 0 else 0
        
        epoch_time = time.time() - epoch_start_time

        # å­¸ç¿’ç‡èª¿åº¦
        self.scheduler.step(avg_loss)

        # æ¯10å€‹epochå ±å‘Šä¸€æ¬¡
        if (epoch + 1) % 10 == 0 or epoch == 0:
            logger.info(f"Epoch [{epoch+1}/{epochs}] "
                      f"Loss: {avg_loss:.6f} "
                      f"Policy: {avg_policy_loss:.6f} "
                      f"Value: {avg_value_loss:.6f} "
                      f"Time: {epoch_time:.2f}s")

        # ä¿å­˜å®šæœŸæª¢æŸ¥é»
        if (epoch + 1) % 50 == 0:
            checkpoint_name = f"supervised_epoch_{epoch+1}.pt"
            self.save_checkpoint(checkpoint_name)

    def load_dataset_chunk(self, file_path="connectx-state-action-value.txt", start_line=0, max_lines=10000):
        """è¼‰å…¥æ•¸æ“šé›†çš„æŒ‡å®šå¡Š"""
        states = []
        action_values = []
        skipped_lines = 0
        current_line = 0

        try:
            with open(file_path, 'r') as f:
                # è·³éé–‹å§‹è¡Œ
                for _ in range(start_line):
                    f.readline()
                
                # è®€å–æŒ‡å®šè¡Œæ•¸
                for i in range(max_lines):
                    line = f.readline()
                    if not line:  # æ–‡ä»¶çµæŸ
                        break
                    
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        # è§£æé‚è¼¯èˆ‡åŸä¾†ç›¸åŒ
                        parts = line.split(',')
                        if len(parts) < 8:
                            skipped_lines += 1
                            continue

                        board_str = parts[0]
                        if len(board_str) != 42:
                            skipped_lines += 1
                            continue

                        board_state = [int(c) for c in board_str if c in '012']
                        if len(board_state) != 42:
                            skipped_lines += 1
                            continue

                        action_vals = []
                        for j in range(1, 8):
                            val_str = parts[j].strip()
                            if val_str == '':
                                action_vals.append(-999.0)
                            else:
                                try:
                                    action_vals.append(float(val_str))
                                except ValueError:
                                    action_vals.append(0.0)

                        if not any(val > -900 for val in action_vals):
                            skipped_lines += 1
                            continue

                        encoded_state = self.encode_state(board_state, 1)
                        states.append(encoded_state)
                        action_values.append(action_vals)

                    except Exception:
                        skipped_lines += 1
                        continue

            if len(states) == 0:
                return None, None

            return np.array(states, dtype=np.float32), np.array(action_values, dtype=np.float32)

        except Exception as e:
            logger.error(f"è¼‰å…¥æ•¸æ“šå¡Šæ™‚å‡ºéŒ¯: {e}")
            return None, None

    def save_checkpoint(self, filename):
        """ä¿å­˜æª¢æŸ¥é»"""
        try:
            checkpoint = {
                'model_state_dict': self.policy_net.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'config': self.config,
                'save_timestamp': datetime.now().isoformat(),
                'pytorch_version': str(torch.__version__)
            }

            checkpoint_path = f"checkpoints/{filename}"
            torch.save(checkpoint, checkpoint_path)

            file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB
            logger.info(f"âœ… å·²ä¿å­˜æª¢æŸ¥é»: {filename} ({file_size:.2f} MB)")

        except Exception as e:
            logger.error(f"ä¿å­˜æª¢æŸ¥é»æ™‚å‡ºéŒ¯: {e}")

    def evaluate_random_games(self, num_games=100):
        """è©•ä¼°æ¨¡å‹å°éš¨æ©Ÿå°æ‰‹çš„æ€§èƒ½"""
        logger.info(f"ğŸ¯ è©•ä¼°æ¨¡å‹æ€§èƒ½ ({num_games} å±€éŠæˆ²)")

        self.policy_net.eval()
        wins = 0
        draws = 0
        losses = 0

        for i in range(num_games):
            try:
                # å‰µå»ºéŠæˆ²ç’°å¢ƒ
                env = make("connectx", debug=False)
                env.reset()

                # ç°¡å–®çš„éŠæˆ²å¾ªç’°
                done = False
                step_count = 0
                max_steps = 42

                while not done and step_count < max_steps:
                    current_player = step_count % 2

                    if current_player == 0:  # AIç©å®¶
                        # ç²å–ç•¶å‰ç‹€æ…‹
                        obs = env.state[0]['observation']
                        board = obs['board']
                        mark = obs['mark']

                        # ç·¨ç¢¼ç‹€æ…‹
                        encoded_state = self.encode_state(board, mark)
                        state_tensor = torch.FloatTensor(encoded_state).unsqueeze(0).to(self.device)

                        # ç²å–å‹•ä½œæ¦‚ç‡
                        with torch.no_grad():
                            action_probs, _ = self.policy_net(state_tensor)

                        # é¸æ“‡å‹•ä½œï¼ˆè²ªå©ªç­–ç•¥ï¼‰
                        valid_actions = [c for c in range(7) if board[c] == 0]
                        if valid_actions:
                            masked_probs = action_probs.cpu().numpy()[0]
                            valid_probs = [masked_probs[a] for a in valid_actions]
                            best_idx = np.argmax(valid_probs)
                            action = valid_actions[best_idx]
                        else:
                            action = 0

                    else:  # éš¨æ©Ÿå°æ‰‹
                        obs = env.state[1]['observation']
                        board = obs['board']
                        valid_actions = [c for c in range(7) if board[c] == 0]
                        action = np.random.choice(valid_actions) if valid_actions else 0

                    # åŸ·è¡Œå‹•ä½œ
                    env.step([action, None] if current_player == 0 else [None, action])

                    # æª¢æŸ¥éŠæˆ²çµæŸ
                    if len(env.state) >= 2:
                        status_0 = env.state[0].get('status', 'ACTIVE')
                        status_1 = env.state[1].get('status', 'ACTIVE')

                        if status_0 != 'ACTIVE' or status_1 != 'ACTIVE':
                            done = True

                    step_count += 1

                # è¨ˆç®—çµæœ
                if len(env.state) >= 2:
                    reward_0 = env.state[0].get('reward', 0)
                    reward_1 = env.state[1].get('reward', 0)

                    if reward_0 > reward_1:
                        wins += 1
                    elif reward_1 > reward_0:
                        losses += 1
                    else:
                        draws += 1

                if (i + 1) % 20 == 0:
                    current_wr = wins / (i + 1) * 100
                    logger.info(f"è©•ä¼°é€²åº¦: {i+1}/{num_games}, ç•¶å‰å‹ç‡: {current_wr:.1f}%")

            except Exception as e:
                logger.error(f"è©•ä¼°ç¬¬ {i+1} å±€æ™‚å‡ºéŒ¯: {e}")
                losses += 1

        win_rate = wins / num_games * 100
        logger.info(f"ğŸ“Š è©•ä¼°çµæœ:")
        logger.info(f"   å‹åˆ©: {wins} ({win_rate:.1f}%)")
        logger.info(f"   å¹³å±€: {draws} ({draws/num_games*100:.1f}%)")
        logger.info(f"   å¤±æ•—: {losses} ({losses/num_games*100:.1f}%)")

        return win_rate

def create_config():
    """å‰µå»ºè¨“ç·´é…ç½®"""
    config = {
        'agent': {
            'input_size': 126,      # 3å€‹é€šé“ Ã— 42å€‹ä½ç½®
            'hidden_size': 150,     # éš±è—å±¤å¤§å°
            'num_layers': 3,        # éš±è—å±¤æ•¸é‡ï¼ˆä¿®æ­£ç‚ºåˆç†å€¼ï¼‰
            'learning_rate': 0.001, # å­¸ç¿’ç‡
            'weight_decay': 0.0001  # æ¬Šé‡è¡°æ¸›
        },
        'training': {
            'epochs': 200,          # è¨“ç·´epochs
            'batch_size': 128,      # æ‰¹æ¬¡å¤§å°
            'max_lines': 50000,     # æœ€å¤§æ•¸æ“šé›†è¡Œæ•¸
            'eval_games': 100,      # è©•ä¼°éŠæˆ²æ•¸é‡
            'memory_efficient': True # æ˜¯å¦ä½¿ç”¨è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼
        }
    }
    return config

def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    print("ğŸ® ConnectX ç›£ç£å­¸ç¿’è¨“ç·´")
    print("=" * 50)

    # å‰µå»ºå¿…è¦ç›®éŒ„
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('logs', exist_ok=True)

    # æª¢æŸ¥æ•¸æ“šé›†æ–‡ä»¶
    dataset_file = "connectx-state-action-value.txt"
    if not os.path.exists(dataset_file):
        logger.error(f"âŒ æ‰¾ä¸åˆ°æ•¸æ“šé›†æ–‡ä»¶: {dataset_file}")
        return

    # å‰µå»ºé…ç½®
    config = create_config()

    # æª¢æŸ¥è¨­å‚™
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")

    try:
        # å‰µå»ºè¨“ç·´å™¨
        trainer = ConnectXTrainer(config)
        logger.info("âœ… è¨“ç·´å™¨å‰µå»ºæˆåŠŸ")

        # é¡¯ç¤ºé…ç½®
        print("\nğŸ“‹ è¨“ç·´é…ç½®:")
        print(f"   ç¶²çµ¡çµæ§‹: {config['agent']['hidden_size']} éš±è—å–®å…ƒ, {config['agent']['num_layers']} å±¤")
        print(f"   å­¸ç¿’ç‡: {config['agent']['learning_rate']}")
        print(f"   è¨“ç·´epochs: {config['training']['epochs']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {config['training']['batch_size']}")
        print(f"   æœ€å¤§æ•¸æ“šé›†è¡Œæ•¸: {config['training']['max_lines']}")

        # é–‹å§‹è¨“ç·´
        print("\nğŸš€ é–‹å§‹ç›£ç£å­¸ç¿’è¨“ç·´...")
        start_time = time.time()

        trained_model = trainer.train(
            epochs=config['training']['epochs'],
            batch_size=config['training']['batch_size'],
            max_lines=config['training']['max_lines'],
            memory_efficient=config['training']['memory_efficient']
        )

        training_time = time.time() - start_time

        if trained_model is not None:
            logger.info(f"âœ… è¨“ç·´å®Œæˆï¼ç”¨æ™‚: {training_time:.1f}ç§’")

            # ä¿å­˜æœ€çµ‚æ¨¡å‹
            final_checkpoint = f"supervised_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt"
            trainer.save_checkpoint(final_checkpoint)

            # è©•ä¼°æ¨¡å‹
            print("\nğŸ¯ è©•ä¼°æ¨¡å‹æ€§èƒ½...")
            win_rate = trainer.evaluate_random_games(num_games=config['training']['eval_games'])

            print(f"\nğŸ‰ è¨“ç·´å®Œæˆ!")
            print(f"   ç¸½ç”¨æ™‚: {training_time:.1f}ç§’ ({training_time/60:.1f}åˆ†é˜)")
            print(f"   æœ€çµ‚å‹ç‡: {win_rate:.1f}%")
            print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: checkpoints/{final_checkpoint}")

            # ä½¿ç”¨å»ºè­°
            if win_rate >= 80:
                print("\nğŸŒŸ æ¨¡å‹æ€§èƒ½å„ªç•°ï¼å¯ä»¥ç”¨æ–¼æ¯”è³½")
            elif win_rate >= 60:
                print("\nğŸ‘ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå»ºè­°é€²è¡Œæ›´å¤šè¨“ç·´")
            else:
                print("\nâš ï¸ æ¨¡å‹æ€§èƒ½éœ€è¦æ”¹é€²ï¼Œå»ºè­°å¢åŠ è¨“ç·´æ™‚é–“æˆ–èª¿æ•´åƒæ•¸")

        else:
            logger.error("âŒ è¨“ç·´å¤±æ•—")

    except KeyboardInterrupt:
        logger.info("â¹ï¸ è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        logger.error(f"âŒ è¨“ç·´éç¨‹ä¸­å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
