#!/usr/bin/env python3
"""
ConnectX ç›£ç£å­¸ç¿’è¨“ç·´è…³æœ¬ - ç°¡åŒ–ç‰ˆæœ¬
ä½¿ç”¨ connectx-state-action-value.txt è³‡æ–™é›†é€²è¡Œè¨“ç·´
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import logging
import time
from datetime import datetime
from collections import deque
from kaggle_environments import make
from tqdm import tqdm

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ConnectXNet(nn.Module):
    """ConnectX æ·±åº¦ç¥ç¶“ç¶²è·¯"""

    def __init__(self, input_size=126, hidden_size=256, num_layers=3):
        super(ConnectXNet, self).__init__()

        # è¼¸å…¥å±¤
        self.input_layer = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # éš±è—å±¤
        self.hidden_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.LayerNorm(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.15),
                nn.Linear(hidden_size, hidden_size),
                nn.LayerNorm(hidden_size)
            ) for _ in range(num_layers)
        ])

        # ç­–ç•¥é ­ï¼ˆå‹•ä½œæ¦‚ç‡ï¼‰
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 7),  # 7 åˆ—
            nn.Softmax(dim=-1)
        )

        # åƒ¹å€¼é ­ï¼ˆç‹€æ…‹åƒ¹å€¼ï¼‰
        self.value_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 1),
            nn.Tanh()
        )

    def forward(self, x):
        # è¼¸å…¥è™•ç†
        x = self.input_layer(x)

        # æ®˜å·®é€£æ¥
        for hidden_layer in self.hidden_layers:
            residual = x
            x = hidden_layer(x)
            x = torch.relu(x + residual)

        # è¼¸å‡ºé ­
        policy = self.policy_head(x)
        value = self.value_head(x)

        return policy, value

class ConnectXTrainer:
    """ConnectX è¨“ç·´å™¨"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ç¥ç¶“ç¶²è·¯
        self.policy_net = ConnectXNet(
            input_size=config['agent']['input_size'],
            hidden_size=config['agent']['hidden_size'],
            num_layers=config['agent']['num_layers']
        ).to(self.device)

        # å„ªåŒ–å™¨
        self.optimizer = optim.Adam(
            self.policy_net.parameters(),
            lr=config['agent']['learning_rate'],
            weight_decay=config['agent']['weight_decay']
        )

        # å­¸ç¿’ç‡èª¿åº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.7,
            patience=10,
            min_lr=1e-8
        )

        # å‰µå»ºç›®éŒ„
        os.makedirs('checkpoints', exist_ok=True)
        os.makedirs('logs', exist_ok=True)

    def encode_state(self, board, mark):
        """ç·¨ç¢¼æ£‹ç›¤ç‹€æ…‹"""
        # ç¢ºä¿ board æ˜¯æœ‰æ•ˆçš„
        if not board:
            board = [0] * 42
        elif len(board) != 42:
            if len(board) < 42:
                board = list(board) + [0] * (42 - len(board))
            else:
                board = list(board)[:42]

        # è½‰æ›ç‚º 6x7 çŸ©é™£
        state = np.array(board).reshape(6, 7)

        # å‰µå»ºä¸‰å€‹ç‰¹å¾µé€šé“
        # é€šé“ 1: ç•¶å‰ç©å®¶çš„æ£‹å­
        player_pieces = (state == mark).astype(np.float32)
        # é€šé“ 2: å°æ‰‹çš„æ£‹å­
        opponent_pieces = (state == (3 - mark)).astype(np.float32)
        # é€šé“ 3: ç©ºä½
        empty_spaces = (state == 0).astype(np.float32)

        # æ‹‰å¹³ä¸¦é€£æ¥
        encoded = np.concatenate([
            player_pieces.flatten(),
            opponent_pieces.flatten(),
            empty_spaces.flatten()
        ])

        return encoded

    def load_dataset(self, file_path="connectx-state-action-value.txt", max_lines=10000):
        """è¼‰å…¥è¨“ç·´æ•¸æ“šé›†"""
        states = []
        action_values = []
        skipped_lines = 0

        try:
            logger.info(f"è¼‰å…¥è¨“ç·´æ•¸æ“šé›†: {file_path}")
            logger.info(f"é™åˆ¶è¼‰å…¥è¡Œæ•¸: {max_lines}")

            with open(file_path, 'r') as f:
                lines = []
                for i, line in enumerate(f):
                    if i >= max_lines:
                        break
                    lines.append(line.strip())

            logger.info(f"å¯¦éš›è¼‰å…¥è¡Œæ•¸: {len(lines)}")

            with tqdm(total=len(lines), desc="è¼‰å…¥æ•¸æ“š") as pbar:
                for line_idx, line in enumerate(lines):
                    pbar.update(1)

                    if not line:
                        continue

                    try:
                        # è§£æä¸€è¡Œæ•¸æ“š
                        parts = line.split(',')
                        if len(parts) < 8:
                            skipped_lines += 1
                            continue

                        # è§£ææ£‹ç›¤ç‹€æ…‹
                        board_str = parts[0]
                        if len(board_str) != 42:
                            skipped_lines += 1
                            continue

                        # è½‰æ›æ£‹ç›¤ç‹€æ…‹
                        board_state = []
                        for char in board_str:
                            if char not in '012':
                                raise ValueError(f"ç„¡æ•ˆå­—ç¬¦: {char}")
                            board_state.append(int(char))

                        # è§£æå‹•ä½œåƒ¹å€¼
                        action_vals = []
                        for i in range(1, 8):  # 7å€‹å‹•ä½œå€¼
                            val_str = parts[i].strip()
                            if val_str == '':
                                action_vals.append(-999.0)  # ç„¡æ•ˆå‹•ä½œ
                            else:
                                try:
                                    action_vals.append(float(val_str))
                                except ValueError:
                                    action_vals.append(0.0)

                        # æª¢æŸ¥æœ‰æ•ˆå‹•ä½œ
                        valid_actions = [i for i, val in enumerate(action_vals) if val > -900]
                        if len(valid_actions) == 0:
                            skipped_lines += 1
                            continue

                        # ç·¨ç¢¼ç‹€æ…‹
                        encoded_state = self.encode_state(board_state, 1)  # å‡è¨­å¾å…ˆæ‰‹è¦–è§’

                        states.append(encoded_state)
                        action_values.append(action_vals)

                    except Exception as e:
                        logger.debug(f"ç¬¬ {line_idx + 1} è¡Œè§£æéŒ¯èª¤: {e}")
                        skipped_lines += 1
                        continue

            logger.info(f"æ•¸æ“šè¼‰å…¥å®Œæˆ:")
            logger.info(f"  æˆåŠŸè§£æ: {len(states)} å€‹æ¨£æœ¬")
            logger.info(f"  è·³éè¡Œæ•¸: {skipped_lines}")

            if len(states) == 0:
                logger.error("æ²’æœ‰æˆåŠŸè§£æä»»ä½•æ•¸æ“šï¼")
                return None, None

            return np.array(states), np.array(action_values)

        except FileNotFoundError:
            logger.error(f"æ‰¾ä¸åˆ°æ•¸æ“šé›†æ–‡ä»¶: {file_path}")
            return None, None
        except Exception as e:
            logger.error(f"è¼‰å…¥æ•¸æ“šé›†æ™‚å‡ºéŒ¯: {e}")
            return None, None

    def train(self, epochs=100, batch_size=128, max_lines=10000):
        """ç›£ç£å­¸ç¿’è¨“ç·´"""
        logger.info("ğŸš€ é–‹å§‹ç›£ç£å­¸ç¿’è¨“ç·´")

        # è¼‰å…¥æ•¸æ“šé›†
        states, action_values = self.load_dataset(max_lines=max_lines)
        if states is None or action_values is None:
            logger.error("âŒ æ•¸æ“šé›†è¼‰å…¥å¤±æ•—")
            return None

        logger.info(f"ğŸ“Š æ•¸æ“šé›†è¼‰å…¥æˆåŠŸ: {len(states)} å€‹æ¨£æœ¬")

        # è¨“ç·´å¾ªç’°
        self.policy_net.train()
        best_loss = float('inf')

        for epoch in range(epochs):
            epoch_start_time = time.time()
            total_loss = 0.0
            total_policy_loss = 0.0
            total_value_loss = 0.0
            num_batches = 0

            # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
            indices = np.random.permutation(len(states))

            # æ‰¹æ¬¡è¨“ç·´
            for batch_start in range(0, len(states), batch_size):
                batch_end = min(batch_start + batch_size, len(states))
                batch_indices = indices[batch_start:batch_end]

                # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
                batch_states = torch.FloatTensor(states[batch_indices]).to(self.device)
                batch_action_values = torch.FloatTensor(action_values[batch_indices]).to(self.device)

                # å‰å‘å‚³æ’­
                predicted_probs, predicted_values = self.policy_net(batch_states)

                # è¨ˆç®—ç›®æ¨™
                # è™•ç†ç„¡æ•ˆå‹•ä½œ
                valid_mask = (batch_action_values > -900).float()
                masked_action_values = batch_action_values * valid_mask + (-1000) * (1 - valid_mask)

                # è½‰æ›ç‚ºç›®æ¨™æ¦‚ç‡åˆ†ä½ˆ
                target_probs = F.softmax(masked_action_values / 0.5, dim=1)  # æº«åº¦åƒæ•¸

                # åƒ¹å€¼ç›®æ¨™ï¼šæœ€å¤§å‹•ä½œåƒ¹å€¼
                target_values = torch.max(batch_action_values * valid_mask + (-1000) * (1 - valid_mask), dim=1)[0].unsqueeze(1)
                target_values = torch.tanh(target_values / 10.0)  # æ­£è¦åŒ–åˆ°[-1,1]

                # è¨ˆç®—æå¤±
                policy_loss = F.kl_div(torch.log(predicted_probs + 1e-8), target_probs, reduction='batchmean')
                value_loss = F.mse_loss(predicted_values, target_values)
                total_loss_batch = policy_loss + 0.5 * value_loss

                # åå‘å‚³æ’­
                self.optimizer.zero_grad()
                total_loss_batch.backward()
                torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
                self.optimizer.step()

                # ç´¯ç©æå¤±
                total_loss += total_loss_batch.item()
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                num_batches += 1

            # è¨ˆç®—å¹³å‡æå¤±
            avg_loss = total_loss / num_batches if num_batches > 0 else 0
            avg_policy_loss = total_policy_loss / num_batches if num_batches > 0 else 0
            avg_value_loss = total_value_loss / num_batches if num_batches > 0 else 0

            epoch_time = time.time() - epoch_start_time

            # å­¸ç¿’ç‡èª¿åº¦
            self.scheduler.step(avg_loss)

            # è¨˜éŒ„æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self.save_checkpoint("best_supervised_model.pt")

            # æ¯10å€‹epochå ±å‘Šä¸€æ¬¡
            if (epoch + 1) % 10 == 0 or epoch == 0:
                logger.info(f"Epoch [{epoch+1}/{epochs}] "
                          f"Loss: {avg_loss:.6f} "
                          f"Policy: {avg_policy_loss:.6f} "
                          f"Value: {avg_value_loss:.6f} "
                          f"Time: {epoch_time:.2f}s")

            # ä¿å­˜å®šæœŸæª¢æŸ¥é»
            if (epoch + 1) % 50 == 0:
                checkpoint_name = f"supervised_epoch_{epoch+1}.pt"
                self.save_checkpoint(checkpoint_name)

        logger.info("âœ… ç›£ç£å­¸ç¿’è¨“ç·´å®Œæˆ")
        return self.policy_net

    def save_checkpoint(self, filename):
        """ä¿å­˜æª¢æŸ¥é»"""
        try:
            checkpoint = {
                'model_state_dict': self.policy_net.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'config': self.config,
                'save_timestamp': datetime.now().isoformat(),
                'pytorch_version': str(torch.__version__)
            }

            checkpoint_path = f"checkpoints/{filename}"
            torch.save(checkpoint, checkpoint_path)

            file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB
            logger.info(f"âœ… å·²ä¿å­˜æª¢æŸ¥é»: {filename} ({file_size:.2f} MB)")

        except Exception as e:
            logger.error(f"ä¿å­˜æª¢æŸ¥é»æ™‚å‡ºéŒ¯: {e}")

    def evaluate_random_games(self, num_games=100):
        """è©•ä¼°æ¨¡å‹å°éš¨æ©Ÿå°æ‰‹çš„æ€§èƒ½"""
        logger.info(f"ğŸ¯ è©•ä¼°æ¨¡å‹æ€§èƒ½ ({num_games} å±€éŠæˆ²)")

        self.policy_net.eval()
        wins = 0
        draws = 0
        losses = 0

        for i in range(num_games):
            try:
                # å‰µå»ºéŠæˆ²ç’°å¢ƒ
                env = make("connectx", debug=False)
                env.reset()

                # ç°¡å–®çš„éŠæˆ²å¾ªç’°
                done = False
                step_count = 0
                max_steps = 42

                while not done and step_count < max_steps:
                    current_player = step_count % 2

                    if current_player == 0:  # AIç©å®¶
                        # ç²å–ç•¶å‰ç‹€æ…‹
                        obs = env.state[0]['observation']
                        board = obs['board']
                        mark = obs['mark']

                        # ç·¨ç¢¼ç‹€æ…‹
                        encoded_state = self.encode_state(board, mark)
                        state_tensor = torch.FloatTensor(encoded_state).unsqueeze(0).to(self.device)

                        # ç²å–å‹•ä½œæ¦‚ç‡
                        with torch.no_grad():
                            action_probs, _ = self.policy_net(state_tensor)

                        # é¸æ“‡å‹•ä½œï¼ˆè²ªå©ªç­–ç•¥ï¼‰
                        valid_actions = [c for c in range(7) if board[c] == 0]
                        if valid_actions:
                            masked_probs = action_probs.cpu().numpy()[0]
                            valid_probs = [masked_probs[a] for a in valid_actions]
                            best_idx = np.argmax(valid_probs)
                            action = valid_actions[best_idx]
                        else:
                            action = 0

                    else:  # éš¨æ©Ÿå°æ‰‹
                        obs = env.state[1]['observation']
                        board = obs['board']
                        valid_actions = [c for c in range(7) if board[c] == 0]
                        action = np.random.choice(valid_actions) if valid_actions else 0

                    # åŸ·è¡Œå‹•ä½œ
                    env.step([action, None] if current_player == 0 else [None, action])

                    # æª¢æŸ¥éŠæˆ²çµæŸ
                    if len(env.state) >= 2:
                        status_0 = env.state[0].get('status', 'ACTIVE')
                        status_1 = env.state[1].get('status', 'ACTIVE')

                        if status_0 != 'ACTIVE' or status_1 != 'ACTIVE':
                            done = True

                    step_count += 1

                # è¨ˆç®—çµæœ
                if len(env.state) >= 2:
                    reward_0 = env.state[0].get('reward', 0)
                    reward_1 = env.state[1].get('reward', 0)

                    if reward_0 > reward_1:
                        wins += 1
                    elif reward_1 > reward_0:
                        losses += 1
                    else:
                        draws += 1

                if (i + 1) % 20 == 0:
                    current_wr = wins / (i + 1) * 100
                    logger.info(f"è©•ä¼°é€²åº¦: {i+1}/{num_games}, ç•¶å‰å‹ç‡: {current_wr:.1f}%")

            except Exception as e:
                logger.error(f"è©•ä¼°ç¬¬ {i+1} å±€æ™‚å‡ºéŒ¯: {e}")
                losses += 1

        win_rate = wins / num_games * 100
        logger.info(f"ğŸ“Š è©•ä¼°çµæœ:")
        logger.info(f"   å‹åˆ©: {wins} ({win_rate:.1f}%)")
        logger.info(f"   å¹³å±€: {draws} ({draws/num_games*100:.1f}%)")
        logger.info(f"   å¤±æ•—: {losses} ({losses/num_games*100:.1f}%)")

        return win_rate

def create_config():
    """å‰µå»ºè¨“ç·´é…ç½®"""
    config = {
        'agent': {
            'input_size': 126,      # 3å€‹é€šé“ Ã— 42å€‹ä½ç½®
            'hidden_size': 256,     # éš±è—å±¤å¤§å°
            'num_layers': 3,        # éš±è—å±¤æ•¸é‡
            'learning_rate': 0.001, # å­¸ç¿’ç‡
            'weight_decay': 0.0001  # æ¬Šé‡è¡°æ¸›
        },
        'training': {
            'epochs': 200,          # è¨“ç·´epochs
            'batch_size': 128,      # æ‰¹æ¬¡å¤§å°
            'max_lines': 50000,     # æœ€å¤§æ•¸æ“šé›†è¡Œæ•¸
            'eval_games': 100       # è©•ä¼°éŠæˆ²æ•¸é‡
        }
    }
    return config

def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    print("ğŸ® ConnectX ç›£ç£å­¸ç¿’è¨“ç·´")
    print("=" * 50)

    # å‰µå»ºå¿…è¦ç›®éŒ„
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('logs', exist_ok=True)

    # æª¢æŸ¥æ•¸æ“šé›†æ–‡ä»¶
    dataset_file = "connectx-state-action-value.txt"
    if not os.path.exists(dataset_file):
        logger.error(f"âŒ æ‰¾ä¸åˆ°æ•¸æ“šé›†æ–‡ä»¶: {dataset_file}")
        return

    # å‰µå»ºé…ç½®
    config = create_config()

    # æª¢æŸ¥è¨­å‚™
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")

    try:
        # å‰µå»ºè¨“ç·´å™¨
        trainer = ConnectXTrainer(config)
        logger.info("âœ… è¨“ç·´å™¨å‰µå»ºæˆåŠŸ")

        # é¡¯ç¤ºé…ç½®
        print("\nğŸ“‹ è¨“ç·´é…ç½®:")
        print(f"   ç¶²çµ¡çµæ§‹: {config['agent']['hidden_size']} éš±è—å–®å…ƒ, {config['agent']['num_layers']} å±¤")
        print(f"   å­¸ç¿’ç‡: {config['agent']['learning_rate']}")
        print(f"   è¨“ç·´epochs: {config['training']['epochs']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {config['training']['batch_size']}")
        print(f"   æœ€å¤§æ•¸æ“šé›†è¡Œæ•¸: {config['training']['max_lines']}")

        # é–‹å§‹è¨“ç·´
        print("\nğŸš€ é–‹å§‹ç›£ç£å­¸ç¿’è¨“ç·´...")
        start_time = time.time()

        trained_model = trainer.train(
            epochs=config['training']['epochs'],
            batch_size=config['training']['batch_size'],
            max_lines=config['training']['max_lines']
        )

        training_time = time.time() - start_time

        if trained_model is not None:
            logger.info(f"âœ… è¨“ç·´å®Œæˆï¼ç”¨æ™‚: {training_time:.1f}ç§’")

            # ä¿å­˜æœ€çµ‚æ¨¡å‹
            final_checkpoint = f"supervised_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt"
            trainer.save_checkpoint(final_checkpoint)

            # è©•ä¼°æ¨¡å‹
            print("\nğŸ¯ è©•ä¼°æ¨¡å‹æ€§èƒ½...")
            win_rate = trainer.evaluate_random_games(num_games=config['training']['eval_games'])

            print(f"\nğŸ‰ è¨“ç·´å®Œæˆ!")
            print(f"   ç¸½ç”¨æ™‚: {training_time:.1f}ç§’ ({training_time/60:.1f}åˆ†é˜)")
            print(f"   æœ€çµ‚å‹ç‡: {win_rate:.1f}%")
            print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: checkpoints/{final_checkpoint}")

            # ä½¿ç”¨å»ºè­°
            if win_rate >= 80:
                print("\nğŸŒŸ æ¨¡å‹æ€§èƒ½å„ªç•°ï¼å¯ä»¥ç”¨æ–¼æ¯”è³½")
            elif win_rate >= 60:
                print("\nğŸ‘ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå»ºè­°é€²è¡Œæ›´å¤šè¨“ç·´")
            else:
                print("\nâš ï¸ æ¨¡å‹æ€§èƒ½éœ€è¦æ”¹é€²ï¼Œå»ºè­°å¢åŠ è¨“ç·´æ™‚é–“æˆ–èª¿æ•´åƒæ•¸")

        else:
            logger.error("âŒ è¨“ç·´å¤±æ•—")

    except KeyboardInterrupt:
        logger.info("â¹ï¸ è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        logger.error(f"âŒ è¨“ç·´éç¨‹ä¸­å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
